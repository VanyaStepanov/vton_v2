{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "collapsed_sections": [
    "Qh3dRdojPCy1",
    "3WXWS5m9Ozyt",
    "SdvcT9PEO5ye",
    "0nznhr0CO8xT"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#CONFIG"
   ],
   "metadata": {
    "id": "Qh3dRdojPCy1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7pOn3vu5dQd"
   },
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "\n",
    "# Queued targets that REQUIRE the detailer (already pre-filtered by your generator)\n",
    "DETAILER_QUEUE_FOLDER = \"/content/drive/MyDrive/SS_OUTPUT_FOLDER/v1-5\"  # @param {type:\"string\"}\n",
    "\n",
    "# Where to save (1) the source garment and (2) the inpainted results\n",
    "TARGET_DIR = \"/content/drive/MyDrive/DETAILER_DONE/vtnon_v1_5\"               # @param {type:\"string\"}\n",
    "\n",
    "# Root that contains your SKU trees (used to locate the source)\n",
    "WORKING_DIR = \"/content/drive/MyDrive/SikSilk\"                  # @param {type:\"string\"}\n",
    "\n",
    "# Root for (subcategory-wide) garment masks to constrain the detector\n",
    "MASKS_ROOT = \"/content/drive/MyDrive/SKSLK_MODELS\"              # @param {type:\"string\"}\n",
    "\n",
    "# Model/runtime knobs\n",
    "DEVICE_STR = \"cuda\"\n",
    "INPAINT_GENEROUS_PAD = 150                                      # @param {type:\"integer\"}\n",
    "INPAINT_TINY_PAD = 6                                            # @param {type:\"integer\"}\n",
    "INPAINT_SEED = 2025                                             # @param {type:\"integer\"}\n",
    "VISUALIZE = True                                                # @param {type:\"boolean\"}\n",
    "\n",
    "# File patterns\n",
    "VALID_EXTS = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\")\n",
    "\n",
    "# Allowed detail tokens (normalized)\n",
    "ALLOWED_DETAIL_TYPES = [\"crest\", \"logo\", \"patch\", \"waist text\", \"sleeve text\"]\n",
    "\n",
    "# Allowed garment tokens (for prompting DINO)\n",
    "ALLOWED_GARMENT_TYPES = [\n",
    "    \"hoodie\",\"jeans\",\"joggers\",\"shorts\",\"sweater\",\"swimwear\",\"t-shirt\",\"shirts\",\n",
    "    \"track top\",\"trousers\",\"twinset\",\"polo\",\"vests\",\"shirts\"\n",
    "]\n",
    "TOP_GARMENTS = [\"t-shirt\", \"shirt\", \"sweater\", \"hoodie\", \"track top\", \"vest\"]\n",
    "BOTTOM_GARMENTS = [\"shorts\", \"jogger-trousers\", \"trousers\", \"jeans\", \"swimwear\"]\n",
    "TWINSET_TYPES = [\"twinset\"]\n",
    "\n",
    "# Angle parsing / source lookup helpers\n",
    "BASE_NAMES = [\"fr_rght\", \"fr_lft\", \"fr_cl\",\n",
    "              #\"bc_rght\", \"bc_lft\", \"bc_cl\", \"bc\",\n",
    "              \"fr\", \"lft\", \"rght\"]\n",
    "ACCEPTABLE_SUFFIXES = [\"cut\"]\n",
    "\n",
    "# Skip if already have any inpainted output in TARGET_DIR for this SKU+angle\n",
    "SKIP_IF_ALREADY_INPAINTED = True # @param {type:\"boolean\"}\n",
    "\n",
    "USE_BF16_INFERENCE = True  # global toggle\n",
    "\n",
    "\n",
    "# Create target dir if missing\n",
    "import os, pathlib\n",
    "pathlib.Path(TARGET_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#INSTALLS (restart & reinstall again after this)"
   ],
   "metadata": {
    "id": "3WXWS5m9Ozyt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ],
   "metadata": {
    "id": "oqCgnsdjBfzs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 via Hugging Face transformers\n",
    "!pip install -q \"git+https://github.com/huggingface/transformers.git\"\n"
   ],
   "metadata": {
    "id": "0ZUgwThxBmUv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# GroundingDINO setup removed \u2014 SAM3 now handles detection + segmentation in one model.\n"
   ],
   "metadata": {
    "id": "MHz7WW6oBpDz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%pip -q install open_clip_torch ninja wheel transformers accelerate \\\n",
    "                 sentencepiece protobuf huggingface_hub opencv-python\n",
    "!pip install -U --no-deps --force-reinstall \"git+https://github.com/huggingface/diffusers.git@main\"\n",
    "#%pip -q install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install --upgrade open_clip_torch"
   ],
   "metadata": {
    "collapsed": true,
    "id": "CjRycBjyBsdk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install piexif"
   ],
   "metadata": {
    "id": "pm9LoEbd5fn9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/\n",
    "!git clone --depth 1 https://github.com/song-wensong/insert-anything.git"
   ],
   "metadata": {
    "id": "kufDQpabB4h7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.2.0+torch2.6-cp312-cp312-linux_x86_64.whl\n",
    "!pip install torch==2.6 torchvision==0.21 torchaudio==2.6\n",
    "!pip install ninja wheel diffusers transformers accelerate sentencepiece protobuf huggingface_hub\n",
    "!git clone https://huggingface.co/aha2023/insert-anything-lora-for-nunchaku"
   ],
   "metadata": {
    "id": "kkLr9zB3CJEl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#SETUP"
   ],
   "metadata": {
    "id": "SdvcT9PEO5ye"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, torch, numpy as np, cv2, base64, gc, json\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageOps\n",
    "import piexif\n",
    "\n",
    "device = torch.device(DEVICE_STR if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\u2705 Torch device:\", device)\n"
   ],
   "metadata": {
    "id": "HNmBx946B-JK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Sam3Processor, Sam3Model\n",
    "\n",
    "HF_SAM3_ID = \"facebook/sam3\"\n",
    "SAM3_CONFIDENCE = 0.05   # permissive to catch small logos; raise if predictions get noisy\n",
    "SAM3_RESOLUTION = 1024\n",
    "SAM3_DEVICE = device\n",
    "\n",
    "sam3_processor = Sam3Processor.from_pretrained(HF_SAM3_ID)\n",
    "sam3_model = Sam3Model.from_pretrained(HF_SAM3_ID).to(SAM3_DEVICE)\n",
    "sam3_model.eval()\n",
    "print(\"\u2705 HF SAM3 ready (text \u2192 boxes \u2192 masks)\")\n"
   ],
   "metadata": {
    "id": "azEqfSb-5p5F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 replaces the old SAM2 predictor \u2014 no extra setup needed.\n"
   ],
   "metadata": {
    "id": "vuJxHufYTNKj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 initialized above.\n"
   ],
   "metadata": {
    "id": "jCTA2cBPTQNJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Insert_anything on nunchaku\n",
    "%cd /content/insert-anything\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import FluxFillPipeline, FluxPriorReduxPipeline\n",
    "from utils.utils import get_bbox_from_mask, expand_bbox, pad_to_square, box2squre, expand_image_mask\n",
    "from nunchaku.models.transformers.transformer_flux import NunchakuFluxTransformer2dModel\n",
    "from datetime import datetime\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(f\"cuda\")\n",
    "dtype = torch.bfloat16\n",
    "size = (1024, 1024)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model and LoRA-for-nunchaku weights\n",
    "# Please replace the paths with your own paths\n",
    "transformer = NunchakuFluxTransformer2dModel.from_pretrained(\"mit-han-lab/svdq-int4-flux.1-fill-dev\")\n",
    "\n",
    "pipe = FluxFillPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Fill-dev\",\n",
    "    transformer=transformer,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "transformer.update_lora_params(\"/content/drive/MyDrive/insert-anything-lora/insert-anything_extracted_lora_rank_256-bf16.safetensors\")\n",
    "# Adjust the LoRA strength\n",
    "transformer.set_lora_strength(1)\n",
    "\n",
    "redux = FluxPriorReduxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Redux-dev\").to(dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "# The purpose of this code is to reduce the GPU memory usage to 26GB, but it will increase the inference time accordingly.\n",
    "pipe.to(\"cuda\")\n",
    "redux.to(\"cuda\")\n",
    "os.environ[\"NNCF_GROUP_SIZE\"] = \"-1\"      # disable token merging\n"
   ],
   "metadata": {
    "id": "f4e99gLxT3TM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UTILS"
   ],
   "metadata": {
    "id": "0nznhr0CO8xT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "q5N9LysfFp9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def open_upright(path) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return ImageOps.exif_transpose(im).convert(\"RGB\")\n",
    "\n",
    "def open_source_with_black_bg(path: str) -> Image.Image:\n",
    "    im = Image.open(path)\n",
    "    im = ImageOps.exif_transpose(im)\n",
    "    name_low = os.path.basename(path).lower()\n",
    "    if \"_cut\" in name_low and im.mode in (\"RGBA\",\"LA\"):\n",
    "        rgb = im.convert(\"RGB\")\n",
    "        alpha = im.getchannel(\"A\")\n",
    "        black = Image.new(\"RGB\", im.size, (0,0,0))\n",
    "        return Image.composite(rgb, black, alpha)\n",
    "    return im.convert(\"RGB\")\n",
    "\n",
    "\n",
    "# NEW \u2014 root of subcategory-wide garment masks\n",
    "MASKS_ROOT = '/content/drive/MyDrive/SKSLK_MODELS'\n",
    "MASK_EXTS = ('.png', '.jpg', '.jpeg', '.webp', '.PNG', '.JPG', '.JPEG', '.WEBP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "\n",
    "# --- Helper: get <Category>/<Subcategory> from the *source* path ------------\n",
    "_SKU_DIR_RE = re.compile(r\"SS-\\d{3,7}\", re.IGNORECASE)\n",
    "\n",
    "def _category_subcategory_from_source(src_path: str) -> tuple[str, str] | None:\n",
    "    \"\"\"\n",
    "    Resolve (Category, Subcategory) from the garment *source* path.\n",
    "    Preferred: relative to WORKING_DIR \u2192 parts[0], parts[1].\n",
    "    Fallback: find the SKU folder in the path and take the two parents.\n",
    "    Returns None if not resolvable.\n",
    "    \"\"\"\n",
    "    p = Path(src_path).resolve()\n",
    "    wr = Path(WORKING_DIR).resolve()\n",
    "\n",
    "    # Preferred: relative to WORKING_DIR\n",
    "    try:\n",
    "        rel = p.relative_to(wr)\n",
    "        parts = rel.parts\n",
    "        # Expect: Category/Subcategory/SKU/<file>\n",
    "        if len(parts) >= 3:\n",
    "            return parts[0], parts[1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: locate the SKU dir and take its two parents as Cat/Subcat\n",
    "    parts = p.parts\n",
    "    sku_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if _SKU_DIR_RE.fullmatch(part or \"\"):\n",
    "            sku_idx = i\n",
    "            break\n",
    "    if sku_idx is not None and sku_idx >= 2:\n",
    "        return parts[sku_idx - 2], parts[sku_idx - 1]\n",
    "\n",
    "    # Last resort: try after an explicit 'SikSilk' anchor\n",
    "    if \"SikSilk\" in parts:\n",
    "        j = parts.index(\"SikSilk\")\n",
    "        if len(parts) >= j + 3:\n",
    "            return parts[j + 1], parts[j + 2]\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- New: derive mask basename from *angle*, not from filename heuristics ----\n",
    "def _mask_basename_from_angle(angle_code: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Map 'fr' -> 'fr_mask', 'fr_lft' -> 'fr_lft_mask', 'bc_cl' -> 'bc_cl_mask', etc.\n",
    "    If angle_code is missing, return None (\u2192 no mask).\n",
    "    \"\"\"\n",
    "    if not angle_code:\n",
    "        return None\n",
    "    angle = angle_code.strip().lower()\n",
    "    return f\"{angle}_mask\"\n",
    "\n",
    "# --- Exact-only mask finder ---------------------------------------------------\n",
    "\n",
    "def find_mask_for_generated_exact(gen_path: str, source_path: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    EXACT lookup (no fuzzy fallbacks):\n",
    "      angle  = parsed from queued filename/path (e.g., SS-12345_fr_cl.* -> 'fr_cl')\n",
    "      (cat, subcat) = derived from source_path\n",
    "      priority: <angle>_mask_agnostic.<ext>  \u2192  <angle>_mask.<ext>\n",
    "      searched in: MASKS_ROOT / cat / subcat\n",
    "    \"\"\"\n",
    "    # 1) angle from queued filename\n",
    "    _, angle = extract_sku_and_angle_from_path(gen_path)\n",
    "    if not angle:\n",
    "        print(\"\u26a0\ufe0f  No angle parsed \u2014 proceeding without a mask.\")\n",
    "        return None\n",
    "    angle = angle.strip().lower()\n",
    "\n",
    "    # 2) category/subcategory from source path\n",
    "    cat_sub = _category_subcategory_from_source(source_path)\n",
    "    if not cat_sub:\n",
    "        print(\"\u26a0\ufe0f  Could not resolve Category/Subcategory from source path \u2014 no mask.\")\n",
    "        return None\n",
    "    category, subcategory = cat_sub\n",
    "\n",
    "    mask_dir = Path(MASKS_ROOT) / category / subcategory\n",
    "\n",
    "    # 3) Try agnostic first, then regular; exact names only\n",
    "    candidates = [f\"{angle}_mask_agnostic\", f\"{angle}_mask\"]\n",
    "\n",
    "    for name in candidates:\n",
    "        for ext in MASK_EXTS:\n",
    "            cand = mask_dir / f\"{name}{ext}\"\n",
    "            if cand.exists():\n",
    "                which = \"agnostic\" if name.endswith(\"_agnostic\") else \"regular\"\n",
    "                print(f\"\u2705 Found {which} mask: {cand}\")\n",
    "                return cand\n",
    "\n",
    "    print(f\"\u26a0\ufe0f  No exact mask found in {mask_dir} for angle '{angle}' \"\n",
    "          f\"(tried {candidates} with MASK_EXTS). Proceeding without mask.\")\n",
    "    return None\n",
    "\n",
    "def load_binary_mask_for_generated(gen_path: str, source_path: str, gen_img: Image.Image) -> np.ndarray | None:\n",
    "    mp = find_mask_for_generated_exact(gen_path, source_path)\n",
    "    if mp is None:\n",
    "        return None\n",
    "    with Image.open(mp) as m:\n",
    "        m = ImageOps.exif_transpose(m)\n",
    "        return align_mask_to_image(m, gen_img)\n",
    "\n",
    "# --- Align (unchanged) --------------------------------------------------------\n",
    "def align_mask_to_image(mask_img: Image.Image, target_img: Image.Image) -> np.ndarray:\n",
    "    mw, mh = mask_img.size\n",
    "    tw, th = target_img.size\n",
    "    if mh == th and mw > 0 and (tw % mw) == 0 and 1 < (tw // mw) <= 3:\n",
    "        k = tw // mw\n",
    "        tiled = Image.new('L', (tw, th), 0)\n",
    "        src = mask_img.convert('L')\n",
    "        for i in range(k):\n",
    "            tiled.paste(src, (i * mw, 0))\n",
    "        M = np.array(tiled, dtype=np.uint8)\n",
    "    else:\n",
    "        if mw == 0 or mh == 0:\n",
    "            return np.zeros((th, tw), np.uint8)\n",
    "        scale = max(mw / tw, mh / th)\n",
    "        new_w = int(round(mw / scale)); new_h = int(round(mh / scale))\n",
    "        m_resized = mask_img.convert('L').resize((new_w, new_h), Image.NEAREST)\n",
    "        M = np.zeros((th, tw), np.uint8)\n",
    "        x0 = (tw - new_w) // 2; y0 = (th - new_h) // 2\n",
    "        M[y0:y0+new_h, x0:x0+new_w] = np.array(m_resized, dtype=np.uint8)\n",
    "    return ((M > 127).astype(np.uint8) * 255)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- visuals\n",
    "def _draw_bbox(img: Image.Image, bb_xyxy, color=\"lime\", width=4):\n",
    "    out = img.copy()\n",
    "    if bb_xyxy is None: return out\n",
    "    draw = ImageDraw.Draw(out)\n",
    "    draw.rectangle(bb_xyxy, outline=color, width=width)\n",
    "    return out\n",
    "\n",
    "def _show_images(pairs, cols=3, figsize=(16,12)):\n",
    "    rows = int(np.ceil(len(pairs) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows*cols>1 else [axes]\n",
    "    for ax,(title,img) in zip(axes, pairs):\n",
    "        ax.imshow(img); ax.set_title(title, fontsize=10); ax.axis(\"off\")\n",
    "    for ax in axes[len(pairs):]: ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def resize_and_pad(image, target_size=1024):\n",
    "    w, h = image.size\n",
    "    scale = target_size / max(1, max(w, h))\n",
    "    new_w, new_h = int(round(w * scale)), int(round(h * scale))\n",
    "    image_resized = image.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    pad_w = (target_size - new_w) // 2\n",
    "    pad_h = (target_size - new_h) // 2\n",
    "    padding = (pad_w, pad_h, target_size - new_w - pad_w, target_size - new_h - pad_h)\n",
    "\n",
    "    # \u2705 Match fill type to mode\n",
    "    mode = image_resized.mode\n",
    "    if mode in (\"L\", \"1\", \"I\", \"F\"):\n",
    "        fill_color = 0                      # int for single-channel\n",
    "    elif mode == \"RGBA\":\n",
    "        fill_color = (0, 0, 0, 0)           # transparent for RGBA\n",
    "    else:\n",
    "        fill_color = (0, 0, 0)              # RGB tuple for RGB/others\n",
    "\n",
    "    return ImageOps.expand(image_resized, padding, fill=fill_color)\n",
    "\n",
    "def box_1024_to_original(box_xyxy_1024, original_w, original_h):\n",
    "    x1_1024, y1_1024, x2_1024, y2_1024 = [float(v) for v in box_xyxy_1024]\n",
    "    target_size = 1024\n",
    "    w, h = original_w, original_h\n",
    "    scale = target_size / max(w, h)\n",
    "    new_w, new_h = int(round(w*scale)), int(round(h*scale))\n",
    "    pad_w = (target_size - new_w)//2\n",
    "    pad_h = (target_size - new_h)//2\n",
    "    x1 = (x1_1024 - pad_w) / scale; x2 = (x2_1024 - pad_w) / scale\n",
    "    y1 = (y1_1024 - pad_h) / scale; y2 = (y2_1024 - pad_h) / scale\n",
    "    x1 = min(max(int(round(x1)),0), w); x2 = min(max(int(round(x2)),0), w)\n",
    "    y1 = min(max(int(round(y1)),0), h); y2 = min(max(int(round(y2)),0), h)\n",
    "    return [x1,y1,x2,y2]\n"
   ],
   "metadata": {
    "id": "VGxk8vnk5wU9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_binary_mask(img_rgb: Image.Image, mask_np: np.ndarray | None, outside_color=(5,5,5)) -> Image.Image:\n",
    "    if mask_np is None:\n",
    "        return img_rgb\n",
    "    mask_L = Image.fromarray(mask_np.astype(np.uint8))\n",
    "    mode = img_rgb.mode\n",
    "    if mode not in (\"RGB\", \"RGBA\", \"L\"):\n",
    "        img_rgb = img_rgb.convert(\"RGB\")\n",
    "        mode = \"RGB\"\n",
    "    if mode == \"RGB\":\n",
    "        if isinstance(outside_color, int):\n",
    "            outside_color = (outside_color,) * 3\n",
    "        bg = Image.new(\"RGB\", img_rgb.size, outside_color)\n",
    "    elif mode == \"RGBA\":\n",
    "        if isinstance(outside_color, int):\n",
    "            outside_color = (outside_color,) * 3 + (255,)\n",
    "        elif len(outside_color) == 3:\n",
    "            outside_color = (*outside_color, 255)\n",
    "        bg = Image.new(\"RGBA\", img_rgb.size, outside_color)\n",
    "    else:\n",
    "        if isinstance(outside_color, tuple):\n",
    "            outside_color = int(np.mean(outside_color))\n",
    "        bg = Image.new(\"L\", img_rgb.size, int(outside_color))\n",
    "    return Image.composite(img_rgb, bg, mask_L)\n",
    "\n",
    "\n",
    "# Dynamic, perimeter-based mask gating for detect_detail (logo-friendly)\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# --- SAM3 helpers -----------------------------------------------------------\n",
    "def _clip_box_to_image(box_xyxy, w: int, h: int):\n",
    "    x1, y1, x2, y2 = box_xyxy\n",
    "    x1 = max(0, min(w, int(round(x1))))\n",
    "    y1 = max(0, min(h, int(round(y1))))\n",
    "    x2 = max(0, min(w, int(round(x2))))\n",
    "    y2 = max(0, min(h, int(round(y2))))\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def _sam3_predict_text(image_pil: Image.Image, prompt: str, *, max_dets: int = 12, score_threshold: float = SAM3_CONFIDENCE):\n",
    "    \"\"\"Run SAM3 (HF) with a text prompt and return sorted predictions.\"\"\"\n",
    "    if not prompt:\n",
    "        return []\n",
    "    if \"sam3_processor\" not in globals() or \"sam3_model\" not in globals():\n",
    "        raise RuntimeError(\"SAM3 is not initialized. Run the SAM3 setup cell first.\")\n",
    "\n",
    "    inputs = sam3_processor(images=image_pil, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(SAM3_DEVICE) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = sam3_model(**inputs)\n",
    "\n",
    "    processed = sam3_processor.post_process_instance_segmentation(\n",
    "        outputs,\n",
    "        threshold=score_threshold,\n",
    "        target_sizes=[image_pil.size[::-1]],\n",
    "    )[0]\n",
    "\n",
    "    boxes = processed.get(\"boxes\")\n",
    "    scores = processed.get(\"scores\")\n",
    "    masks = processed.get(\"masks\")\n",
    "    if boxes is None or scores is None or boxes.numel() == 0:\n",
    "        return []\n",
    "\n",
    "    boxes_np = boxes.detach().cpu().numpy()\n",
    "    scores_np = scores.detach().cpu().numpy()\n",
    "    masks_np = masks.detach().cpu().numpy() if masks is not None else None\n",
    "\n",
    "    order = scores_np.argsort()[::-1]\n",
    "    preds = []\n",
    "    for idx in order[:max_dets]:\n",
    "        mask_np = None\n",
    "        if masks_np is not None:\n",
    "            mask_np = (masks_np[idx] > 0.5).astype(np.uint8)\n",
    "        preds.append({\n",
    "            \"box\": boxes_np[idx].tolist(),\n",
    "            \"score\": float(scores_np[idx]),\n",
    "            \"mask\": mask_np,\n",
    "        })\n",
    "    return preds\n",
    "\n",
    "def _mask_crop_to_full(mask_crop: np.ndarray | None, crop_box_on_full, full_size):\n",
    "    \"\"\"\n",
    "    Place a mask (aligned to a crop image) back onto the full canvas.\n",
    "    crop_box_on_full = (lx, ty, rx, by) used to produce the crop.\n",
    "    full_size = (W, H) of the destination image.\n",
    "    \"\"\"\n",
    "    if mask_crop is None or crop_box_on_full is None:\n",
    "        return mask_crop\n",
    "\n",
    "    full_w, full_h = full_size\n",
    "    lx, ty, rx, by = [int(round(v)) for v in crop_box_on_full]\n",
    "    x0, y0 = max(0, lx), max(0, ty)\n",
    "    x1, y1 = min(rx, full_w), min(by, full_h)\n",
    "    if x1 <= x0 or y1 <= y0:\n",
    "        return np.zeros((full_h, full_w), np.uint8)\n",
    "\n",
    "    mx0, my0 = max(0, -lx), max(0, -ty)\n",
    "    mx1, my1 = mx0 + (x1 - x0), my0 + (y1 - y0)\n",
    "\n",
    "    patch = mask_crop[my0:my1, mx0:mx1]\n",
    "    if patch.shape[1] != (x1 - x0) or patch.shape[0] != (y1 - y0):\n",
    "        patch = cv2.resize(patch, (x1 - x0, y1 - y0), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    full_mask = np.zeros((full_h, full_w), np.uint8)\n",
    "    full_mask[y0:y1, x0:x1] = (patch > 0).astype(np.uint8) * 255\n",
    "    return full_mask\n",
    "\n",
    "def enlarge_mask(mask_np: np.ndarray, scale: float = 1.05) -> np.ndarray:\n",
    "    \"\"\"Dilate mask outward based on object size (\u2248scale of the foreground).\"\"\"\n",
    "    if mask_np is None:\n",
    "        return None\n",
    "    mask = (mask_np > 0).astype(np.uint8)\n",
    "    ys, xs = np.where(mask)\n",
    "    if ys.size == 0 or scale <= 1.0:\n",
    "        return (mask_np > 0).astype(np.uint8) * 255\n",
    "    h_obj = ys.max() - ys.min() + 1\n",
    "    w_obj = xs.max() - xs.min() + 1\n",
    "    grow = max(1, int(round(max(h_obj, w_obj) * (scale - 1.0))))\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (grow * 2 + 1, grow * 2 + 1))\n",
    "    out = cv2.dilate(mask, kernel, iterations=1)\n",
    "    return (out > 0).astype(np.uint8) * 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Spatially guided detect_detail:\n",
    "# - perimeter-based mask polarity (as before)\n",
    "# - strict+tolerant mask gates\n",
    "# - spatial prior from source garment (normalized center+area)\n",
    "# - combined score = w_score * norm_score + w_spatial * spatial_affinity\n",
    "\n",
    "\n",
    "# ---- helpers ---------------------------------------------------------------\n",
    "# Top-7 spatial re-ranking for detail detection\n",
    "# - Keeps perimeter-based mask polarity & light gates\n",
    "# - Takes SAM3's highest-scoring proposals, filters with mask gates, then\n",
    "#   re-ranks TOP_K using spatial prior from the SOURCE garment\n",
    "# - Normalizes SAM3 scores within those K and slightly down-weights rank-1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers you already use elsewhere ----------\n",
    "def make_spatial_prior_from_box(bb_xyxy, img_size):\n",
    "    \"\"\"Build prior from SOURCE detail box on its garment crop. Normalized to [0,1].\"\"\"\n",
    "    if bb_xyxy is None:\n",
    "        return None\n",
    "    W, H = img_size\n",
    "    x1, y1, x2, y2 = [float(v) for v in bb_xyxy]\n",
    "    x1 = max(0.0, min(W, x1)); y1 = max(0.0, min(H, y1))\n",
    "    x2 = max(0.0, min(W, x2)); y2 = max(0.0, min(H, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    cx = ((x1 + x2) / 2.0) / max(1.0, W)\n",
    "    cy = ((y1 + y2) / 2.0) / max(1.0, H)\n",
    "    area = ((x2 - x1) * (y2 - y1)) / max(1.0, (W * H))\n",
    "    return {\"cx\": float(cx), \"cy\": float(cy), \"area\": float(area)}\n",
    "\n",
    "def _spatial_affinity(cx_n, cy_n, area_n, prior, mirror_ok=True,\n",
    "                      sigma_center=0.16, sigma_area=0.50):\n",
    "    \"\"\"Gaussian affinity in [0,1] for center & (log)area; mirror-aware.\"\"\"\n",
    "    def _aff(cx_p):\n",
    "        dc2 = (cx_n - cx_p)**2 + (cy_n - prior[\"cy\"])**2\n",
    "        s_center = np.exp(- dc2 / (2.0 * (sigma_center**2)))\n",
    "        a = max(1e-6, area_n); ap = max(1e-6, prior[\"area\"])\n",
    "        dlog = np.log(a / ap)\n",
    "        s_area = np.exp(- (dlog**2) / (2.0 * (sigma_area**2)))\n",
    "        return float(s_center * s_area)\n",
    "    base = _aff(prior[\"cx\"])\n",
    "    if mirror_ok:\n",
    "        return max(base, _aff(1.0 - prior[\"cx\"]))\n",
    "    return base\n",
    "\n",
    "# ---------- main: top-7 re-ranking ----------\n",
    "\n",
    "\n",
    "def _ensure_mask_for_image(mask_input, image_pil, *, crop_box_on_full=None):\n",
    "    \"\"\"\n",
    "    Align a mask to image_pil.\n",
    "\n",
    "    mask_input:\n",
    "      \u2022 np.ndarray aligned to image_pil (H\u00d7W)  OR\n",
    "      \u2022 (mask_full_np, \"FULL\") + crop_box_on_full=(lx,ty,rx,by) from crop_to_square\n",
    "\n",
    "    Returns: uint8 mask (0/255) aligned to image_pil.size, with the same padding\n",
    "    behavior as crop_to_square (i.e., if the crop went outside, we pad zeros).\n",
    "    \"\"\"\n",
    "    if mask_input is None:\n",
    "        return None\n",
    "\n",
    "    # Case 1: already aligned to this image\n",
    "    if not (isinstance(mask_input, tuple) and len(mask_input) == 2 and isinstance(mask_input[0], np.ndarray) and mask_input[1] == \"FULL\"):\n",
    "        m = mask_input\n",
    "        if m.ndim == 3:\n",
    "            m = m[...,0] if m.shape[2] > 1 else m.squeeze(-1)\n",
    "        if m.dtype != np.uint8:\n",
    "            m = (m > 0).astype(np.uint8) * 255\n",
    "        if (m.shape[1], m.shape[0]) != image_pil.size:\n",
    "            m = cv2.resize(m, image_pil.size, interpolation=cv2.INTER_NEAREST)\n",
    "        return m\n",
    "\n",
    "    # Case 2: FULL mask + crop box from crop_to_square\n",
    "    mask_full, _ = mask_input\n",
    "    assert crop_box_on_full is not None, \"crop_box_on_full is required for FULL mask.\"\n",
    "\n",
    "    Hf, Wf = mask_full.shape[:2]\n",
    "    lx, ty, rx, by = crop_box_on_full  # exactly what crop_to_square returned\n",
    "\n",
    "    # Target canvas (the square side used by crop_to_square)\n",
    "    tgt_w = int(round(rx - lx))\n",
    "    tgt_h = int(round(by - ty))\n",
    "\n",
    "    # Source window (clamped to the full image bounds)\n",
    "    sx1 = int(np.floor(max(0, lx)))\n",
    "    sy1 = int(np.floor(max(0, ty)))\n",
    "    sx2 = int(np.ceil(min(Wf, rx)))\n",
    "    sy2 = int(np.ceil(min(Hf, by)))\n",
    "\n",
    "    # Offsets where the source window lands on the target canvas\n",
    "    dx = int(np.floor(max(0, -lx)))   # same as crop_to_square's dx\n",
    "    dy = int(np.floor(max(0, -ty)))   # same as crop_to_square's dy\n",
    "\n",
    "    # Build canvas and paste the clipped region at (dx,dy)\n",
    "    canvas = np.zeros((tgt_h, tgt_w), dtype=np.uint8)\n",
    "    if sx2 > sx1 and sy2 > sy1:\n",
    "        patch = mask_full[sy1:sy2, sx1:sx2]\n",
    "        if patch.ndim == 3:\n",
    "            patch = patch[...,0] if patch.shape[2] > 1 else patch.squeeze(-1)\n",
    "        ph, pw = patch.shape[:2]\n",
    "        canvas[dy:dy+ph, dx:dx+pw] = (patch > 0).astype(np.uint8) * 255\n",
    "\n",
    "    # If image_pil size differs by a pixel due to rounding, align by resize\n",
    "    if (canvas.shape[1], canvas.shape[0]) != image_pil.size:\n",
    "        canvas = cv2.resize(canvas, image_pil.size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def _build_inside_mask_1024(mask_aligned_np, image_pil, *,\n",
    "                            border_sample_px=2, erode_px=1, dilate_px=2,\n",
    "                            debug=False):\n",
    "    \"\"\"\n",
    "    Build 1024\u00d71024 INSIDE mask with perimeter-based polarity, using the\n",
    "    SAME resize_and_pad as the image to guarantee geometric alignment.\n",
    "    \"\"\"\n",
    "    if mask_aligned_np is None:\n",
    "        return None\n",
    "\n",
    "    # 1) pad the mask to 1024 with the SAME routine as the image\n",
    "    mL = Image.fromarray(mask_aligned_np, mode=\"L\")\n",
    "    m1024L = resize_and_pad(mL, target_size=1024).convert(\"L\")\n",
    "    m1024 = (np.array(m1024L) > 0)\n",
    "\n",
    "    # 2) Perimeter-majority: which value dominates the border?\n",
    "    h, w = m1024.shape\n",
    "    b = max(1, int(border_sample_px))\n",
    "    perim = np.concatenate([m1024[0:b,:].ravel(), m1024[h-b:h,:].ravel(),\n",
    "                            m1024[:,0:b].ravel(), m1024[:,w-b:w].ravel()])\n",
    "    ones = int(perim.sum()); zeros = int(perim.size - perim.sum())\n",
    "    background_is_true = (ones >= zeros)   # majority on border = background\n",
    "    inside = (~m1024) if background_is_true else m1024\n",
    "\n",
    "    # 3) Moprhology for robust gating\n",
    "    if erode_px > 0:\n",
    "        k_e = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erode_px*2+1, erode_px*2+1))\n",
    "        inside = cv2.erode(inside.astype(np.uint8), k_e, 1).astype(bool)\n",
    "    if dilate_px > 0:\n",
    "        k_d = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_px*2+1, dilate_px*2+1))\n",
    "        inside = cv2.dilate(inside.astype(np.uint8), k_d, 1).astype(bool)\n",
    "\n",
    "    if debug:\n",
    "        bg_txt = \"white/True\" if background_is_true else \"black/False\"\n",
    "        cov = float(inside.mean())\n",
    "        print(f\"[mask1024] perimeter True={ones} False={zeros} \u2192 background={bg_txt}; inside_cov={cov:.3f}\")\n",
    "\n",
    "    return inside\n",
    "\n",
    "\n",
    "def detect_detail_topk7(image_pil: Image.Image,\n",
    "                        detail_type: str,\n",
    "                        *,\n",
    "                        source_prior: dict | None,\n",
    "                        restrict_mask,                 # EITHER aligned np.ndarray OR (mask_full_np, \"FULL\")\n",
    "                        crop_box_on_full=None,         # required if restrict_mask is (\"FULL\")\n",
    "                        threshold: float = 0.05,\n",
    "                        TOP_K: int = 7,\n",
    "                        mirror_ok: bool = True,\n",
    "                        # light mask gates\n",
    "                        min_inside_frac: float = 0.30,\n",
    "                        center_must_be_inside: bool = True,\n",
    "                        erode_px: int = 1,\n",
    "                        dilate_px: int = 2,\n",
    "                        border_sample_px: int = 2,\n",
    "                        # scoring weights\n",
    "                        w_spatial: float = 0.65,\n",
    "                        w_score: float = 0.35,\n",
    "                        rank_weights: list[float] = None,\n",
    "                        debug: bool = False,\n",
    "                        viz: bool = False,\n",
    "                        viz_overlay_mask: bool = True):\n",
    "    \"\"\"\n",
    "    Robust top-7 re-ranking using SAM3 detections and optional mask gates.\n",
    "    Returns (xyxy_on_image, raw_score, mask_on_image or None)\n",
    "    \"\"\"\n",
    "    if rank_weights is None:\n",
    "        rank_weights = [0.92, 1.00, 0.98, 0.97, 0.96, 0.955, 0.95]\n",
    "\n",
    "    W0, H0 = image_pil.size\n",
    "    prompt = (detail_type or \"\").strip() + \".\"\n",
    "\n",
    "    mask_aligned = _ensure_mask_for_image(restrict_mask, image_pil, crop_box_on_full=crop_box_on_full)\n",
    "    mask_bool = (mask_aligned > 0) if mask_aligned is not None else None\n",
    "\n",
    "    preds = _sam3_predict_text(image_pil, prompt, max_dets=max(TOP_K * 3, 12), score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return (None, None, None)\n",
    "\n",
    "    picked = []\n",
    "    for rank, p in enumerate(preds):\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], W0, H0)\n",
    "        x1, y1, x2, y2 = box\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        inside_frac = 1.0\n",
    "        center_ok = True\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[y1:y2, x1:x2]\n",
    "            area = max(1, (x2 - x1) * (y2 - y1))\n",
    "            inside_frac = float(crop.sum()) / float(area)\n",
    "            cxp, cyp = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            center_ok = (0 <= cxp < W0 and 0 <= cyp < H0 and bool(mask_bool[cyp, cxp]))\n",
    "            if inside_frac < min_inside_frac or (center_must_be_inside and not center_ok):\n",
    "                continue\n",
    "\n",
    "        picked.append({\n",
    "            \"box\": box,\n",
    "            \"score\": float(p[\"score\"]),\n",
    "            \"rank\": rank,\n",
    "            \"mask\": p[\"mask\"],\n",
    "            \"inside_frac\": inside_frac,\n",
    "        })\n",
    "        if len(picked) >= TOP_K:\n",
    "            break\n",
    "\n",
    "    if not picked:\n",
    "        base = preds[0]\n",
    "        picked = [{\"box\": _clip_box_to_image(base[\"box\"], W0, H0),\n",
    "                   \"score\": float(base[\"score\"]),\n",
    "                   \"rank\": 0,\n",
    "                   \"mask\": base[\"mask\"],\n",
    "                   \"inside_frac\": 0.0}]\n",
    "\n",
    "    s = np.array([p[\"score\"] for p in picked], dtype=np.float32)\n",
    "    s_min, s_max = float(s.min()), float(s.max())\n",
    "    s_norm = np.ones_like(s) * 0.5 if s_max == s_min else (s - s_min) / (s_max - s_min)\n",
    "\n",
    "    best = None\n",
    "    for j, p in enumerate(picked):\n",
    "        rw = rank_weights[p[\"rank\"]] if p[\"rank\"] < len(rank_weights) else rank_weights[-1]\n",
    "        score_normed = float(s_norm[j] * rw)\n",
    "        x1, y1, x2, y2 = p[\"box\"]\n",
    "        area = max(1, (x2 - x1) * (y2 - y1))\n",
    "        cx_n = ((x1 + x2) / 2.0) / max(1.0, W0)\n",
    "        cy_n = ((y1 + y2) / 2.0) / max(1.0, H0)\n",
    "        area_n = area / float(max(1, W0 * H0))\n",
    "        spatial = _spatial_affinity(cx_n, cy_n, area_n, source_prior, mirror_ok=mirror_ok) if source_prior else 0.0\n",
    "        combo = w_spatial * spatial + w_score * score_normed\n",
    "        if best is None or combo > best[\"combo\"]:\n",
    "            best = {**p, \"combo\": float(combo), \"spatial\": float(spatial), \"score_norm\": score_normed}\n",
    "\n",
    "    return best[\"box\"], best[\"score\"], best[\"mask\"]\n",
    "\n",
    "\n",
    "def detect_detail(image_pil: Image.Image,\n",
    "                  detail_type: str,\n",
    "                  threshold: float = 0.05,\n",
    "                  used_boxes=None,\n",
    "                  keep_best: bool = False,\n",
    "                  iou_thr: float = 0.35,\n",
    "                  restrict_mask: np.ndarray | None = None,\n",
    "                  min_inside_frac: float = 0.40,\n",
    "                  max_outside_frac: float = 0.70,\n",
    "                  center_must_be_inside: bool = True,\n",
    "                  erode_px: int = 1,\n",
    "                  dilate_px: int = 2,\n",
    "                  border_sample_px: int = 2,\n",
    "                  debug: bool = False,\n",
    "                  debug_topk: int = 5,\n",
    "                  crop_box_on_full=None):\n",
    "    \"\"\"\n",
    "    Simplified detail locator using SAM3 text grounding.\n",
    "    Returns: (xyxy_on_image, score, mask_on_image)\n",
    "    \"\"\"\n",
    "    used_boxes = used_boxes or []\n",
    "    prompt = (detail_type or \"\").strip() + \".\"\n",
    "    W, H = image_pil.size\n",
    "\n",
    "    mask_aligned = _ensure_mask_for_image(restrict_mask, image_pil, crop_box_on_full=crop_box_on_full)\n",
    "    mask_bool = (mask_aligned > 0) if mask_aligned is not None else None\n",
    "\n",
    "    preds = _sam3_predict_text(image_pil, prompt, max_dets=10, score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return (None, None, None)\n",
    "\n",
    "    def _iou(a, b):\n",
    "        ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "        xi1, yi1 = max(ax1, bx1), max(ay1, by1)\n",
    "        xi2, yi2 = min(ax2, bx2), min(ay2, by2)\n",
    "        iw, ih = max(0, xi2 - xi1), max(0, yi2 - yi1)\n",
    "        inter = iw * ih\n",
    "        if inter == 0:\n",
    "            return 0.0\n",
    "        area_a = max(1, (ax2 - ax1) * (ay2 - ay1))\n",
    "        area_b = max(1, (bx2 - bx1) * (by2 - by1))\n",
    "        union = area_a + area_b - inter\n",
    "        return inter / union\n",
    "\n",
    "    best = None\n",
    "    debug_rows = []\n",
    "    for p in preds:\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], W, H)\n",
    "        x1, y1, x2, y2 = box\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        if any(_iou(box, ub) > iou_thr for ub in used_boxes):\n",
    "            continue\n",
    "\n",
    "        inside_frac = 1.0\n",
    "        outside_frac = 0.0\n",
    "        center_ok = True\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[y1:y2, x1:x2]\n",
    "            area = max(1, (x2 - x1) * (y2 - y1))\n",
    "            inside_frac = float(crop.sum()) / float(area)\n",
    "            outside_frac = 1.0 - inside_frac\n",
    "            cx_i, cy_i = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            center_ok = (0 <= cx_i < W and 0 <= cy_i < H and bool(mask_bool[cy_i, cx_i])) if center_must_be_inside else True\n",
    "            if not center_ok or inside_frac < min_inside_frac or outside_frac > max_outside_frac:\n",
    "                continue\n",
    "\n",
    "        if best is None or p[\"score\"] > best[\"score\"]:\n",
    "            best = {\"box\": box, \"score\": float(p[\"score\"]), \"mask\": p[\"mask\"], \"inside_frac\": inside_frac}\n",
    "\n",
    "        if debug and len(debug_rows) < debug_topk:\n",
    "            debug_rows.append({\n",
    "                \"score\": float(p[\"score\"]),\n",
    "                \"box\": box,\n",
    "                \"inside\": inside_frac,\n",
    "                \"outside\": outside_frac,\n",
    "                \"center\": center_ok,\n",
    "            })\n",
    "\n",
    "    if best is None:\n",
    "        if keep_best:\n",
    "            base = preds[0]\n",
    "            best = {\"box\": _clip_box_to_image(base[\"box\"], W, H), \"score\": float(base[\"score\"]), \"mask\": base[\"mask\"], \"inside_frac\": 0.0}\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"[detect_detail] No candidate satisfied mask gates.\")\n",
    "            return (None, None, None)\n",
    "\n",
    "    if debug and debug_rows:\n",
    "        print(\"[detect_detail/debug] top candidates (after score>thr):\")\n",
    "        for row in sorted(debug_rows, key=lambda r: r[\"score\"], reverse=True):\n",
    "            print(f\"  score={row['score']:.3f} inside={row['inside']:.2f} outside={row['outside']:.2f} center={row['center']} box={row['box']}\")\n",
    "\n",
    "    return best[\"box\"], best[\"score\"], best[\"mask\"]\n",
    "\n",
    "\n",
    "def detect_garment_box(img: Image.Image, garment_tag: str, threshold=0.25, restrict_mask: np.ndarray | None = None):\n",
    "    O_W, O_H = img.size\n",
    "    if restrict_mask is not None:\n",
    "        m1024 = resize_and_pad(Image.fromarray(restrict_mask, 'L'), 1024).convert('L')\n",
    "        mask_1024_np = (np.array(m1024) > 127)\n",
    "        ys, xs = np.where(mask_1024_np > 0)\n",
    "        if xs.size == 0 or ys.size == 0:\n",
    "            return None\n",
    "        x1, y1, x2, y2 = [float(xs.min()), float(ys.min()), float(xs.max()), float(ys.max())]\n",
    "        return box_1024_to_original([x1, y1, x2, y2], O_W, O_H)\n",
    "\n",
    "    preds = _sam3_predict_text(img, f\"{garment_tag.strip()} .\", max_dets=6, score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return None\n",
    "\n",
    "    mask_bool = (restrict_mask > 0) if restrict_mask is not None else None\n",
    "    best = None\n",
    "    for p in preds:\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], O_W, O_H)\n",
    "        if box[2] <= box[0] or box[3] <= box[1]:\n",
    "            continue\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[box[1]:box[3], box[0]:box[2]]\n",
    "            if crop.size == 0 or float(crop.mean()) < 0.05:\n",
    "                continue\n",
    "        if best is None or p[\"score\"] > best[\"score\"]:\n",
    "            best = {\"box\": box, \"score\": float(p[\"score\"])}\n",
    "\n",
    "    return best[\"box\"] if best else None\n",
    "\n",
    "\n",
    "def bbox_to_mask(bb, img_size, pad_px=10):\n",
    "    W, H = img_size\n",
    "    x1, y1, x2, y2 = bb\n",
    "    x1 = max(0, x1 - pad_px); y1 = max(0, y1 - pad_px)\n",
    "    x2 = min(W - 1, x2 + pad_px); y2 = min(H - 1, y2 + pad_px)\n",
    "    m = np.zeros((H, W), np.uint8)\n",
    "    m[y1:y2, x1:x2] = 255\n",
    "    return m\n",
    "\n",
    "\n",
    "def crop_detail(image_pil, mask_np, bb_xyxy, out_size=1024, pad_px=20):\n",
    "    W, H = image_pil.size\n",
    "    x1, y1, x2, y2 = bb_xyxy\n",
    "    x1 = max(0, x1 - pad_px); y1 = max(0, y1 - pad_px)\n",
    "    x2 = min(W, x2 + pad_px); y2 = min(H, y2 + pad_px)\n",
    "    side = max(x2 - x1, y2 - y1)\n",
    "    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    lx = max(0, cx - side // 2); rx = lx + side\n",
    "    ty = max(0, cy - side // 2); by = ty + side\n",
    "    if rx > W:\n",
    "        lx -= (rx - W); rx = W\n",
    "    if by > H:\n",
    "        ty -= (by - H); by = H\n",
    "    crop_box = (lx, ty, rx, by)\n",
    "    img_c = image_pil.crop(crop_box).resize((out_size, out_size), Image.Resampling.LANCZOS)\n",
    "    m_c = mask_np[ty:by, lx:rx]\n",
    "    m_c = cv2.resize(m_c, (out_size, out_size), interpolation=cv2.INTER_NEAREST)\n",
    "    return img_c, m_c, crop_box\n",
    "\n",
    "\n",
    "def adaptive_brightness(img, strength_dark=0.15, strength_light=0.03, clip=(0, 245)):\n",
    "    a = np.asarray(img).astype(np.float32)\n",
    "    lum = 0.2126 * a[..., 0] + 0.7152 * a[..., 1] + 0.0722 * a[..., 2]\n",
    "    mean_lum = float(lum.mean() / 255.0)\n",
    "    if mean_lum < 0.5:\n",
    "        factor = 1 + (-strength_dark) * (0.5 - mean_lum) * 2\n",
    "    else:\n",
    "        factor = 1 + (strength_light) * (mean_lum - 0.5) * 2\n",
    "    out = np.clip(a * factor, *clip).astype(np.uint8)\n",
    "    return Image.fromarray(out)\n",
    "\n",
    "\n",
    "def paste_crop_back(full_img: Image.Image, edited_crop: Image.Image, crop_box, crop_mask: np.ndarray,\n",
    "                    expand_px=20, feather_px=10) -> Image.Image:\n",
    "    edited_crop = adaptive_brightness(edited_crop, strength_dark=0.15, strength_light=0.03)\n",
    "    x1, y1, x2, y2 = crop_box\n",
    "    tgt_w, tgt_h = x2 - x1, y2 - y1\n",
    "    edit_rs = edited_crop.resize((tgt_w, tgt_h), Image.Resampling.LANCZOS)\n",
    "    mask_np = cv2.resize(crop_mask, (tgt_w, tgt_h), interpolation=cv2.INTER_NEAREST)\n",
    "    bin_mask = (mask_np > 0).astype(np.uint8)\n",
    "    if expand_px > 0:\n",
    "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (expand_px * 2 + 1, expand_px * 2 + 1))\n",
    "        bin_mask = cv2.dilate(bin_mask, k, iterations=1)\n",
    "    alpha = cv2.GaussianBlur(bin_mask.astype(np.float32) * 255, (0, 0), sigmaX=feather_px, sigmaY=feather_px)\n",
    "    alpha[mask_np > 0] = 255\n",
    "    alpha = alpha.clip(0, 255).astype(np.uint8)\n",
    "    mask_img = Image.fromarray(alpha)\n",
    "\n",
    "    region = full_img.crop((x1, y1, x2, y2))\n",
    "    comp = Image.composite(edit_rs, region, mask_img)\n",
    "    full_img.paste(comp, (x1, y1))\n",
    "    return full_img\n"
   ],
   "metadata": {
    "id": "9TD4abQU5yio"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalization for detail types from legacy names\n",
    "def _normalize_detail_type(t: str) -> str:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    mapping = {\n",
    "        \"waistband lettering\": \"waist text\",\n",
    "        \"sleeve lettering\": \"sleeve text\",\n",
    "        \"sleeve_text\": \"sleeve text\",\n",
    "        \"waist_text\": \"waist text\",\n",
    "    }\n",
    "    return mapping.get(t, t)\n",
    "\n",
    "def _postprocess_details(payload: dict) -> dict:\n",
    "    details = payload.get(\"details\", [])\n",
    "    fixed = []\n",
    "    for d in details:\n",
    "        typ = _normalize_detail_type(d.get(\"type\"))\n",
    "        col = d.get(\"color\")\n",
    "        if typ in ALLOWED_DETAIL_TYPES:\n",
    "            ent = {\"type\": typ}\n",
    "            if typ != \"sleeve text\" and isinstance(col, str) and col.strip():\n",
    "                ent[\"color\"] = col.strip()\n",
    "            fixed.append(ent)\n",
    "    return {\"details\": fixed}\n",
    "\n",
    "def _try_parse_json(s: str) -> dict | None:\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"details\" in obj:\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # try to extract first {...}\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            if isinstance(obj, dict) and \"details\" in obj:\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def read_details_from_metadata(img_path: str) -> dict:\n",
    "    \"\"\"Return {'details':[...]} or {'details':[{'type':'logo'}]} if metadata not found.\"\"\"\n",
    "    try:\n",
    "        im = Image.open(img_path)\n",
    "        # 1) PNG/JPEG info dict\n",
    "        for k, v in (im.info or {}).items():\n",
    "            if isinstance(v, str):\n",
    "                obj = _try_parse_json(v)\n",
    "                if obj:\n",
    "                    return _postprocess_details(obj)\n",
    "        # 2) EXIF: UserComment / XPComment\n",
    "        try:\n",
    "            exif_dict = piexif.load(im.info.get(\"exif\", b\"\") or im.tobytes())\n",
    "        except Exception:\n",
    "            exif_dict = None\n",
    "\n",
    "        def _decode_uc(x):\n",
    "            if isinstance(x, bytes):\n",
    "                for head in [b\"ASCII\\0\\0\\0\", b\"UNICODE\\0\", b\"JIS\\0\\0\\0\"]:\n",
    "                    if x.startswith(head):\n",
    "                        x = x[len(head):]\n",
    "                try:\n",
    "                    return x.decode(\"utf-8\", \"ignore\")\n",
    "                except Exception:\n",
    "                    return x.decode(\"latin-1\", \"ignore\")\n",
    "            if isinstance(x, str):\n",
    "                return x\n",
    "            return None\n",
    "\n",
    "        if exif_dict:\n",
    "            uc = exif_dict.get(\"Exif\", {}).get(piexif.ExifIFD.UserComment, None)\n",
    "            s = _decode_uc(uc)\n",
    "            if s:\n",
    "                obj = _try_parse_json(s)\n",
    "                if obj:\n",
    "                    return _postprocess_details(obj)\n",
    "            xp = exif_dict.get(\"0th\", {}).get(0x9C9C, None)\n",
    "            if xp:\n",
    "                try:\n",
    "                    s = bytes(xp).decode(\"utf-16le\", \"ignore\").rstrip(\"\\x00\")\n",
    "                    obj = _try_parse_json(s)\n",
    "                    if obj:\n",
    "                        return _postprocess_details(obj)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # 3) XMP sidecar embedded?\n",
    "        if \"XML:com.adobe.xmp\" in (im.info or {}):\n",
    "            obj = _try_parse_json(im.info[\"XML:com.adobe.xmp\"])\n",
    "            if obj:\n",
    "                return _postprocess_details(obj)\n",
    "        # 4) Optional sidecar .json next to image\n",
    "        side = Path(img_path).with_suffix(\".json\")\n",
    "        if side.exists():\n",
    "            try:\n",
    "                obj = json.loads(side.read_text())\n",
    "                if \"details\" in obj:\n",
    "                    return _postprocess_details(obj)\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f metadata read failed for {img_path}: {e}\")\n",
    "\n",
    "    # \ud83d\udc47 Fallback when nothing found\n",
    "    return {\"details\": [{\"type\": \"logo\"}]}\n",
    "\n",
    "\n",
    "# Garment type inference (from path)\n",
    "def extract_garment_type_from_path(image_path: str, allowed_types=ALLOWED_GARMENT_TYPES) -> str:\n",
    "    from pathlib import Path as _P\n",
    "    import re\n",
    "    def singularize(s):\n",
    "        if len(s)>4:\n",
    "            if s.endswith(\"es\"): return s[:-2]\n",
    "            if s.endswith(\"s\"):  return s[:-1]\n",
    "        return s\n",
    "    def normalize_key(s): return singularize(s.replace(\"-\",\"\").replace(\"_\",\"\").lower().strip())\n",
    "    norm_map = {}\n",
    "    for t in allowed_types:\n",
    "        base = normalize_key(t)\n",
    "        norm_map[base]=t\n",
    "        if not base.endswith(\"s\"): norm_map[base+\"s\"]=t\n",
    "        else:\n",
    "            if base.endswith(\"es\"): norm_map[base[:-2]]=t\n",
    "            else: norm_map[base[:-1]]=t\n",
    "    p = _P(image_path)\n",
    "    file_compact = re.sub(r\"[^a-z]+\",\"\", p.stem.lower())\n",
    "    for k,v in norm_map.items():\n",
    "        if k and k in file_compact: return v\n",
    "    # parent folders\n",
    "    for part in reversed(p.parts[:-1]):\n",
    "        if part.startswith(\".\"): continue\n",
    "        toks = [singularize(t) for t in re.split(r\"[^a-z]+\", part.lower()) if t]\n",
    "        for tok in toks:\n",
    "            if tok in norm_map: return norm_map[tok]\n",
    "    return \"\"\n"
   ],
   "metadata": {
    "id": "DL1y4tUs51S_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# === Robust SKU+angle parsing (handles: \"SS-28623_fr (1).png\", \"Copy of SS-12345_bc_lft_v2.png\", \"SS-55555_fr_cl.png\") ===\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "# Reuse your global config: BASE_NAMES, ACCEPTABLE_SUFFIXES, VALID_EXTS, WORKING_DIR\n",
    "\n",
    "_SKU_RE = re.compile(r\"(SS-\\d{3,7})\", re.IGNORECASE)\n",
    "_COPY_RE = re.compile(r\"^(?:copy of\\s+)+\", re.IGNORECASE)\n",
    "\n",
    "def _strip_copy_prefix(s: str) -> str:\n",
    "    return _COPY_RE.sub(\"\", s).strip()\n",
    "\n",
    "def _angle_tokens_desc() -> list[str]:\n",
    "    # Longest-first to prefer 'fr_rght' over 'fr'\n",
    "    return sorted(list(set(BASE_NAMES)), key=len, reverse=True)\n",
    "\n",
    "def _token_delim_search(token: str, text: str) -> re.Match | None:\n",
    "    \"\"\"\n",
    "    Find token delimited by non-alphanumerics (underscore is allowed as a delimiter).\n",
    "    We treat [A-Za-z0-9] as 'wordy'; underscores/spaces/()/- etc. are delimiters.\n",
    "    \"\"\"\n",
    "    # Escape underscores in token for regex\n",
    "    tok = re.escape(token)\n",
    "    pattern = rf\"(?<![A-Za-z0-9]){tok}(?![A-Za-z0-9])\"\n",
    "    return re.search(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "def extract_sku_and_angle_from_path(path_like: str) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    Returns (SKU like 'SS-12345', angle_base like 'fr_lft'/'fr').\n",
    "    Strategy:\n",
    "      1) Extract SKU from filename; if not found, try parent dirs.\n",
    "      2) After SKU in the filename, scan the suffix for the LONGEST valid angle token.\n",
    "      3) Fallback to whole filename scan, then parent dirs.\n",
    "    \"\"\"\n",
    "    p = Path(path_like)\n",
    "    name = _strip_copy_prefix(p.name)\n",
    "\n",
    "    # --- 1) SKU from filename, else parents\n",
    "    m = _SKU_RE.search(name)\n",
    "    sku = m.group(1).upper() if m else None\n",
    "    if sku is None:\n",
    "        for part in reversed(p.parts):\n",
    "            mm = _SKU_RE.search(part)\n",
    "            if mm:\n",
    "                sku = mm.group(1).upper()\n",
    "                break\n",
    "\n",
    "    # --- 2) Angle after SKU region\n",
    "    angle = None\n",
    "    tokens = _angle_tokens_desc()\n",
    "    if sku:\n",
    "        mname = _SKU_RE.search(name)\n",
    "        if mname:\n",
    "            suffix = name[mname.end():]  # everything after the SKU\n",
    "            for tok in tokens:\n",
    "                if _token_delim_search(tok, suffix):\n",
    "                    angle = tok\n",
    "                    break\n",
    "\n",
    "    # --- 3) Fallback: whole filename, then parents\n",
    "    if angle is None:\n",
    "        for tok in tokens:\n",
    "            if _token_delim_search(tok, name):\n",
    "                angle = tok\n",
    "                break\n",
    "    if angle is None:\n",
    "        # Look in parent folders\n",
    "        for part in reversed(p.parts[:-1]):\n",
    "            part_clean = _strip_copy_prefix(part)\n",
    "            for tok in tokens:\n",
    "                if _token_delim_search(tok, part_clean):\n",
    "                    angle = tok\n",
    "                    break\n",
    "            if angle:\n",
    "                break\n",
    "\n",
    "    return sku, angle\n",
    "\n",
    "# ===================== Source finding via SKU folder anywhere =====================\n",
    "@lru_cache(maxsize=1024)\n",
    "def _find_sku_folder_anywhere(working_root: str, sku_name: str) -> Path | None:\n",
    "    wr = Path(working_root)\n",
    "    if not wr.exists():\n",
    "        return None\n",
    "    sku_low = sku_name.lower()\n",
    "    best: tuple[int, Path] | None = None\n",
    "    for dirpath, dirnames, _ in os.walk(wr):\n",
    "        leaf = os.path.basename(dirpath)\n",
    "        if leaf.lower() == sku_low:\n",
    "            depth = len(Path(dirpath).parts)\n",
    "            cand = Path(dirpath)\n",
    "            if best is None or depth < best[0]:\n",
    "                best = (depth, cand)\n",
    "    return best[1] if best else None\n",
    "\n",
    "\n",
    "def _list_valid_images(folder: Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Return candidate source images in `folder`, excluding:\n",
    "      - any with 'generated', 'inpainted', '_nd', '_no_details', '_processed_by_detailer_'\n",
    "      - any with '_sec' anywhere in the filename (case-insensitive)\n",
    "    \"\"\"\n",
    "    deny_substrings = (\n",
    "        \"generated\",\n",
    "        \"inpainted\",\n",
    "        \"_nd\",\n",
    "        \"_no_details\",\n",
    "        \"_processed_by_detailer_\",\n",
    "        \"_sec\",   # \u2190 NEW: ignore secondary variants\n",
    "    )\n",
    "    out = []\n",
    "    for p in folder.iterdir():\n",
    "        if not (p.is_file() and p.suffix in VALID_EXTS):\n",
    "            continue\n",
    "        name_low = p.name.lower()\n",
    "        if any(s in name_low for s in deny_substrings):\n",
    "            continue\n",
    "        out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _rank_exact_angle(norm_stem: str, base: str, acceptable_suffixes: set[str]) -> int | None:\n",
    "    if norm_stem == f\"{base}_cut\": return 1\n",
    "    if norm_stem.startswith(base + \"_\") and norm_stem.endswith(\"_cut\"): return 2\n",
    "    if norm_stem == base: return 3\n",
    "    if norm_stem.startswith(base + \"_\"):\n",
    "        suf = norm_stem[len(base)+1:]\n",
    "        if suf in acceptable_suffixes: return 4\n",
    "    return None\n",
    "\n",
    "def _is_fr_family(base: str | None) -> bool:\n",
    "    if not base: return False\n",
    "    return base in (\"fr\",\"fr_cl\",\"fr_lft\",\"fr_rght\") or base.startswith(\"fr\")\n",
    "\n",
    "def _pick_source_in_dir(angle_base: str, directory: Path) -> Path | None:\n",
    "    entries = _list_valid_images(directory)\n",
    "    if not entries: return None\n",
    "    acceptable = set(ACCEPTABLE_SUFFIXES)\n",
    "\n",
    "    def _norm(p: Path) -> str:\n",
    "        return _strip_copy_prefix(p.stem).lower()\n",
    "\n",
    "    ranked: list[tuple[int,int,Path]] = []\n",
    "    if _is_fr_family(angle_base):\n",
    "        for p in entries:\n",
    "            n = _norm(p)\n",
    "            if n == \"fr_cut\": ranked.append((1,len(p.name),p)); continue\n",
    "            if n.startswith(\"fr_\") and n.endswith(\"_cut\"): ranked.append((2,len(p.name),p)); continue\n",
    "            if n == \"fr\": ranked.append((3,len(p.name),p)); continue\n",
    "            if n.startswith(\"fr_\"):\n",
    "                suf = n[len(\"fr_\"):]\n",
    "                if suf in acceptable: ranked.append((4,len(p.name),p)); continue\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        ranked=[]\n",
    "        for p in entries:\n",
    "            n=_norm(p)\n",
    "            r=_rank_exact_angle(n, angle_base, acceptable)\n",
    "            if r is not None: ranked.append((r,len(p.name),p))\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        return None\n",
    "    else:\n",
    "        for p in entries:\n",
    "            n=_norm(p)\n",
    "            r=_rank_exact_angle(n, angle_base, acceptable)\n",
    "            if r is not None: ranked.append((r,len(p.name),p))\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        return None\n",
    "\n",
    "def find_source_via_sku(gen_path: Path | str, working_root: Path | str) -> Path | None:\n",
    "    gen_path = Path(gen_path)\n",
    "    sku, angle_base = extract_sku_and_angle_from_path(str(gen_path))\n",
    "\n",
    "    if not sku:\n",
    "        print(f\"\u274c Could not extract SKU from: {gen_path.name}\")\n",
    "        return None\n",
    "\n",
    "    if not angle_base:\n",
    "        # No noisy warning anymore; we\u2019ll gracefully default.\n",
    "        angle_base = \"fr\"\n",
    "\n",
    "    sku_dir = _find_sku_folder_anywhere(str(working_root), sku)\n",
    "    if not sku_dir:\n",
    "        print(f\"\u274c SKU folder '{sku}' not found anywhere under {working_root}\")\n",
    "        return None\n",
    "\n",
    "    ricardo = sku_dir / \"Ricardo\"\n",
    "    for d in (ricardo, sku_dir):\n",
    "        if d.exists() and d.is_dir():\n",
    "            hit = _pick_source_in_dir(angle_base, d)\n",
    "            if hit: return hit\n",
    "\n",
    "    print(f\"\u26a0\ufe0f No suitable source found in '{sku_dir}' (Ricardo or root) for angle '{angle_base}'\")\n",
    "    return None\n",
    "\n",
    "def build_inpaint_suffix(details: list[dict]) -> str:\n",
    "    def slug(s: str) -> str:\n",
    "        s = s.lower().replace(\" \", \"-\")\n",
    "        return re.sub(r\"[^a-z0-9\\-]+\", \"\", s).strip(\"-\")\n",
    "    parts=[]\n",
    "    for d in details:\n",
    "        t = d[\"type\"]\n",
    "        c = d.get(\"color\",\"\")\n",
    "        if t != \"sleeve text\" and c:\n",
    "            parts.append(slug(f\"{c}-{t}\"))\n",
    "        else:\n",
    "            parts.append(slug(t))\n",
    "    return \"_\".join(parts) if parts else \"none\"\n",
    "\n",
    "# --- Build the required base \"SS-12345-bc_lft\" from the queued filename ---\n",
    "def build_out_base_from_gen(gen_path: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Returns (sku_upper, angle_lower, out_base).\n",
    "    out_base is 'SS-12345-bc_lft' (SKU + '-' + angle).\n",
    "    \"\"\"\n",
    "    sku, angle = extract_sku_and_angle_from_path(gen_path)\n",
    "    if not sku:\n",
    "        raise ValueError(f\"Cannot derive SKU from: {gen_path}\")\n",
    "    if not angle:\n",
    "        angle = \"fr\"\n",
    "    sku_up = sku.upper()\n",
    "    angle_lo = angle.lower()\n",
    "    return sku_up, angle_lo, f\"{sku_up}-{angle_lo}\"\n",
    "\n",
    "def target_already_has_inpainted(target_dir: str, sku: str, angle: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check TARGET_DIR for any file starting with 'SS-12345-bc_lft_inpainted'.\n",
    "    Case-insensitive; extension-agnostic.\n",
    "    \"\"\"\n",
    "    td = Path(target_dir)\n",
    "    if not td.exists():\n",
    "        return False\n",
    "    prefix = f\"{sku.upper()}-{angle.lower()}_inpainted\"\n",
    "    prefix_low = prefix.lower()\n",
    "    for p in td.iterdir():\n",
    "        if p.is_file() and p.suffix in VALID_EXTS:\n",
    "            if p.stem.lower().startswith(prefix_low):\n",
    "                return True\n",
    "    return False"
   ],
   "metadata": {
    "id": "pbCd-SBI530x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def _inpaint_one_detail(gen_full: Image.Image,\n",
    "                        src_full: Image.Image,\n",
    "                        detail_prompt: str,\n",
    "                        *,\n",
    "                        garment_tag: str,\n",
    "                        restrict_mask_full: np.ndarray | None,\n",
    "                        generous_pad_px: int,\n",
    "                        tiny_pad_px: int,\n",
    "                        seed: int,\n",
    "                        visualize: bool) -> Image.Image:\n",
    "\n",
    "    gen_view_for_sam3 = apply_binary_mask(gen_full, restrict_mask_full) if restrict_mask_full is not None else gen_full\n",
    "\n",
    "    gar_src_bb = detect_garment_box(src_full, garment_tag)\n",
    "    gar_gen_bb = detect_garment_box(gen_view_for_sam3, garment_tag, restrict_mask=restrict_mask_full)\n",
    "    if gar_src_bb is None or gar_gen_bb is None:\n",
    "        print(\"\u274c garment detection failed\"); return gen_full\n",
    "\n",
    "    # square garment crops\n",
    "    def crop_to_square(image: Image.Image, bbox, pad_px=0):\n",
    "        x1,y1,x2,y2 = bbox\n",
    "        w,h = x2-x1, y2-y1\n",
    "        side = max(w,h) + 2*pad_px\n",
    "        cx,cy = (x1+x2)//2, (y1+y2)//2\n",
    "        lx=max(0,cx-side//2); ty=max(0,cy-side//2)\n",
    "        rx=lx+side; by=ty+side\n",
    "        W,H=image.size\n",
    "        if rx>W: lx -= (rx-W); rx=W\n",
    "        if by>H: ty -= (by-H); by=H\n",
    "        crop = image.crop((max(lx,0),max(ty,0),min(rx,W),min(by,H)))\n",
    "        out  = Image.new(\"RGB\",(side,side),(255,255,255))\n",
    "        dx=max(0,-lx); dy=max(0,-ty)\n",
    "        out.paste(crop,(dx,dy))\n",
    "        return out, (lx,ty,rx,by)\n",
    "\n",
    "    src_sq, sq_src = crop_to_square(src_full, gar_src_bb)\n",
    "    gen_sq, sq_gen = crop_to_square(gen_view_for_sam3, gar_gen_bb)\n",
    "\n",
    "    src_garm_sq, sq_coords_src = crop_to_square(src_full, gar_src_bb, pad_px=0)\n",
    "    gen_garm_sq, sq_coords_gen = crop_to_square(gen_view_for_sam3, gar_gen_bb, pad_px=0)\n",
    "\n",
    "    det_src_bb, _, det_src_mask_crop = detect_detail(src_sq, detail_prompt, crop_box_on_full=sq_src)\n",
    "    prior = make_spatial_prior_from_box(det_src_bb, src_garm_sq.size)\n",
    "\n",
    "    det_gen_bb, _, det_gen_mask_crop = detect_detail_topk7(\n",
    "        gen_garm_sq,\n",
    "        detail_prompt,\n",
    "        source_prior=prior,\n",
    "        restrict_mask=(restrict_mask_full, \"FULL\"),  # pass FULL mask\n",
    "        crop_box_on_full=sq_coords_gen,              # the (x1,y1,x2,y2) used to make gen_garm_sq\n",
    "        viz=False, debug=False\n",
    "    )\n",
    "    if det_src_bb is None or det_gen_bb is None:\n",
    "        print(f\"\u274c detail not found: {detail_prompt}\"); return gen_full\n",
    "\n",
    "    # back to full coords\n",
    "    lx_s, ty_s, _, _ = sq_src\n",
    "    lx_g, ty_g, _, _ = sq_gen\n",
    "    src_det_bb = [det_src_bb[0]+lx_s, det_src_bb[1]+ty_s, det_src_bb[2]+lx_s, det_src_bb[3]+ty_s]\n",
    "    gen_det_bb = [det_gen_bb[0]+lx_g, det_gen_bb[1]+ty_g, det_gen_bb[2]+lx_g, det_gen_bb[3]+ty_g]\n",
    "\n",
    "    src_mask_full = _mask_crop_to_full(det_src_mask_crop, sq_src, src_full.size) if det_src_mask_crop is not None else None\n",
    "    gen_mask_full = _mask_crop_to_full(det_gen_mask_crop, sq_coords_gen, gen_full.size) if det_gen_mask_crop is not None else None\n",
    "    if src_mask_full is None:\n",
    "        src_mask_full = bbox_to_mask(src_det_bb, src_full.size, INPAINT_TINY_PAD)\n",
    "    if gen_mask_full is None:\n",
    "        gen_mask_full = bbox_to_mask(gen_det_bb, gen_full.size, INPAINT_TINY_PAD)\n",
    "    else:\n",
    "        gen_mask_full = enlarge_mask(gen_mask_full, scale=1.05)\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"detail on source\", _draw_bbox(src_full, src_det_bb)),\n",
    "            (\"detail on generated (masked)\", _draw_bbox(gen_view_for_sam3, gen_det_bb))\n",
    "        ], cols=2, figsize=(12,8))\n",
    "\n",
    "    # crops for IA\n",
    "    src_crop, src_mask, _   = crop_detail(src_full, src_mask_full, src_det_bb, 1024, 20)\n",
    "    gen_crop, gen_mask, box = crop_detail(gen_full, gen_mask_full, gen_det_bb, 1024, INPAINT_GENEROUS_PAD)\n",
    "\n",
    "    # diptych\n",
    "    src_arr = np.array(src_crop)\n",
    "    masked_src = src_arr  # keep source unmasked for IA\n",
    "\n",
    "    gen_arr = np.array(gen_crop)\n",
    "    gen_msk3 = np.stack([gen_mask]*3, -1)\n",
    "    zeros = np.zeros_like(masked_src)\n",
    "\n",
    "    diptych = np.concatenate([masked_src, gen_arr], axis=1).astype(np.uint8)\n",
    "    dip_mask = np.concatenate([zeros, gen_msk3], axis=1).astype(np.uint8)\n",
    "    dip_mask[dip_mask>0]=255\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"diptych\", Image.fromarray(diptych)),\n",
    "            (\"diptych mask\", Image.fromarray(dip_mask).convert(\"RGB\"))\n",
    "        ], cols=2, figsize=(12,8))\n",
    "\n",
    "    prior = redux(Image.fromarray(masked_src))\n",
    "    gen_obj = torch.Generator(device).manual_seed(seed)\n",
    "    ia_out = pipe(\n",
    "        image=Image.fromarray(diptych),\n",
    "        mask_image=Image.fromarray(dip_mask),\n",
    "        height=1024,\n",
    "        width=2048,\n",
    "        max_sequence_length=512,\n",
    "        num_inference_steps=60,\n",
    "        guidance_scale=30,\n",
    "        generator=gen_obj,\n",
    "        **prior\n",
    "    ).images[0]\n",
    "\n",
    "    right_crop = ia_out.crop((1024,0,2048,1024))\n",
    "    gen_full = paste_crop_back(gen_full, right_crop, box, gen_mask)\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"IA result (2048\u00d71024)\", ia_out),\n",
    "            (\"after this detail\", gen_full)\n",
    "        ], cols=2, figsize=(14,8))\n",
    "\n",
    "    return gen_full\n",
    "\n",
    "def inpaint_with_details_list(generated_path: str,\n",
    "                              source_path: str,\n",
    "                              details: list[dict],\n",
    "                              garment_type: str | None,\n",
    "                              visualize: bool = True) -> Image.Image:\n",
    "\n",
    "    gen_full = open_upright(generated_path)\n",
    "    src_full = open_source_with_black_bg(source_path)\n",
    "\n",
    "    restrict_mask_full = load_binary_mask_for_generated(generated_path, source_path, gen_full)\n",
    "\n",
    "    if restrict_mask_full is None:\n",
    "        print(\"\u26a0\ufe0f  No garment mask found \u2014 proceeding without restriction.\")\n",
    "    else:\n",
    "        print(\"\u2705 Garment mask loaded & aligned for\", os.path.basename(generated_path))\n",
    "\n",
    "    if garment_type is None or not garment_type.strip():\n",
    "        garment_type = extract_garment_type_from_path(source_path)\n",
    "    if not garment_type:\n",
    "        garment_type = \"t-shirt\"  # conservative default prompt\n",
    "\n",
    "    # twinset (optional, keep simple)\n",
    "    garment_tags = [garment_type.lower()]\n",
    "    if garment_type.lower() in TWINSET_TYPES:\n",
    "        garment_tags = [TOP_GARMENTS[0], BOTTOM_GARMENTS[0]]\n",
    "\n",
    "    out_img = gen_full.copy()\n",
    "    for gtag in garment_tags:\n",
    "        for d in details:\n",
    "            d_type = d[\"type\"]\n",
    "            prompt_str = f\"{d_type}\".strip()\n",
    "            print(f\"\ud83d\udd04 Inpainting detail: {prompt_str}  (garment={gtag})\")\n",
    "            out_img = _inpaint_one_detail(\n",
    "                out_img, src_full, prompt_str,\n",
    "                garment_tag=gtag,\n",
    "                restrict_mask_full=restrict_mask_full,\n",
    "                generous_pad_px=INPAINT_GENEROUS_PAD,\n",
    "                tiny_pad_px=INPAINT_TINY_PAD,\n",
    "                seed=INPAINT_SEED,\n",
    "                visualize=visualize\n",
    "            )\n",
    "\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return out_img\n"
   ],
   "metadata": {
    "id": "UkjJwDwh558v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "def process_detailer_queue():\n",
    "    queue_root = Path(DETAILER_QUEUE_FOLDER)\n",
    "    if not queue_root.exists():\n",
    "        print(f\"\u274c Queue folder does not exist: {queue_root}\")\n",
    "        return\n",
    "\n",
    "    gen_files = [p for p in queue_root.rglob(\"*\") if p.is_file() and p.suffix in VALID_EXTS]\n",
    "    if not gen_files:\n",
    "        print(f\"\u2139\ufe0f No images found in {queue_root}\")\n",
    "        return\n",
    "\n",
    "    processed = skipped = failed = 0\n",
    "\n",
    "    for gen_path in sorted(gen_files, key=lambda p: (str(p.parent), p.name)):\n",
    "        try:\n",
    "            print(\"\\n\" + \"_\"*80)\n",
    "            print(f\"\ud83c\udfaf Queue item: {gen_path}\")\n",
    "\n",
    "            # 1) Read details from metadata\n",
    "            meta = read_details_from_metadata(str(gen_path))\n",
    "            if not meta or not meta.get(\"details\"):\n",
    "                print(\"\u23ed\ufe0f  No details found in metadata \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            details = [d for d in meta[\"details\"] if d[\"type\"] in ALLOWED_DETAIL_TYPES]\n",
    "            if not details:\n",
    "                print(\"\u23ed\ufe0f  Details list empty after normalization \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Find source garment near this item\n",
    "            src_p = find_source_via_sku(gen_path, Path(WORKING_DIR))\n",
    "            if not src_p:\n",
    "                print(\"\u23ed\ufe0f  Source garment not found \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            source_base = Path(src_p).stem  # e.g. SS-12345_fr\n",
    "            sku_up, angle_lo, out_base = build_out_base_from_gen(str(gen_path))\n",
    "            out_ext = \".png\"\n",
    "\n",
    "            # 3) Skip guard: any prior inpainted for this SKU+angle?\n",
    "            if SKIP_IF_ALREADY_INPAINTED and target_already_has_inpainted(TARGET_DIR, sku_up, angle_lo):\n",
    "              print(f\"\u23ed\ufe0f  Already have inpainted for {out_base} in TARGET_DIR \u2014 skipping\")\n",
    "              skipped += 1\n",
    "              continue\n",
    "\n",
    "            # 4) Inpaint\n",
    "            garment_type = extract_garment_type_from_path(str(src_p))\n",
    "            out_img = inpaint_with_details_list(\n",
    "                str(gen_path),\n",
    "                str(src_p),\n",
    "                details=details,\n",
    "                garment_type=garment_type,\n",
    "                visualize=VISUALIZE\n",
    "            )\n",
    "\n",
    "            # 5) Build output names (keep source naming; append detail suffixes)\n",
    "            suffix   = build_inpaint_suffix(details)   # unchanged\n",
    "            dst_src  = Path(TARGET_DIR) / f\"{out_base}{out_ext}\"                         # e.g., SS-12345-bc_lft.jpg\n",
    "            dst_ia   = Path(TARGET_DIR) / f\"{out_base}_inpainted_{suffix}{out_ext}\"      # e.g., SS-12345-bc_lft_inpainted_red_logo.jpg\n",
    "\n",
    "            # 6) Save outputs: copy source + save inpainted\n",
    "            #if not dst_src.exists():\n",
    "            #    shutil.copy2(str(src_p), str(dst_src))\n",
    "            #    print(f\"\ud83d\udcce Saved source \u2192 {dst_src.name}\")\n",
    "            #else:\n",
    "            #    print(f\"\ud83d\udcce Source already present in TARGET_DIR \u2192 {dst_src.name}\")\n",
    "\n",
    "            # --- Save inpainted result ---\n",
    "            out_img.save(str(dst_ia))\n",
    "            print(f\"\u2705 Saved inpainted \u2192 {dst_ia.name}\")\n",
    "            processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Failed on {gen_path.name}: {e}\")\n",
    "            failed += 1\n",
    "\n",
    "    print(\"\\n==== SUMMARY ====\")\n",
    "    print(f\"Processed: {processed}  |  Skipped: {skipped}  |  Failed: {failed}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ekO6QtZR59KR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#RUN"
   ],
   "metadata": {
    "id": "XPGXKHLhO_hS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run\n",
    "process_detailer_queue()"
   ],
   "metadata": {
    "id": "pxMzQe-U5_Om"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#UNASSIGN"
   ],
   "metadata": {
    "id": "qpoDyMhnPO83"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "metadata": {
    "id": "GUsJyqO4PN0L"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}