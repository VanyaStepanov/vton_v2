{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4KzsXGV_ee1"
   },
   "source": [
    "# INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3aXFfY54iAH"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- Setup: Google auth + Drive + NanoBanana Pro deps ---\n",
    "\n",
    "import sys, os, subprocess, textwrap, importlib\n",
    "\n",
    "from google.colab import auth, drive\n",
    "auth.authenticate_user()\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install fal-client requests\n",
    "!pip -q install --upgrade pip\n",
    "!pip install \"google-genai>=1.40.0\" pillow numpy opencv-python-headless matplotlib gspread google-auth google-auth-oauthlib google-api-python-client piexif tqdm\n",
    "\n",
    "print(\"✅ Setup done for NanoBanana Pro.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DU2Vo3gUxdl"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys, subprocess, textwrap, importlib\n",
    "import os, re, fnmatch, math, uuid, pytz, random, gc, tempfile, traceback\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umaizfLYv7ww"
   },
   "source": [
    "# Select angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4VplVzrviJT"
   },
   "outputs": [],
   "source": [
    "fr_lft = False #@param {type:\"boolean\"}\n",
    "fr_rght = False #@param {type:\"boolean\"}\n",
    "fr_cl = False #@param {type:\"boolean\"}\n",
    "bc_lft = False #@param {type:\"boolean\"}\n",
    "bc_rght = False #@param {type:\"boolean\"}\n",
    "lft = False #@param {type:\"boolean\"}\n",
    "rght = False #@param {type:\"boolean\"}\n",
    "bc_ = False #@param {type:\"boolean\"}\n",
    "fr_ = True #@param {type:\"boolean\"}\n",
    "fr_cl_btm = False #@param {type:\"boolean\"}\n",
    "fr_cl_tp = False #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "names = [\"fr_lft\",\"fr_rght\",\"fr_cl\",\"bc_lft\",\"bc_rght\",\"lft\",\"rght\",\"bc_\",\"fr_\",\"fr_cl_btm\",\"fr_cl_tp\"]\n",
    "ALLOWED_BASES = [n for n in names if locals()[n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7pcfKd0_iB_"
   },
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq87guNN-V3D"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Unified CONFIG ---\n",
    "from google.colab import userdata\n",
    "FAL_KEY = userdata.get('FAL_KEY')\n",
    "\n",
    "# Selection mode: list only\n",
    "RUN_MODE = \"sku_list\"     #@param [\"sku_list\"]\n",
    "\n",
    "# For RUN_MODE == \"sku_list\"\n",
    "SKU_CSV = \"28894\"  #@param {type:\"string\"}\n",
    "\n",
    "# Paths\n",
    "BASE_PHOTOS_ROOT  = \"/content/drive/MyDrive/Dazzl/SikSilk/SKSLK_MODELS/\"\n",
    "GARMENTS_ROOT     = \"/content/drive/MyDrive/Dazzl/SikSilk/AlexGens/SikSilk/\"\n",
    "\n",
    "\n",
    "# Filename/dir policy\n",
    "VALID_EXTENSIONS  = (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")\n",
    "IGNORE_DIRS       = {\"old\", \"__MACOSX\", \".ds_store\", \"Ricardo\", \"toweling\"}\n",
    "SKIP_FILENAME_TOKENS_CSV   = \"mask, generated, _close, _open, freelance, _backup\"   # substrings to skip\n",
    "SKIP_BASENAME_SUFFIXES_CSV = \"_sec\"                             # stem endings to skip\n",
    "REQUIRE_CUT_IN_FILENAME    = False   #@param {type:\"boolean\"}\n",
    "PREFER_AGNOSTIC_MASKS = True #@param {type:\"boolean\"}\n",
    "secondary_garment = True #@param {type:\"boolean\"}\n",
    "SECONDARY_GARMENT = secondary_garment\n",
    "\n",
    "# Cropping / paste-back (square 1:1, generous garment margin)\n",
    "CROP_PADDING      = 300        # px padding around garment when building crop\n",
    "UPPER_PADDING     = 200        # extra padding above garment\n",
    "HORIZ_PADDING     = 150        # horizontal padding\n",
    "MASK_EXPAND_PX    = 100        # outward growth before feather\n",
    "MASK_FEATHER_PX   = 30         # Gaussian sigma for feathering\n",
    "CROP_MIN_MARGIN   = 20        # minimum margin even if mask touches edge\n",
    "\n",
    "TARGET_ASPECT = (1, 1)         # enforce square crops for 1:1 generations\n",
    "\n",
    "# NanoBanana Pro (Google GenAI)\n",
    "NANOBANANA_MODEL_ID = \"gemini-3-pro-image-preview\"\n",
    "GEN_ASPECT_RATIO    = \"1:1\"\n",
    "GEN_IMAGE_SIZE      = \"4K\" #@param [\"1K\", \"2K\", \"4K\"]\n",
    "\n",
    "\n",
    "MAIN_PROMPT_INTRO = \"\"\"You are an expert virtual try-on AI. You will be given a 'model image' and a 'garment image'. Your task is to create a new photorealistic image where the person from the 'model image' is wearing the clothing from the 'garment image'.\"\"\"\n",
    "\n",
    "SECONDARY_PROMPT_INTRO = \"\"\"You are an expert virtual try-on AI. You will be given a 'model image' and a 'garment image'. Your task is to create a new photorealistic image where the person from the 'model image' is wearing the clothing from the 'garment image' as a complementary garment to their main.\"\"\"\n",
    "\n",
    "MAIN_PROMPT_RULES = [\n",
    "    \"**Complete Garment Replacement:** You MUST completely REMOVE and REPLACE the clothing item worn by the person in the 'model image' with the new garment. No part of the original clothing (e.g., collars, sleeves, patterns) should be visible in the final image.\",\n",
    "    \"**Preserve the Model:** The person's face, hair, body shape, and pose from the 'model image' MUST remain unchanged, pixel-for-pixel.\",\n",
    "    \"**Preserve the Background:** The entire background from the 'model image' MUST be preserved perfectly, pixel-for-pixel.\",\n",
    "    \"**Apply the Garment:** Realistically fit the new garment onto the person. It should adapt to their pose with natural folds, shadows, and lighting consistent with the original scene.\",\n",
    "    \"**Output:** Return ONLY the final, edited image. Do not include any text.\",\n",
    "    \"**Bespoke quality:** The garment should be ironed (if applicable), pretty and sit perfectly well — this is a professional fashion product photoshoot.\",\n",
    "]\n",
    "\n",
    "MAIN_TEXTURE_RULE = \"**Fabric Texture:** Use the provided 'Main texture reference' image to match the fabric texture, print, and sheen perfectly on the garment.\"\n",
    "\n",
    "SECONDARY_PROMPT_RULES = [\n",
    "    \"**Complete {sec_type} Garment Replacement:** You MUST completely REMOVE and REPLACE the {sec_type} clothing item worn by the person in the 'model image' with the new {sec_type} garment. No part of the original cloth (e.g., collars, sleeves, patterns) should be visible in the final image.\",\n",
    "    \"**Preserve the Model:** The person's face, hair, body shape, and pose from the 'model image' MUST remain unchanged, pixel-for-pixel.\",\n",
    "    \"**Preserve the Background:** The entire background from the 'model image' MUST be preserved perfectly, pixel-for-pixel.\",\n",
    "    \"**Apply the {sec_type} Garment:** Realistically fit the new {sec_type} garment onto the person. Even if only a small part of it is in frame. It should adapt to their pose with natural folds, shadows, and lighting consistent with the original scene.\",\n",
    "    \"**Output:** Return ONLY the final, edited image. Do not include any text.\",\n",
    "    \"**Bespoke quality:** the {sec_type} garment should be ironed (if applicable), pretty and sit perfectly well — this is a professional fashion product photoshoot.\",\n",
    "]\n",
    "\n",
    "SECONDARY_TEXTURE_RULE = \"**Fabric Texture:** Use the provided texture reference image to match the {sec_type} garment's fabric texture, print, and sheen perfectly.\"\n",
    "\n",
    "def _numbered_rules(rules):\n",
    "    return \"\\n\".join(f\"{i+1}. {rule}\" for i, rule in enumerate(rules))\n",
    "\n",
    "def build_main_prompt(include_texture: bool = False):\n",
    "    rules = list(MAIN_PROMPT_RULES)\n",
    "    if include_texture:\n",
    "        rules.insert(5, MAIN_TEXTURE_RULE)  # ensure texture guidance is #6\n",
    "    return f\"{MAIN_PROMPT_INTRO}\\n\\n**Crucial Rules:**\\n\" + _numbered_rules(rules)\n",
    "\n",
    "def build_secondary_prompt(sec_type: str, include_texture: bool = False):\n",
    "    sec = sec_type or \"secondary\"\n",
    "    rules = [r.format(sec_type=sec) for r in SECONDARY_PROMPT_RULES]\n",
    "    if include_texture:\n",
    "        rules.insert(5, SECONDARY_TEXTURE_RULE.format(sec_type=sec))\n",
    "    intro = SECONDARY_PROMPT_INTRO.format(sec_type=sec)\n",
    "    return f\"{intro}\\n\\n**Crucial Rules:**\\n\" + _numbered_rules(rules)\n",
    "\n",
    "TRYON_PROMPT = build_main_prompt(include_texture=False)\n",
    "SECONDARY_TRYON_PROMPT = build_secondary_prompt(sec_type=\"secondary\", include_texture=False)\n",
    "\n",
    "# SAM3 segmentation (fal.ai)\n",
    "FAL_SAM_MODEL_ID = \"fal-ai/sam-3/image\"\n",
    "MASK_MAX_SIZE     = 1024   # px max side sent for segmentation\n",
    "MASK_PROMPT_TEMPLATE = \"{category}\"\n",
    "\n",
    "\n",
    "# Sheet-related (Ops removed) kept only for Gen Log appends\n",
    "SPREADSHEET_ID = \"1Kbq9__sEUQiuDPuza5Xy_hRyIn8pUvmfFj6vhPBrp8Y\"\n",
    "GEN_LOG_SHEET  = \"Gen Log\"\n",
    "\n",
    "# Misc\n",
    "SHOW_VISUALS = True\n",
    "TIMEZONE     = \"Europe/Lisbon\"\n",
    "OPERATOR     = \"Ivan\"\n",
    "OUTPUT_DIR   = \"/content/drive/MyDrive/Dazzl/SikSilk/SS_OUTPUT_FOLDER/linear/\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "# === Garment/type taxonomy (kept) ===\n",
    "ALLOWED_GARMENT_TYPES = [\n",
    "    \"hoodie\",\"jeans\",\"joggers\",\"shorts\",\"sweater\",\"swimwear\",\n",
    "    \"t-shirt\",\"shirts\",\"track top\",\"trousers\",\"twinset\",\"polo\",\"vests\",\"shirts\"\n",
    "]\n",
    "TOP_GARMENTS    = [\"t-shirts\",\"shirts\",\"sweaters\",\"hoodies\",\"polos\",\"vests\"]\n",
    "BOTTOM_GARMENTS = [\"shorts\",\"joggers\",\"trousers\",\"jeans\", \"pants\", \"swimwear\"]\n",
    "TWINSET_TYPES   = [\"twinset\"]\n",
    "\n",
    "# === Details tokens ===\n",
    "ALLOWED_DETAIL_TYPES = [\"crest\",\"logo\",\"patch\"]\n",
    "\n",
    "# Angle sheet tokens (kept for compatibility, not used in v1.3)\n",
    "ANGLE_NEEDS_REGENERATE_TOKEN = \"Regenerate\"\n",
    "ENFORCE_BAN_SUBSTRINGS     = True\n",
    "BANNED_SUBSTRINGS_CSV      = \"wrong, pair, combo\"\n",
    "ENFORCE_REQUIRE_SUBSTRINGS = False\n",
    "REQUIRED_SUBSTRINGS_CSV    = \"\"\n",
    "REQUIRED_SUBSTRINGS_MODE   = \"ANY\"   # \"ANY\" | \"ALL\"\n",
    "\n",
    "def normalize_sku_list(sku_csv: str) -> str:\n",
    "    skus = []\n",
    "    for raw in sku_csv.split(','):\n",
    "        sku = raw.strip().upper()\n",
    "        match = re.search(r'(\\d+)', sku)\n",
    "        if match:\n",
    "            sku_number = match.group(1)\n",
    "            skus.append(f\"SS-{sku_number}\")\n",
    "    # Return as CSV string\n",
    "    return \", \".join(skus)\n",
    "\n",
    "SKU_CSV = normalize_sku_list(SKU_CSV)\n",
    "\n",
    "print(\"✅ Config ready for NanoBanana Pro v1.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3WTzeGw_nEv"
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAeR9BUW-8Lp"
   },
   "outputs": [],
   "source": [
    "# --- Core utilities: normalization, angles, walking, masks (agnostic-first) ---\n",
    "\n",
    "import os, re, fnmatch, math, uuid, pytz, random, gc, tempfile, traceback\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter\n",
    "\n",
    "\n",
    "\n",
    "# Parsers\n",
    "def _parse_csv_list(s):  return [x.strip().casefold() for x in (s or \"\").split(\",\") if x.strip()]\n",
    "BANNED_SUBSTRINGS       = _parse_csv_list(BANNED_SUBSTRINGS_CSV)\n",
    "REQUIRED_SUBSTRINGS     = _parse_csv_list(REQUIRED_SUBSTRINGS_CSV)\n",
    "SKIP_FILENAME_TOKENS    = set(_parse_csv_list(SKIP_FILENAME_TOKENS_CSV))\n",
    "SKIP_BASENAME_SUFFIXES  = tuple(_parse_csv_list(SKIP_BASENAME_SUFFIXES_CSV))\n",
    "\n",
    "# Normalizers\n",
    "def _norm_sku(s):\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).replace(\"\\u00A0\",\" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s.casefold()\n",
    "\n",
    "def _norm_angle(s):\n",
    "    s = (s or \"\").strip().lower()\n",
    "    return s.strip(\"_ \").replace(\"-\", \"_\")\n",
    "\n",
    "# Angle aliases\n",
    "ANGLE_ALIASES = {\n",
    "    \"fr_cl\":   [\"fr\", \"fr_\"],\n",
    "    \"fr\":      [\"fr_cl\"],\n",
    "    \"bc_lft\":  [\"bc\", \"bc_\"],\n",
    "    \"bc_rght\": [\"bc\", \"bc_\"],\n",
    "}\n",
    "\n",
    "\n",
    "# --- Helpers to keep outputs strict, sources flexible ---\n",
    "def expand_as_list(angles):\n",
    "    exp = list(expand_allowed_angles(angles))\n",
    "    exp = [_norm_angle(a) for a in exp]\n",
    "    exp.sort(key=len, reverse=True)  # prefer 'fr_cl' over 'fr'\n",
    "    return exp\n",
    "\n",
    "def pick_target_angle(source_angle: str, allowed_outputs: set) -> str | None:\n",
    "    s = _norm_angle(source_angle)\n",
    "    for target in allowed_outputs:\n",
    "        fam = {_norm_angle(x) for x in expand_allowed_angles([target])}\n",
    "        if s in fam:\n",
    "            return _norm_angle(target)\n",
    "    return None\n",
    "\n",
    "\n",
    "def expand_allowed_angles(angles):\n",
    "    expanded = set()\n",
    "    for a in (angles or []):\n",
    "        a_norm = _norm_angle(a)\n",
    "        expanded.add(a_norm)\n",
    "        for alt in ANGLE_ALIASES.get(a_norm, []):\n",
    "            expanded.add(_norm_angle(alt))\n",
    "    return expanded\n",
    "\n",
    "# Ignore set\n",
    "IGNORE_DIRS = {d.lower() for d in IGNORE_DIRS}\n",
    "\n",
    "# Walkers\n",
    "def _is_sku_folder(path: str) -> bool:\n",
    "    if os.path.basename(os.path.normpath(path)).lower() in IGNORE_DIRS:\n",
    "        return False\n",
    "    try:\n",
    "        for f in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, f)) and f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)):\n",
    "                return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def iter_sku_folders(root: str):\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        dirnames[:] = [d for d in dirnames if d.lower() not in IGNORE_DIRS]\n",
    "        if any(f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)) for f in filenames):\n",
    "            yield dirpath\n",
    "\n",
    "def resolve_targets(idents_csv: str, garments_root: str):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      • Plain SKU names, relative paths (Category/Subcategory/SKU), or absolute dirs\n",
    "      • Glob patterns (e.g., 'Hoodies/*' or 'SKSLK_12*')\n",
    "      • Directories that are NOT SKU leaves → expand to all descendant SKU leaves\n",
    "    \"\"\"\n",
    "    idents = [s.strip() for s in idents_csv.replace(\"\\n\", \",\").split(\",\") if s.strip()]\n",
    "    if not idents: return [], []\n",
    "\n",
    "    all_sku_dirs = list(iter_sku_folders(garments_root))\n",
    "    rel_map = {p: os.path.relpath(p, garments_root) for p in all_sku_dirs}\n",
    "    base_map = {p: os.path.basename(p) for p in all_sku_dirs}\n",
    "\n",
    "    seen, out, unmatched = set(), [], []\n",
    "    def add_path(p):\n",
    "        ap = os.path.abspath(p)\n",
    "        if os.path.isdir(ap):\n",
    "            if _is_sku_folder(ap):\n",
    "                if ap not in seen:\n",
    "                    seen.add(ap); out.append(ap)\n",
    "            else:\n",
    "                # Expand directory to all descendant SKU leaves\n",
    "                for leaf in iter_sku_folders(ap):\n",
    "                    a = os.path.abspath(leaf)\n",
    "                    if a not in seen:\n",
    "                        seen.add(a); out.append(a)\n",
    "\n",
    "    for ident in idents:\n",
    "        before = len(out)\n",
    "        # Absolute directory or SKU path\n",
    "        if os.path.isabs(ident) and os.path.isdir(ident):\n",
    "            add_path(ident)\n",
    "\n",
    "        # Relative under garments root (dir or SKU)\n",
    "        rel_candidate = os.path.join(garments_root, ident)\n",
    "        if os.path.exists(rel_candidate):\n",
    "            add_path(rel_candidate)\n",
    "\n",
    "        # Glob/pattern over known SKU leaves (by basename or relative path)\n",
    "        for p in all_sku_dirs:\n",
    "            if fnmatch.fnmatch(base_map[p], ident) or fnmatch.fnmatch(rel_map[p], ident):\n",
    "                add_path(p)\n",
    "\n",
    "        if len(out) == before:\n",
    "            unmatched.append(ident)\n",
    "\n",
    "    out.sort()\n",
    "    return out, unmatched\n",
    "\n",
    "# Base/mask location resolution\n",
    "def resolve_base_mask_dir(sku_folder: str,\n",
    "                          garments_root: str = GARMENTS_ROOT,\n",
    "                          base_root: str = BASE_PHOTOS_ROOT):\n",
    "    \"\"\"\n",
    "    Map .../GARMENTS_ROOT/Category/Subcategory/SKU → .../BASE_ROOT/Category/Subcategory\n",
    "    With robust fallbacks.\n",
    "    \"\"\"\n",
    "    abs_sku = os.path.abspath(sku_folder)\n",
    "    abs_gar = os.path.abspath(garments_root)\n",
    "    try:\n",
    "        rel = os.path.relpath(abs_sku, abs_gar)\n",
    "    except Exception:\n",
    "        rel = None\n",
    "\n",
    "    if rel and not rel.startswith(\"..\"):\n",
    "        rel_parent = os.path.dirname(rel)\n",
    "        cand = os.path.join(base_root, rel_parent)\n",
    "        if os.path.isdir(cand): return cand\n",
    "\n",
    "    subcat = os.path.basename(os.path.dirname(abs_sku))\n",
    "    cat    = os.path.basename(os.path.dirname(os.path.dirname(abs_sku)))\n",
    "    cand2  = os.path.join(base_root, cat, subcat)\n",
    "    if os.path.isdir(cand2): return cand2\n",
    "\n",
    "    cand3  = os.path.join(base_root, subcat)\n",
    "    if os.path.isdir(cand3): return cand3\n",
    "    return None\n",
    "\n",
    "def _valid_ext(fname): return fname.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS))\n",
    "\n",
    "def _file_prefix_or_none(filename: str):\n",
    "    low = filename.lower()\n",
    "    for base in ALLOWED_BASES:\n",
    "        if low.startswith(base): return base\n",
    "    return None\n",
    "\n",
    "def _find_image_with_stem_and_suffix(directory, stem, suffix=\"\"):\n",
    "    if not directory or not os.path.isdir(directory):\n",
    "        return None\n",
    "    stem = stem.lower()\n",
    "    for file in os.listdir(directory):\n",
    "        fname, fext = os.path.splitext(file)\n",
    "        if fext.lower() in (\".png\",\".jpg\",\".jpeg\") and fname.lower() == f\"{stem}{suffix}\":\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "# --- Existence check in Google Drive by Colab-style path ---\n",
    "def drive_file_exists_any_ext_at_colab_path(target_colab_path: str,\n",
    "                                            exts=(\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")) -> bool:\n",
    "    \"\"\"\n",
    "    Given a Colab-style *file* path (incl. a filename with any extension),\n",
    "    checks if a file with the SAME stem exists in the same folder with any of the allowed extensions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(target_colab_path)\n",
    "        stem, _ = os.path.splitext(desired_name)\n",
    "        files = _list_children(parent_id, q_extra=\"\")  # list once; filter locally\n",
    "        allowed = {e.lower() for e in exts}\n",
    "        for f in files:\n",
    "            fname = f.get(\"name\", \"\")\n",
    "            s, e = os.path.splitext(fname)\n",
    "            if s == stem and e.lower() in allowed:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Ext-agnostic existence check failed for {target_colab_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# === NEW: mask finding with AGNOSTIC priority ===\n",
    "def find_mask_path(base_subcat_dir: str, stem_no_cut: str):\n",
    "    \"\"\"\n",
    "    Priority:\n",
    "      1) {stem}_mask_agnostic.(png|jpg|jpeg)\n",
    "      2) {stem}_mask.(png|jpg|jpeg)\n",
    "    \"\"\"\n",
    "    if not base_subcat_dir or not os.path.isdir(base_subcat_dir):\n",
    "        return None\n",
    "\n",
    "    candidates = []\n",
    "    if PREFER_AGNOSTIC_MASKS:\n",
    "      for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n",
    "          candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask_agnostic{ext}\"))\n",
    "    for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n",
    "        candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask{ext}\"))\n",
    "\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_secondary_garment_path(folder_path: str, main_filename: str):\n",
    "    \"\"\"\n",
    "    Locate the secondary garment paired with a primary garment file.\n",
    "    Example: main 'bc_lft_cut.png' -> looks for 'bc_lft_sec_cut.(png|jpg)'.\n",
    "    Falls back to a non-cut variant when REQUIRE_CUT_IN_FILENAME is False.\n",
    "    \"\"\"\n",
    "    stem, _ = os.path.splitext(main_filename)\n",
    "    has_cut = stem.endswith(\"_cut\")\n",
    "    core = stem[:-4] if has_cut else stem\n",
    "\n",
    "    candidates = [f\"{core}_sec_cut\"]\n",
    "    if not REQUIRE_CUT_IN_FILENAME:\n",
    "        candidates.append(f\"{core}_sec\")\n",
    "    if not has_cut:\n",
    "        candidates.append(f\"{stem}_sec_cut\")\n",
    "\n",
    "    seen = set()\n",
    "    for cand in candidates:\n",
    "        if cand in seen:\n",
    "            continue\n",
    "        seen.add(cand)\n",
    "        for ext in VALID_EXTENSIONS:\n",
    "            path = os.path.join(folder_path, f\"{cand}{ext}\")\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "    return None\n",
    "\n",
    "# ──────────────────────────────────────────\n",
    "# --- Aspect-ratio bbox (replaces square bbox usage) ---\n",
    "\n",
    "\n",
    "def find_aspect_bbox(\n",
    "    mask: Image.Image,\n",
    "    aspect: tuple[int,int] = (1,1),   # width:height, e.g. (1280,1600)\n",
    "    padding: int = 40,\n",
    "    upper_padding: int | None = None,\n",
    "    horiz_padding: int = 0,\n",
    "    min_margin: int | None = None,\n",
    "    allow_padding: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a rectangular bbox [x0, y0, x1, y1] that fully contains the mask + padding\n",
    "    and matches the requested aspect ratio. When allow_padding is True the box may\n",
    "    extend outside the image; callers should pad when cropping to preserve aspect.\n",
    "    \"\"\"\n",
    "    if min_margin is None:\n",
    "        try:\n",
    "            min_margin = int(MASK_EXPAND_PX + 3 * MASK_FEATHER_PX + 5)\n",
    "        except Exception:\n",
    "            min_margin = 40\n",
    "\n",
    "    m = np.array(mask.convert(\"L\"))\n",
    "    h, w = m.shape\n",
    "    ys, xs = np.where(m > 128)\n",
    "    if xs.size == 0:\n",
    "        raise ValueError(\"Mask has no white pixels!\")\n",
    "\n",
    "    x_min, x_max = int(xs.min()), int(xs.max())\n",
    "    y_min, y_max = int(ys.min()), int(ys.max())\n",
    "\n",
    "    if upper_padding is None:\n",
    "        upper_padding = padding\n",
    "\n",
    "    # Initial padded bbox (can go outside image bounds; padding applied later)\n",
    "    x0 = x_min - horiz_padding - min_margin\n",
    "    x1 = x_max + horiz_padding + min_margin\n",
    "    y0 = y_min - upper_padding - min_margin\n",
    "    y1 = y_max + padding + min_margin\n",
    "\n",
    "    bw, bh = (x1 - x0), (y1 - y0)\n",
    "    aw, ah = aspect\n",
    "    target_ar = float(aw) / float(max(1, ah))\n",
    "\n",
    "    def expand_to_aspect(x0, y0, x1, y1):\n",
    "        bw = x1 - x0; bh = y1 - y0\n",
    "        cur_ar = bw / float(max(1, bh))\n",
    "        if cur_ar < target_ar:\n",
    "            need_w = int(np.ceil(target_ar * bh))\n",
    "            grow = max(0, need_w - bw)\n",
    "            x0 -= grow // 2\n",
    "            x1 += grow - grow // 2\n",
    "        elif cur_ar > target_ar:\n",
    "            need_h = int(np.ceil(bw / target_ar))\n",
    "            grow = max(0, need_h - bh)\n",
    "            y0 -= grow // 2\n",
    "            y1 += grow - grow // 2\n",
    "        return x0, y0, x1, y1\n",
    "\n",
    "    x0, y0, x1, y1 = expand_to_aspect(x0, y0, x1, y1)\n",
    "\n",
    "    if not allow_padding:\n",
    "        x0, y0 = max(0, int(x0)), max(0, int(y0))\n",
    "        x1, y1 = min(w, int(x1)), min(h, int(y1))\n",
    "    else:\n",
    "        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "    return [x0, y0, x1, y1]\n",
    "\n",
    "\n",
    "def crop_with_padding(img: Image.Image, bbox, fill):\n",
    "    \"\"\"Crop using bbox (which may extend outside the image) and pad missing areas with fill.\"\"\"\n",
    "    x0, y0, x1, y1 = map(int, bbox)\n",
    "    w, h = img.size\n",
    "    tgt_w, tgt_h = x1 - x0, y1 - y0\n",
    "    out = Image.new(img.mode, (tgt_w, tgt_h), fill)\n",
    "\n",
    "    src_box = (\n",
    "        max(0, x0),\n",
    "        max(0, y0),\n",
    "        min(w, x1),\n",
    "        min(h, y1),\n",
    "    )\n",
    "    dst_xy = (max(0, -x0), max(0, -y0))\n",
    "\n",
    "    if src_box[2] > src_box[0] and src_box[3] > src_box[1]:\n",
    "        region = img.crop(src_box)\n",
    "        out.paste(region, dst_xy)\n",
    "    return out\n",
    "\n",
    "WHITE_RGB = (255,255,255)\n",
    "\n",
    "def flatten_alpha_to_white(img: Image.Image) -> Image.Image:\n",
    "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n",
    "        bg = Image.new(\"RGB\", img.size, WHITE_RGB)\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "        return bg\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "def _tight_bbox_nonwhite_or_opaque(img: Image.Image):\n",
    "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n",
    "        arr = np.asarray(img.convert(\"RGBA\"))\n",
    "        alpha = arr[...,3]\n",
    "        fg = alpha > 0\n",
    "    else:\n",
    "        arr = np.asarray(img.convert(\"RGB\"))\n",
    "        fg = ~((arr[...,0]==255)&(arr[...,1]==255)&(arr[...,2]==255))\n",
    "    if not np.any(fg): return None\n",
    "    ys, xs = np.where(fg)\n",
    "    x0, x1 = int(xs.min()), int(xs.max())+1\n",
    "    y0, y1 = int(ys.min()), int(ys.max())+1\n",
    "    return (x0,y0,x1,y1)\n",
    "\n",
    "def crop_garment_keep_aspect(img: Image.Image) -> Image.Image:\n",
    "    bbox = _tight_bbox_nonwhite_or_opaque(img)\n",
    "    base = flatten_alpha_to_white(img)\n",
    "    if bbox is None: return base\n",
    "    full_bbox = (0,0,base.width,base.height)\n",
    "    if bbox == full_bbox: return base\n",
    "    return base.crop(bbox)\n",
    "\n",
    "def to_centered_square(gar: Image.Image, fill=WHITE_RGB) -> Image.Image:\n",
    "    w,h = gar.size; side = max(w,h)\n",
    "    sq = Image.new(\"RGB\", (side, side), fill)\n",
    "    ox, oy = (side-w)//2, (side-h)//2\n",
    "    sq.paste(gar, (ox,oy)); return sq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _add_caption_above_square(square_img: Image.Image, heading: str) -> Image.Image:\n",
    "    pad_top = max(60, square_img.height // 5)\n",
    "    canvas = Image.new(\"RGB\", (square_img.width, square_img.height + pad_top), WHITE_RGB)\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    font_size = max(18, square_img.width // 18)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", font_size)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "    bbox = draw.textbbox((0, 0), heading, font=font)\n",
    "    text_w = bbox[2] - bbox[0]\n",
    "    text_h = bbox[3] - bbox[1]\n",
    "    text_x = max(0, (canvas.width - text_w) // 2)\n",
    "    text_y = max(0, (pad_top - text_h) // 2)\n",
    "    draw.text((text_x, text_y), heading, fill=(0, 0, 0), font=font)\n",
    "    canvas.paste(square_img, (0, pad_top))\n",
    "    return canvas\n",
    "\n",
    "def load_texture_reference(folder_path: str, *, secondary: bool = False, heading: str = \"Main texture reference\") -> Image.Image | None:\n",
    "    \"\"\"\n",
    "    Load a texture reference if present:\n",
    "      - texture.(png|jpg|jpeg) for primary\n",
    "      - texture_sec.(png|jpg|jpeg) for secondary\n",
    "    Crop to a square and add a caption banner above on white.\n",
    "    \"\"\"\n",
    "    stem = \"texture_sec\" if secondary else \"texture\"\n",
    "    for ext in VALID_EXTENSIONS:\n",
    "        candidate = os.path.join(folder_path, f\"{stem}{ext}\")\n",
    "        if os.path.exists(candidate):\n",
    "            try:\n",
    "                raw = open_upright(candidate).convert(\"RGB\")\n",
    "                side = min(raw.size)\n",
    "                if side <= 0:\n",
    "                    continue\n",
    "                square = ImageOps.fit(raw, (side, side), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5))\n",
    "                return _add_caption_above_square(square, heading)\n",
    "            except Exception as tex_err:\n",
    "                print(f\"      ⚠️ Unable to use texture reference '{candidate}': {tex_err}\")\n",
    "                return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbJ9VKUEzhaP"
   },
   "outputs": [],
   "source": [
    "# --- Visualisation helpers (restored) ---\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def open_upright(path) -> Image.Image:\n",
    "    # EXIF-aware loader (same as before)\n",
    "    with Image.open(path) as im:\n",
    "        return ImageOps.exif_transpose(im)\n",
    "\n",
    "def show_gallery(img_list, titles=None, cols=3, w=4):\n",
    "    \"\"\"\n",
    "    Display PIL images in a flexible grid (identical behaviour to your original).\n",
    "    Only renders if SHOW_VISUALS is True.\n",
    "    \"\"\"\n",
    "    if not globals().get(\"SHOW_VISUALS\", False):\n",
    "        return\n",
    "\n",
    "    n = len(img_list)\n",
    "    rows = math.ceil(n / cols)\n",
    "    plt.figure(figsize=(cols * w, rows * w))\n",
    "\n",
    "    for i, img in enumerate(img_list):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        # Accept PIL, torch tensors or numpy arrays (4-D batch ⇒ pick first)\n",
    "        if isinstance(img, np.ndarray) and img.ndim == 4:\n",
    "            img = img[0]  # (B,H,W,C) → (H,W,C)\n",
    "        # Torch tensors are printed via duck-typing check to avoid hard import\n",
    "        if \"Tensor\" in str(type(img)):\n",
    "            img = img.detach().cpu().permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img)\n",
    "        if titles and i < len(titles):\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOhtSL7B-_-f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def paste_crop_back_debug(\n",
    "    full_img: Image.Image,\n",
    "    edited_crop: Image.Image,\n",
    "    crop_box,               # (x0, y0, x1, y1) in full_img coords\n",
    "    crop_mask,              # H×W uint8/bool, garment=white in crop coords\n",
    "    solid_expand_px: int = 8,   # grow the 100% opaque region\n",
    "    halo_px: int = 40,          # thickness of the soft halo OUTSIDE solid\n",
    "    feather_px: int = 20,       # Gaussian sigma for halo\n",
    "    *,\n",
    "    bin_thresh: int = 127,\n",
    "    edge_feather_px: int = 15,  # clamp width at crop borders\n",
    "):\n",
    "    x0, y0, x1, y1 = map(int, crop_box)\n",
    "    tgt_w, tgt_h   = (x1 - x0), (y1 - y0)\n",
    "\n",
    "    # --- resize edited crop ---\n",
    "    edit_rs = edited_crop.resize((tgt_w, tgt_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # --- 1) binary silhouette mask in crop coords ---\n",
    "    mask_np = crop_mask\n",
    "    if isinstance(mask_np, Image.Image):\n",
    "        mask_np = np.array(mask_np.convert(\"L\"))\n",
    "    if mask_np.ndim == 3:\n",
    "        mask_np = mask_np[..., 0]\n",
    "\n",
    "    mask_np = cv2.resize(mask_np, (tgt_w, tgt_h), interpolation=cv2.INTER_NEAREST)\n",
    "    mask_bin = (mask_np > bin_thresh).astype(np.uint8)\n",
    "\n",
    "    # --- 2) solid = expanded garment, outer = solid + halo -------------------\n",
    "    def dilate(mask, r):\n",
    "        if r <= 0:\n",
    "            return mask.copy()\n",
    "        ksize = max(1, r * 2 + 1)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n",
    "        return cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    solid = dilate(mask_bin, solid_expand_px)               # fully opaque region\n",
    "    outer = dilate(solid, halo_px)                          # solid + halo shell\n",
    "\n",
    "    # band where we want partial alpha\n",
    "    band = outer.clip(0, 1).astype(np.float32) * 255.0\n",
    "\n",
    "    # --- 3) blur the band to get a smooth halo --------------------------------\n",
    "    if feather_px > 0:\n",
    "        band = cv2.GaussianBlur(\n",
    "            band, (0, 0),\n",
    "            sigmaX=feather_px,\n",
    "            sigmaY=feather_px,\n",
    "        )\n",
    "\n",
    "    # --- 4) clamp halo near crop borders (no recursion) -----------------------\n",
    "    H, W = band.shape\n",
    "    ef = max(1, int(edge_feather_px))\n",
    "    ef = min(ef, H // 2, W // 2)\n",
    "\n",
    "    leaks_top    = band[0, :].max() > 0\n",
    "    leaks_bottom = band[-1, :].max() > 0\n",
    "    leaks_left   = band[:, 0].max() > 0\n",
    "    leaks_right  = band[:, -1].max() > 0\n",
    "\n",
    "    if leaks_top and ef > 0:\n",
    "        ramp = np.linspace(0.0, 1.0, ef, endpoint=True).reshape(-1, 1)\n",
    "        band[:ef, :] *= ramp\n",
    "    if leaks_bottom and ef > 0:\n",
    "        ramp = np.linspace(1.0, 0.0, ef, endpoint=True).reshape(-1, 1)\n",
    "        band[-ef:, :] *= ramp\n",
    "    if leaks_left and ef > 0:\n",
    "        ramp = np.linspace(0.0, 1.0, ef, endpoint=True).reshape(1, -1)\n",
    "        band[:, :ef] *= ramp\n",
    "    if leaks_right and ef > 0:\n",
    "        ramp = np.linspace(1.0, 0.0, ef, endpoint=True).reshape(1, -1)\n",
    "        band[:, -ef:] *= ramp\n",
    "\n",
    "    # --- 5) final alpha: 255 inside solid, halo in the band only --------------\n",
    "    alpha = band.copy()\n",
    "    alpha[solid > 0] = 255.0\n",
    "    alpha = np.clip(alpha, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # --- 6) composite back into full image ------------------------------------\n",
    "    mask_img = Image.fromarray(alpha, mode=\"L\")\n",
    "    region   = full_img.crop((x0, y0, x1, y1))\n",
    "    comp     = Image.composite(edit_rs, region, mask_img)\n",
    "    out_img  = full_img.copy()\n",
    "    out_img.paste(comp, (x0, y0))\n",
    "\n",
    "    return out_img, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Br9dskEm_CBL"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- NanoBanana Pro try-on (Google Cloud GenAI) ---\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "\n",
    "def _load_gemini_api_key():\n",
    "    key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GEMINI_APIKEY\")\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        key = key or userdata.get(\"GEMINI_API_KEY\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not key:\n",
    "        raise ValueError(\"Set GEMINI_API_KEY in environment or Colab userdata.\")\n",
    "    return key\n",
    "\n",
    "genai_client = genai.Client(api_key=_load_gemini_api_key())\n",
    "\n",
    "def _extract_first_image(resp):\n",
    "    import io\n",
    "    parts = []\n",
    "    if hasattr(resp, \"parts\"):\n",
    "        parts.extend(resp.parts)\n",
    "    for cand in getattr(resp, \"candidates\", []):\n",
    "        parts.extend(getattr(getattr(cand, \"content\", None), \"parts\", []) or [])\n",
    "\n",
    "    for part in parts:\n",
    "        if isinstance(part, Image.Image):\n",
    "            return part\n",
    "        as_image = getattr(part, \"as_image\", None)\n",
    "        if callable(as_image):\n",
    "            img = as_image()\n",
    "            if isinstance(img, Image.Image):\n",
    "                return img\n",
    "        inline = getattr(part, \"inline_data\", None)\n",
    "        if inline and getattr(inline, \"data\", None):\n",
    "            return Image.open(io.BytesIO(inline.data)).convert(\"RGB\")\n",
    "    raise ValueError(\"No image returned from NanoBanana Pro response.\")\n",
    "\n",
    "\n",
    "def run_nanobanana_tryon(model_image: Image.Image, garment_image: Image.Image,\n",
    "                         *, aspect_ratio: str = GEN_ASPECT_RATIO,\n",
    "                         image_size: str = GEN_IMAGE_SIZE,\n",
    "                         prompt: str | None = None,\n",
    "                         extra_images=None):\n",
    "    prompt_text = prompt or TRYON_PROMPT\n",
    "    contents = [\n",
    "        prompt_text,\n",
    "        \"Model image:\",\n",
    "        model_image.convert(\"RGB\"),\n",
    "        \"Garment image:\",\n",
    "        garment_image.convert(\"RGB\"),\n",
    "    ]\n",
    "    if extra_images:\n",
    "        contents.extend(extra_images)\n",
    "    resp = genai_client.models.generate_content(\n",
    "        model=NANOBANANA_MODEL_ID,\n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"IMAGE\"],\n",
    "            image_config=types.ImageConfig(\n",
    "                aspect_ratio=aspect_ratio,\n",
    "                image_size=image_size,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    img = _extract_first_image(resp)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "print(\"✅ NanoBanana Pro client ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESv1VQVOnkTu"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- SAM3 segmentation via fal.ai for on-the-fly garment masks ---\n",
    "\n",
    "import base64, io, json\n",
    "import fal_client\n",
    "import requests\n",
    "\n",
    "MASK_CACHE = {}\n",
    "TOP_GARMENTS_SET = {g.casefold() for g in TOP_GARMENTS}\n",
    "BOTTOM_GARMENTS_SET = {g.casefold() for g in BOTTOM_GARMENTS}\n",
    "\n",
    "def _classify_garment_category(category: str) -> str | None:\n",
    "    cat_norm = re.sub(r\"\\s+\", \" \", str(category or \"\").strip()).casefold()\n",
    "    if cat_norm in TOP_GARMENTS_SET:\n",
    "        return \"top\"\n",
    "    if cat_norm in BOTTOM_GARMENTS_SET:\n",
    "        return \"bottom\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _garment_category_from_path(path: str) -> str:\n",
    "    parts = os.path.normpath(path).split(os.sep)\n",
    "    lowers = [p.casefold() for p in parts]\n",
    "    cat = None\n",
    "    if \"siksilk\" in lowers:\n",
    "        last_idx = len(lowers) - 1 - lowers[::-1].index(\"siksilk\")\n",
    "        if last_idx + 1 < len(parts):\n",
    "            cat = parts[last_idx + 1]\n",
    "    if not cat:\n",
    "        try:\n",
    "            rel = os.path.relpath(path, GARMENTS_ROOT)\n",
    "            if not rel.startswith(\"..\"):  # path is under garments root\n",
    "                rel_parts = rel.split(os.sep)\n",
    "                if rel_parts:\n",
    "                    cat = rel_parts[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cat:\n",
    "        parent = os.path.basename(os.path.dirname(path))\n",
    "        cat = parent or \"garment\"\n",
    "    cat_clean = re.sub(r\"[_]+\", \" \", str(cat)).strip()\n",
    "    cat_clean = re.sub(r\"\\s+\", \" \", cat_clean)\n",
    "    return cat_clean or \"garment\"\n",
    "\n",
    "def _build_mask_prompt(category: str, variant: str | None = None) -> str:\n",
    "    clean_category = str(category).strip()\n",
    "    clean_category = clean_category.rstrip(\"s\") if clean_category else clean_category\n",
    "    prompt = MASK_PROMPT_TEMPLATE.format(category=clean_category, variant=(variant or \"\").strip())\n",
    "    prompt = prompt.strip()\n",
    "    return prompt or clean_category\n",
    "\n",
    "def _strip_json_block(text: str) -> str:\n",
    "    if \"```json\" in text:\n",
    "        return text.split(\"```json\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "    if \"```\" in text:\n",
    "        return text.split(\"```\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def _mask_data_to_canvas(data_url: str, target_size):\n",
    "    w, h = target_size\n",
    "    if data_url.startswith(\"http\"):\n",
    "        resp = requests.get(data_url)\n",
    "        resp.raise_for_status()\n",
    "        raw = resp.content\n",
    "    else:\n",
    "        encoded = data_url.split(\",\", 1)[1] if \",\" in data_url else data_url\n",
    "        encoded = encoded.strip()\n",
    "        if not encoded:\n",
    "            raise ValueError(\"Mask data missing from SAM response.\")\n",
    "        pad_len = (-len(encoded)) % 4\n",
    "        if pad_len:\n",
    "            encoded = encoded + \"=\" * pad_len\n",
    "        try:\n",
    "            raw = base64.b64decode(encoded)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid mask image data: {e}\")\n",
    "    mask_img = Image.open(io.BytesIO(raw)).convert(\"L\")\n",
    "    if mask_img.size != (w, h):\n",
    "        mask_img = mask_img.resize((w, h), Image.Resampling.NEAREST)\n",
    "    return mask_img.point(lambda v: 255 if v > 128 else 0)\n",
    "\n",
    "def _image_to_data_url(img: Image.Image) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode()\n",
    "\n",
    "def _extract_masks_from_result(result):\n",
    "    masks = []\n",
    "    for m in result.get(\"masks\") or []:\n",
    "        if isinstance(m, dict):\n",
    "            data = m.get(\"file_data\") or m.get(\"data\")\n",
    "            ct = m.get(\"content_type\") or \"image/png\"\n",
    "            if data:\n",
    "                if not str(data).startswith(\"data:\"):\n",
    "                    data = f\"data:{ct};base64,{data}\"\n",
    "                masks.append(data)\n",
    "            elif m.get(\"url\"):\n",
    "                masks.append(m[\"url\"])\n",
    "        elif isinstance(m, str):\n",
    "            masks.append(m)\n",
    "    img = result.get(\"image\")\n",
    "    if img:\n",
    "        if isinstance(img, dict):\n",
    "            data = img.get(\"file_data\") or img.get(\"data\")\n",
    "            ct = img.get(\"content_type\") or \"image/png\"\n",
    "            if data:\n",
    "                if not str(data).startswith(\"data:\"):\n",
    "                    data = f\"data:{ct};base64,{data}\"\n",
    "                masks.append(data)\n",
    "            elif img.get(\"url\"):\n",
    "                masks.append(img[\"url\"])\n",
    "        elif isinstance(img, str):\n",
    "            masks.append(img)\n",
    "    return masks\n",
    "\n",
    "def _fetch_sam_masks(image_data_url: str, category: str):\n",
    "    logs = []\n",
    "    def _on_queue(update):\n",
    "        if isinstance(update, fal_client.InProgress):\n",
    "            for log in update.logs:\n",
    "                msg = log.get(\"message\")\n",
    "                if msg:\n",
    "                    logs.append(msg)\n",
    "                    print(msg)\n",
    "    result = fal_client.subscribe(\n",
    "        FAL_SAM_MODEL_ID,\n",
    "        arguments={\n",
    "            \"image_url\": image_data_url,\n",
    "            \"text_prompt\": category,\n",
    "            \"apply_mask\": False,\n",
    "            \"return_multiple_masks\": False,\n",
    "            \"max_masks\": 1,\n",
    "            \"output_format\": \"png\",\n",
    "            \"sync_mode\": True,\n",
    "        },\n",
    "        with_logs=True,\n",
    "        on_queue_update=_on_queue,\n",
    "    )\n",
    "    return _extract_masks_from_result(result)\n",
    "\n",
    "def generate_mask_with_gemini(base_img_path: str, garment_folder: str, *, mask_variant: str | None = None):\n",
    "    cache_key = (base_img_path, garment_folder, mask_variant or \"primary\")\n",
    "    if cache_key in MASK_CACHE:\n",
    "        return MASK_CACHE[cache_key]\n",
    "    if not FAL_KEY:\n",
    "        raise ValueError(\"Set FAL_KEY variable for fal.ai SAM3 segmentation.\")\n",
    "    category = _garment_category_from_path(garment_folder)\n",
    "    prompt_category = str(category).strip()\n",
    "    main_class = _classify_garment_category(category)\n",
    "    if mask_variant:\n",
    "        prompt_category = f\"{prompt_category} ({mask_variant})\"\n",
    "        if \"sec\" in str(mask_variant).lower() and main_class:\n",
    "            if main_class == \"top\":\n",
    "                prompt_category = \"bottom garment\"\n",
    "            elif main_class == \"bottom\":\n",
    "                prompt_category = \"top cloth\"\n",
    "    prompt_category = prompt_category.rstrip(\"s\") if prompt_category else prompt_category\n",
    "    base_img = Image.open(base_img_path).convert(\"RGB\")\n",
    "    orig_w, orig_h = base_img.size\n",
    "    inf_img = base_img\n",
    "    scale = 1.0\n",
    "    max_side = max(orig_w, orig_h)\n",
    "    if max_side > MASK_MAX_SIZE:\n",
    "        scale = MASK_MAX_SIZE / float(max_side)\n",
    "        inf_img = base_img.resize((max(1, int(orig_w * scale)), max(1, int(orig_h * scale))), Image.Resampling.LANCZOS)\n",
    "\n",
    "    print(f\"Segmentation started for {os.path.basename(base_img_path)} [{prompt_category}] via fal.ai SAM3\")\n",
    "    prompt = _build_mask_prompt(prompt_category, mask_variant)\n",
    "    masks = _fetch_sam_masks(_image_to_data_url(inf_img), prompt)\n",
    "    if not masks:\n",
    "        raise ValueError(\"SAM3 did not return any masks.\")\n",
    "\n",
    "    mask_canvas = None\n",
    "    errors = []\n",
    "    for m in masks:\n",
    "        if not isinstance(m, str):\n",
    "            continue\n",
    "        try:\n",
    "            mask_canvas = _mask_data_to_canvas(m, inf_img.size)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            errors.append(str(e))\n",
    "            continue\n",
    "\n",
    "    if mask_canvas is None:\n",
    "        raise ValueError(f\"SAM3 returned masks but none were usable: {errors}\")\n",
    "\n",
    "    if scale != 1.0:\n",
    "        mask_canvas = mask_canvas.resize((orig_w, orig_h), Image.Resampling.NEAREST)\n",
    "    result = (mask_canvas, f\"sam3:{prompt_category}\")\n",
    "    MASK_CACHE[cache_key] = result\n",
    "    return result\n",
    "\n",
    "print(\"✅ SAM3 mask generation ready (fal.ai)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfNf3NHxjMaj"
   },
   "outputs": [],
   "source": [
    "# Colab cell — output routing + metadata helpers\n",
    "import os, json\n",
    "from PIL import PngImagePlugin\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "def build_output_filename(sku_name: str, angle_code: str, ext=\".png\", suffix=\"\") -> str:\n",
    "    # Examples: SS-12345-fr_rght or SS-12345-bc_lft\n",
    "    angle_clean = _norm_angle(angle_code)\n",
    "    suffix = suffix or \"\"\n",
    "    return f\"{sku_name}-{angle_clean}{suffix}{ext}\"\n",
    "\n",
    "\n",
    "import json, piexif\n",
    "from PIL import Image\n",
    "\n",
    "def save_png_with_metadata(img, out_path, details_payload=None, quality=95):\n",
    "    if details_payload:\n",
    "        # Encode JSON as UTF-8 with an ASCII prefix per EXIF spec for UserComment\n",
    "        payload = json.dumps(details_payload, ensure_ascii=False).encode(\"utf-8\")\n",
    "        user_comment = b\"ASCII\\x00\\x00\\x00\" + payload  # indicates undefined/UTF-8\n",
    "        exif_dict = {\"0th\": {}, \"Exif\": {piexif.ExifIFD.UserComment: user_comment}, \"1st\": {}, \"GPS\": {}, \"Interop\": {}}\n",
    "        exif_bytes = piexif.dump(exif_dict)\n",
    "        img.save(out_path, format=\"PNG\", exif=exif_bytes)\n",
    "    else:\n",
    "        img.save(out_path, format=\"PNG\")\n",
    "\n",
    "import json, piexif\n",
    "from PIL import Image, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBN_kgOo_GRI"
   },
   "outputs": [],
   "source": [
    "# --- Google APIs: gspread + Drive upload (Operations sync removed) ---\n",
    "\n",
    "import google.auth\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\", \"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "creds, _ = google.auth.default(scopes=SCOPES)\n",
    "\n",
    "import gspread\n",
    "gs = gspread.authorize(creds)\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "drive_svc = build(\"drive\", \"v3\", credentials=creds)\n",
    "\n",
    "FOLDER_MIME   = \"application/vnd.google-apps.folder\"\n",
    "SHORTCUT_MIME = \"application/vnd.google-apps.shortcut\"\n",
    "PATH_PREFIX   = \"/content/drive/MyDrive/\"\n",
    "\n",
    "def _escape_name(name: str) -> str: return name.replace(\"'\", r\"'\")\n",
    "\n",
    "def _maybe_follow_shortcut(file_obj):\n",
    "    if file_obj.get(\"mimeType\") == SHORTCUT_MIME:\n",
    "        sd = file_obj.get(\"shortcutDetails\", {}) or {}\n",
    "        return sd.get(\"targetId\"), sd.get(\"targetMimeType\")\n",
    "    return file_obj.get(\"id\"), file_obj.get(\"mimeType\")\n",
    "\n",
    "def _list_children(parent_id: str, q_extra: str, page_size: int = 1000):\n",
    "    q = f\"'{parent_id}' in parents and trashed = false\"\n",
    "    if q_extra: q += f\" and ({q_extra})\"\n",
    "    resp = drive_svc.files().list(\n",
    "        q=q, spaces=\"drive\", pageSize=page_size,\n",
    "        fields=\"files(id,name,mimeType,shortcutDetails)\",\n",
    "        includeItemsFromAllDrives=True, supportsAllDrives=True,\n",
    "    ).execute()\n",
    "    return resp.get(\"files\", [])\n",
    "\n",
    "def _find_folder_id(parent_id: str, name: str):\n",
    "    files = _list_children(\n",
    "        parent_id,\n",
    "        q_extra=(\n",
    "            f\"name = '{_escape_name(name)}' and \"\n",
    "            f\"(mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Direct folder match\n",
    "    for f in files:\n",
    "        if f[\"mimeType\"] == FOLDER_MIME:\n",
    "            return f[\"id\"]\n",
    "\n",
    "    # Shortcut to folder\n",
    "    for f in files:\n",
    "        if f[\"mimeType\"] == SHORTCUT_MIME:\n",
    "            tid, tmime = _maybe_follow_shortcut(f)\n",
    "            if tmime == FOLDER_MIME:\n",
    "                return tid\n",
    "\n",
    "    # Fallback: match by case-insensitive name\n",
    "    files = _list_children(\n",
    "        parent_id,\n",
    "        q_extra=(\n",
    "            f\"(mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\"\n",
    "        ),\n",
    "    )\n",
    "    needle = name.strip().casefold()\n",
    "\n",
    "    for f in files:\n",
    "        if f.get(\"name\", \"\").strip().casefold() == needle:\n",
    "            tid, tmime = _maybe_follow_shortcut(f)\n",
    "            if tmime == FOLDER_MIME:\n",
    "                return tid\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _resolve_parent_id_and_filename_from_colab_path(colab_path: str):\n",
    "    if not colab_path.startswith(PATH_PREFIX):\n",
    "        raise ValueError(\n",
    "            f\"This helper supports only '{PATH_PREFIX}...'. Got: {colab_path}\"\n",
    "        )\n",
    "\n",
    "    parts = colab_path[len(PATH_PREFIX):].strip(\"/\").split(\"/\")\n",
    "    if not parts:\n",
    "        raise ValueError(\"Path must include a file name.\")\n",
    "\n",
    "    parent_id = \"root\"\n",
    "\n",
    "    for part in parts[:-1]:\n",
    "        next_id = _find_folder_id(parent_id, part)\n",
    "        if not next_id:\n",
    "            raise FileNotFoundError(f\"Folder not found in path: '{part}'\")\n",
    "        parent_id = next_id\n",
    "\n",
    "    desired_name = parts[-1]\n",
    "    return parent_id, desired_name\n",
    "\n",
    "\n",
    "def upload_to_drive_folder(\n",
    "    local_path: str,\n",
    "    parent_folder_id: str,\n",
    "    desired_name: str | None = None\n",
    "):\n",
    "    media = MediaFileUpload(local_path, resumable=True)\n",
    "    body = {\n",
    "        \"name\": desired_name or os.path.basename(local_path),\n",
    "        \"parents\": [parent_folder_id],\n",
    "    }\n",
    "\n",
    "    file = (\n",
    "        drive_svc.files()\n",
    "        .create(\n",
    "            body=body,\n",
    "            media_body=media,\n",
    "            fields=\"id, webViewLink, name, parents\",\n",
    "            supportsAllDrives=True,\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    drive_svc.permissions().create(\n",
    "        fileId=file[\"id\"],\n",
    "        body={\"type\": \"anyone\", \"role\": \"reader\"},\n",
    "        fields=\"id\",\n",
    "        supportsAllDrives=True,\n",
    "    ).execute()\n",
    "\n",
    "    return file\n",
    "\n",
    "\n",
    "def upload_file_and_append_to_sheet(\n",
    "    local_path: str,\n",
    "    target_colab_path: str,\n",
    "    sku_name: str,\n",
    "    angle: str,\n",
    "    spreadsheet_id: str | None,\n",
    "    worksheet_name: str | None,\n",
    "):\n",
    "    parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(\n",
    "        target_colab_path\n",
    "    )\n",
    "\n",
    "    uploaded = upload_to_drive_folder(local_path, parent_id, desired_name)\n",
    "    file_id = uploaded[\"id\"]\n",
    "\n",
    "    file_url = (\n",
    "        uploaded.get(\"webViewLink\")\n",
    "        or f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
    "    )\n",
    "    folder_url = f\"https://drive.google.com/drive/folders/{parent_id}\"\n",
    "\n",
    "    if spreadsheet_id and worksheet_name:\n",
    "        ts = datetime.now(pytz.timezone(TIMEZONE)).strftime(\"%m-%d %H:%M:%S\")\n",
    "        uid = str(uuid.uuid4())\n",
    "\n",
    "        sh = gs.open_by_key(spreadsheet_id)\n",
    "        ws = sh.worksheet(worksheet_name)\n",
    "\n",
    "        sku_cell = f'=HYPERLINK(\"{folder_url}\"; \"{sku_name}\")'\n",
    "\n",
    "        ws.append_row(\n",
    "            [\n",
    "                sku_cell,\n",
    "                angle,\n",
    "                ts,\n",
    "                file_url,\n",
    "                uid,\n",
    "                \"Girls need to check\",\n",
    "                OPERATOR,\n",
    "            ],\n",
    "            value_input_option=\"USER_ENTERED\",\n",
    "        )\n",
    "\n",
    "    return {\"file_url\": file_url}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU5VwKPQIody"
   },
   "source": [
    "# BATCH HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQi5CzF2_O92"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Batch processor  ---\n",
    "\n",
    "\n",
    "def process_one_garment_folder(folder_path: str, allowed_angles=None):\n",
    "    allowed_outputs_set = {_norm_angle(a) for a in (allowed_angles or [])}\n",
    "    allowed_outputs = sorted(allowed_outputs_set)\n",
    "    allowed_sources = expand_as_list(allowed_angles) if allowed_angles else None\n",
    "\n",
    "    main_category = _garment_category_from_path(folder_path)\n",
    "    main_class = _classify_garment_category(main_category)\n",
    "    secondary_type = \"bottom\" if main_class == \"top\" else (\"top\" if main_class == \"bottom\" else \"secondary\")\n",
    "\n",
    "    base_subcat_dir = resolve_base_mask_dir(folder_path)\n",
    "    if not base_subcat_dir:\n",
    "        print(f\"⚠️ Cannot resolve base/mask dir for SKU: {folder_path}\")\n",
    "    files_sorted = sorted(os.listdir(folder_path))\n",
    "\n",
    "    main_texture_card = load_texture_reference(folder_path, secondary=False, heading=\"Main texture reference\")\n",
    "    secondary_texture_card = load_texture_reference(folder_path, secondary=True, heading=\"Main texture reference\") if SECONDARY_GARMENT else None\n",
    "\n",
    "    main_prompt_text = build_main_prompt(include_texture=main_texture_card is not None)\n",
    "    secondary_prompt_text = build_secondary_prompt(sec_type=secondary_type, include_texture=secondary_texture_card is not None)\n",
    "\n",
    "    def source_priority(target_angle: str, filename: str) -> int:\n",
    "        stem = os.path.splitext(filename.lower())[0]\n",
    "        if target_angle == \"fr_cl\":\n",
    "            if stem.startswith(\"fr_cl\"):\n",
    "                return 0\n",
    "            if stem.startswith(\"fr_\") and not (stem.startswith(\"fr_rght\") or stem.startswith(\"fr_lft\")):\n",
    "                return 1\n",
    "            if stem.startswith(\"fr_rght\") or stem.startswith(\"fr_lft\"):\n",
    "                return 2\n",
    "        if stem.startswith(target_angle):\n",
    "            return 0\n",
    "        return 3\n",
    "\n",
    "    best_for_target = {}\n",
    "\n",
    "    for file in files_sorted:\n",
    "        low = file.lower()\n",
    "\n",
    "        if \"_sec\" in low:\n",
    "          # Ignore secondary garments in primary detection\n",
    "          continue\n",
    "\n",
    "        if allowed_sources and not any(low.startswith(src) for src in allowed_sources):\n",
    "            continue\n",
    "        if SKIP_FILENAME_TOKENS and any(tok in low for tok in SKIP_FILENAME_TOKENS):\n",
    "            continue\n",
    "        if REQUIRE_CUT_IN_FILENAME and (\"cut\" not in low):\n",
    "            continue\n",
    "        if not _valid_ext(file):\n",
    "            continue\n",
    "\n",
    "        matching_sources = [src for src in (allowed_sources or []) if low.startswith(src)] if allowed_sources else [_norm_angle(os.path.splitext(file)[0])]\n",
    "        if allowed_sources and not matching_sources:\n",
    "            continue\n",
    "\n",
    "        target_candidates = set()\n",
    "        for src in matching_sources:\n",
    "            norm_src = _norm_angle(src)\n",
    "            for target in (allowed_outputs or [norm_src]):\n",
    "                fam = {_norm_angle(x) for x in expand_allowed_angles([target])}\n",
    "                fam.add(_norm_angle(target))\n",
    "                if norm_src in fam:\n",
    "                    target_candidates.add(_norm_angle(target))\n",
    "\n",
    "        if allowed_outputs and not target_candidates:\n",
    "            continue\n",
    "\n",
    "        for target_angle in sorted(target_candidates):\n",
    "            base_img_path = _find_image_with_stem_and_suffix(base_subcat_dir, target_angle)\n",
    "            if not base_img_path:\n",
    "                print(f\"⚠️ Missing BASE for target '{target_angle}' → skipping that target for {file}\")\n",
    "                continue\n",
    "\n",
    "            mask_path = find_mask_path(base_subcat_dir, target_angle)\n",
    "            if not mask_path:\n",
    "                print(f\"ℹ️ No local mask for target '{target_angle}', will request SAM3 segmentation.\")\n",
    "\n",
    "            priority = source_priority(target_angle, low)\n",
    "            current = best_for_target.get(target_angle)\n",
    "            if current and priority >= current[\"priority\"]:\n",
    "                continue\n",
    "\n",
    "            best_for_target[target_angle] = {\n",
    "                \"priority\": priority,\n",
    "                \"file\": file,\n",
    "                \"base_img_path\": base_img_path,\n",
    "                \"mask_path\": mask_path,\n",
    "            }\n",
    "\n",
    "    worklist = [\n",
    "        (data[\"file\"], target_angle, data[\"base_img_path\"], data[\"mask_path\"])\n",
    "        for target_angle, data in best_for_target.items()\n",
    "    ]\n",
    "    worklist.sort(key=lambda x: x[1])\n",
    "\n",
    "    def find_existing_output_local(target_colab_path: str):\n",
    "        stem, _ = os.path.splitext(target_colab_path)\n",
    "        for ext in (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\"):\n",
    "            cand = f\"{stem}{ext}\"\n",
    "            if os.path.exists(cand):\n",
    "                return cand\n",
    "        return None\n",
    "\n",
    "    sku_name = os.path.basename(folder_path)\n",
    "    if allowed_sources:\n",
    "        print(f\"▶️  {sku_name}: {len(worklist)} image(s) to generate (outputs={sorted(list(allowed_outputs_set))}, sources={sorted(list(set(allowed_sources)))})\")\n",
    "    else:\n",
    "        print(f\"▶️  {sku_name}: {len(worklist)} image(s) to generate\")\n",
    "\n",
    "    if not worklist:\n",
    "        return\n",
    "\n",
    "    for idx, (file, target_angle, base_img_path, mask_path) in enumerate(worklist, start=1):\n",
    "        print(f\"   {idx:>3}/{len(worklist):<3}  {file}  | USING STRICT base/mask='{target_angle}'\")\n",
    "        garment_path = os.path.join(folder_path, file)\n",
    "\n",
    "        sku_name = os.path.basename(folder_path)\n",
    "        angle_code = _norm_angle(target_angle)\n",
    "        main_suffix = \"_onlymain\" if SECONDARY_GARMENT else \"\"\n",
    "        main_out_name = build_output_filename(sku_name, angle_code, ext=\".png\", suffix=main_suffix)\n",
    "        final_out_name = build_output_filename(sku_name, angle_code, ext=\".png\", suffix=\"_both\") if SECONDARY_GARMENT else None\n",
    "\n",
    "        main_colab_path = os.path.join(OUTPUT_DIR, main_out_name)\n",
    "        final_colab_path = os.path.join(OUTPUT_DIR, final_out_name) if final_out_name else None\n",
    "\n",
    "        main_exists = drive_file_exists_any_ext_at_colab_path(main_colab_path)\n",
    "        final_exists = final_colab_path and drive_file_exists_any_ext_at_colab_path(final_colab_path)\n",
    "\n",
    "        reuse_main_img = None\n",
    "        sec_garment_override = None\n",
    "\n",
    "        if not SECONDARY_GARMENT:\n",
    "            if main_exists:\n",
    "                print(f\"      ⏭️  Skip: {main_out_name} already exists in {OUTPUT_DIR} (main target)\")\n",
    "                continue\n",
    "        else:\n",
    "            if final_exists:\n",
    "                print(f\"      ⏭️  Skip: {final_out_name} already exists in {OUTPUT_DIR} (secondary target)\")\n",
    "                continue\n",
    "\n",
    "            if main_exists:\n",
    "                sec_garment_override = find_secondary_garment_path(folder_path, file)\n",
    "                if not sec_garment_override:\n",
    "                    print(f\"      ⏭️  Skip: {main_out_name} exists but no secondary garment found in {folder_path}\")\n",
    "                    continue\n",
    "\n",
    "                existing_main_path = find_existing_output_local(main_colab_path)\n",
    "                if not existing_main_path:\n",
    "                    print(f\"      ⏭️  Skip: {main_out_name} exists in Drive but not on disk → cannot reuse without regenerating main\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    reuse_main_img = Image.open(existing_main_path).convert(\"RGB\")\n",
    "                    print(f\"      ▶️  Reusing existing main output '{os.path.basename(existing_main_path)}' for secondary stage\")\n",
    "                except Exception as reuse_err:\n",
    "                    print(f\"      ❌ Unable to reuse existing main output '{existing_main_path}': {reuse_err}\")\n",
    "                    continue\n",
    "\n",
    "        try:\n",
    "            mask_dir = base_subcat_dir\n",
    "\n",
    "            def perform_tryon_stage(stage_base_full, stage_mask_img, stage_garment_img, suffix, stage_label, mask_label, prompt_text=None, texture_card=None, texture_label=None):\n",
    "                show_gallery(\n",
    "                    [stage_garment_img, stage_base_full, stage_mask_img.convert(\"RGB\")],\n",
    "                    [f\"Source garment (white BG) [{stage_label}]\", f\"Base photo [{angle_code}] [{stage_label}]\", f\"Mask [{mask_label}]\"]\n",
    "                )\n",
    "\n",
    "                upper_padding = UPPER_PADDING if (stage_label == \"secondary\") else (UPPER_PADDING + 100)\n",
    "\n",
    "                bbox = find_aspect_bbox(\n",
    "                    stage_mask_img,\n",
    "                    aspect=TARGET_ASPECT,\n",
    "                    padding=CROP_PADDING,\n",
    "                    upper_padding=upper_padding,\n",
    "                    horiz_padding=HORIZ_PADDING,\n",
    "                    min_margin=CROP_MIN_MARGIN,\n",
    "                    allow_padding=True,\n",
    "                )\n",
    "                base_crop = crop_with_padding(stage_base_full, bbox, fill=WHITE_RGB)\n",
    "                mask_crop = crop_with_padding(stage_mask_img.convert(\"L\"), bbox, fill=0)\n",
    "                crop_thumb = texture_card if texture_card is not None else stage_garment_img\n",
    "                crop_title = (texture_label or \"Texture reference\") if texture_card is not None else \"Garment (white BG)\"\n",
    "                show_gallery(\n",
    "                    [base_crop, mask_crop.convert(\"RGB\"), crop_thumb],\n",
    "                    [f\"Cropped base (1:1) [{stage_label}]\", \"Cropped mask\", crop_title],\n",
    "                )\n",
    "\n",
    "                extra_images = None\n",
    "                if texture_card is not None:\n",
    "                    label = texture_label or \"Texture reference\"\n",
    "                    extra_images = [f\"{label}:\", texture_card.convert(\"RGB\")]\n",
    "\n",
    "                print(\"🍌 started...\")\n",
    "                tryon_gen = run_nanobanana_tryon(\n",
    "                    model_image=base_crop,\n",
    "                    garment_image=stage_garment_img,\n",
    "                    aspect_ratio=GEN_ASPECT_RATIO,\n",
    "                    image_size=GEN_IMAGE_SIZE,\n",
    "                    prompt=prompt_text or TRYON_PROMPT,\n",
    "                    extra_images=extra_images,\n",
    "                )\n",
    "\n",
    "                if tryon_gen.size != base_crop.size:\n",
    "                    if tryon_gen.width != tryon_gen.height:\n",
    "                        print(f\"      ⚠️ Generator returned non-square image {tryon_gen.size}; resizing to {base_crop.size}.\")\n",
    "                    tryon_sq = tryon_gen.resize(base_crop.size, Image.Resampling.LANCZOS)\n",
    "                else:\n",
    "                    tryon_sq = tryon_gen\n",
    "\n",
    "                final_img_local, alpha_dbg = paste_crop_back_debug(\n",
    "                    full_img   = stage_base_full.copy(),\n",
    "                    edited_crop= tryon_sq,\n",
    "                    crop_box   = bbox,\n",
    "                    crop_mask  = np.array(mask_crop),\n",
    "                    solid_expand_px = max(5, MASK_EXPAND_PX // 4),\n",
    "                    halo_px         = MASK_EXPAND_PX,\n",
    "                    feather_px      = MASK_FEATHER_PX,\n",
    "                    edge_feather_px = 15,   # tweak to taste\n",
    "                )\n",
    "\n",
    "                show_gallery(\n",
    "                    [tryon_sq, alpha_dbg, final_img_local],\n",
    "                    [\"Try-on crop\", \"Alpha (debug)\", \"Final paste-back\"],\n",
    "                )\n",
    "\n",
    "                out_name_local = build_output_filename(sku_name, angle_code, ext=\".png\", suffix=suffix)\n",
    "                tmp_path_local = os.path.join(\"/tmp\", out_name_local)\n",
    "\n",
    "                save_png_with_metadata(final_img_local, tmp_path_local, details_payload=None)\n",
    "                target_path_for_drive = os.path.join(OUTPUT_DIR, out_name_local)\n",
    "\n",
    "                info = upload_file_and_append_to_sheet(\n",
    "                    local_path       = tmp_path_local,\n",
    "                    target_colab_path= target_path_for_drive,\n",
    "                    sku_name         = sku_name,\n",
    "                    angle            = angle_code,\n",
    "                    spreadsheet_id   = SPREADSHEET_ID,\n",
    "                    worksheet_name   = GEN_LOG_SHEET,\n",
    "                )\n",
    "                print(f\"      ✅ Uploaded [{stage_label}] → {info['file_url']}\")\n",
    "                return final_img_local\n",
    "\n",
    "            main_result = reuse_main_img\n",
    "            skipped_main = reuse_main_img is not None\n",
    "\n",
    "            if not skipped_main:\n",
    "                garment_img = flatten_alpha_to_white(open_upright(garment_path))\n",
    "                base_full   = Image.open(base_img_path).convert(\"RGB\")\n",
    "                mask_label  = None\n",
    "\n",
    "                try:\n",
    "                    mask_full, mask_label = generate_mask_with_gemini(base_img_path, folder_path)\n",
    "                except Exception as mask_err:\n",
    "                    if mask_path:\n",
    "                        print(f\"      ⚠️ SAM3 mask failed for '{target_angle}', using disk mask instead: {mask_err}\")\n",
    "                        mask_full = Image.open(mask_path).convert(\"L\")\n",
    "                        mask_label = os.path.basename(mask_path)\n",
    "                        mask_dir = os.path.dirname(mask_path)\n",
    "                    else:\n",
    "                        print(f\"      ❌ SAM3 mask failed and no local mask for '{target_angle}': {mask_err}\")\n",
    "                        continue\n",
    "\n",
    "                main_result = perform_tryon_stage(\n",
    "                    stage_base_full=base_full,\n",
    "                    stage_mask_img=mask_full,\n",
    "                    stage_garment_img=garment_img,\n",
    "                    suffix=main_suffix,\n",
    "                    prompt_text=main_prompt_text,\n",
    "                    stage_label=\"main\",\n",
    "                    mask_label=mask_label or \"gemini-mask\",\n",
    "                    texture_card=main_texture_card,\n",
    "                    texture_label=\"Main texture reference\",\n",
    "                )\n",
    "\n",
    "            if SECONDARY_GARMENT:\n",
    "                sec_mask_path = find_mask_path(mask_dir, f\"{target_angle}_sec\")\n",
    "\n",
    "                sec_garment_path = sec_garment_override or find_secondary_garment_path(folder_path, file)\n",
    "                if not sec_garment_path:\n",
    "                    print(f\"      ⚠️ Secondary garment missing for '{target_angle}' → kept main-only output.\")\n",
    "                    continue\n",
    "\n",
    "                if main_result is None:\n",
    "                    print(f\"      ❌ Cannot run secondary stage for '{target_angle}' without a main result.\")\n",
    "                    continue\n",
    "\n",
    "                sec_mask_full = None\n",
    "                sec_mask_label = None\n",
    "                try:\n",
    "                    sec_mask_full, sec_mask_label = generate_mask_with_gemini(\n",
    "                        base_img_path,\n",
    "                        folder_path,\n",
    "                        mask_variant=f\"{target_angle}_sec\",\n",
    "                    )\n",
    "                except Exception as sec_mask_err:\n",
    "                    if sec_mask_path:\n",
    "                        print(f\"      ⚠️ Secondary SAM3 mask failed for '{target_angle}', using disk mask instead: {sec_mask_err}\")\n",
    "                        sec_mask_full = Image.open(sec_mask_path).convert(\"L\")\n",
    "                        sec_mask_label = os.path.basename(sec_mask_path)\n",
    "                    else:\n",
    "                        print(f\"      ❌ Secondary SAM3 mask failed and no local mask for '{target_angle}': {sec_mask_err}\")\n",
    "                        continue\n",
    "\n",
    "                if sec_mask_full is None:\n",
    "                    if sec_mask_path:\n",
    "                        sec_mask_full = Image.open(sec_mask_path).convert(\"L\")\n",
    "                        sec_mask_label = os.path.basename(sec_mask_path)\n",
    "                    else:\n",
    "                        print(f\"      ⚠️ Secondary mask missing for '{target_angle}' → kept main-only output.\")\n",
    "                        continue\n",
    "\n",
    "                sec_garment_img = flatten_alpha_to_white(open_upright(sec_garment_path))\n",
    "\n",
    "                perform_tryon_stage(\n",
    "                    stage_base_full=main_result,\n",
    "                    prompt_text=secondary_prompt_text,\n",
    "                    stage_mask_img=sec_mask_full,\n",
    "                    stage_garment_img=sec_garment_img,\n",
    "                    suffix=\"_both\",\n",
    "                    stage_label=\"secondary\",\n",
    "                    mask_label=sec_mask_label or (os.path.basename(sec_mask_path) if sec_mask_path else \"gemini-mask\"),\n",
    "                    texture_card=secondary_texture_card,\n",
    "                    texture_label=\"Main texture reference\",\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzFKYPQO_RPb"
   },
   "outputs": [],
   "source": [
    "# --- Sheet-driven angle selection + runners ---\n",
    "\n",
    "def build_sku_folder_index(garments_root: str):\n",
    "    return { _norm_sku(os.path.basename(p)) : p for p in iter_sku_folders(garments_root) }\n",
    "\n",
    "def run_list():\n",
    "    targets, unmatched = resolve_targets(SKU_CSV, GARMENTS_ROOT)\n",
    "    if not targets:\n",
    "        print(\"⚠️ No matching SKU folders found.\")\n",
    "        if unmatched: print(\"Unmatched:\", \", \".join(unmatched))\n",
    "        return\n",
    "    print(f\"➡️  Will process {len(targets)} SKU(s).\")\n",
    "    for i, p in enumerate(targets, start=1):\n",
    "        name = os.path.basename(p)\n",
    "        print(f\"\\nSKU {i}/{len(targets)} ▶️  {name}\")\n",
    "        try:\n",
    "            process_one_garment_folder(p, allowed_angles=ALLOWED_BASES)\n",
    "            print(f\"✅ Finished: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {name}: {e}\")\n",
    "    if unmatched:\n",
    "        print(\"\\nℹ️  Unmatched identifiers:\")\n",
    "        for u in unmatched: print(\"   -\", u)\n",
    "    print(\"\\n🏁 List run complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sinwLlWu_aCm"
   },
   "source": [
    "# DISPATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJAcEUnW_Y7f"
   },
   "outputs": [],
   "source": [
    "# Dispatch\n",
    "run_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK_cKHr9Uutd"
   },
   "source": [
    "#UNASSIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXlc2wlNyNDm"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "O4KzsXGV_ee1",
    "umaizfLYv7ww",
    "D7pcfKd0_iB_",
    "w3WTzeGw_nEv",
    "HFMF2dYwIlyZ",
    "IU5VwKPQIody"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}