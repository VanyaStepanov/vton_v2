{"cells": [{"cell_type": "markdown", "metadata": {"id": "O4KzsXGV_ee1"}, "source": ["# SETUP (restart after this)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "m7ieaWBKfOOB", "outputId": "3f8cbb3e-7d55-4688-e1f5-43b9b1ad909e"}, "outputs": [], "source": ["!pip install piexif"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "m3aXFfY54iAH", "outputId": "deb0c1a4-1456-4da5-97e3-1122bbb46431"}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "# --- Setup: Google auth + Drive + NanoBanana Pro deps ---\n", "\n", "import sys, os, subprocess, textwrap, importlib\n", "\n", "from google.colab import auth, drive\n", "auth.authenticate_user()\n", "drive.mount('/content/drive')\n", "\n", "!pip -q install --upgrade pip\n", "!pip -q install -U \"google-genai>=1.40.0\" pillow numpy opencv-python-headless matplotlib gspread google-auth google-auth-oauthlib google-api-python-client piexif tqdm\n", "\n", "print(\"âœ… Setup done for NanoBanana Pro.\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "umaizfLYv7ww"}, "source": ["# Select angles"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "t4VplVzrviJT"}, "outputs": [], "source": ["fr_lft = True #@param {type:\"boolean\"}\n", "fr_rght = True #@param {type:\"boolean\"}\n", "fr_cl = True #@param {type:\"boolean\"}\n", "bc_lft = True #@param {type:\"boolean\"}\n", "bc_rght = True #@param {type:\"boolean\"}\n", "lft = True #@param {type:\"boolean\"}\n", "rght = True #@param {type:\"boolean\"}\n", "bc_ = True #@param {type:\"boolean\"}\n", "fr_ = True #@param {type:\"boolean\"}\n", "fr_cl_btm = False #@param {type:\"boolean\"}\n", "fr_cl_tp = False #@param {type:\"boolean\"}\n", "\n", "\n", "names = [\"fr_lft\",\"fr_rght\",\"fr_cl\",\"bc_lft\",\"bc_rght\",\"lft\",\"rght\",\"bc_\",\"fr_\",\"fr_cl_btm\",\"fr_cl_tp\"]\n", "ALLOWED_BASES = [n for n in names if locals()[n]]"]}, {"cell_type": "markdown", "metadata": {"id": "D7pcfKd0_iB_"}, "source": ["# CONFIG"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Sq87guNN-V3D", "outputId": "b20df0bb-4466-42ce-f698-e3ee7e6612d3"}, "outputs": [], "source": ["# --- Unified CONFIG ---\n", "\n", "# Selection mode: list only\n", "RUN_MODE = \"sku_list\"     #@param [\"sku_list\"]\n", "\n", "# For RUN_MODE == \"sku_list\"\n", "SKU_CSV = \"28748, 28920, 28747, 29018, 29095, 29094\"  #@param {type:\"string\"}\n", "\n", "# Paths\n", "BASE_PHOTOS_ROOT  = \"/content/drive/MyDrive/Dazzl/SikSilk/SKSLK_MODELS/\"\n", "GARMENTS_ROOT     = \"/content/drive/MyDrive/Dazzl/SikSilk/AlexGens/SikSilk/\"\n", "\n", "\n", "# Filename/dir policy\n", "VALID_EXTENSIONS  = (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")\n", "IGNORE_DIRS       = {\"old\", \"__MACOSX\", \".ds_store\", \"Ricardo\", \"toweling\"}\n", "SKIP_FILENAME_TOKENS_CSV   = \"mask, generated, freelance, _sec, _backup\"   # substrings to skip\n", "SKIP_BASENAME_SUFFIXES_CSV = \"_sec\"                             # stem endings to skip\n", "REQUIRE_CUT_IN_FILENAME    = True   #@param {type:\"boolean\"}\n", "\n", "# Cropping / paste-back (square 1:1, generous garment margin)\n", "CROP_PADDING      = 100        # px padding around garment when building crop\n", "UPPER_PADDING     = 100        # extra padding above garment\n", "HORIZ_PADDING     = 100        # horizontal padding\n", "MASK_EXPAND_PX    = 100        # outward growth before feather\n", "MASK_FEATHER_PX   = 40         # Gaussian sigma for feathering\n", "CROP_MIN_MARGIN   = 100        # minimum margin even if mask touches edge\n", "\n", "TARGET_ASPECT = (1, 1)         # enforce square crops for 1:1 generations\n", "\n", "# NanoBanana Pro (Google GenAI)\n", "NANOBANANA_MODEL_ID = \"gemini-3-pro-image-preview\"\n", "GEN_ASPECT_RATIO    = \"1:1\"\n", "GEN_IMAGE_SIZE      = \"4K\"\n", "TRYON_PROMPT = \"\"\"You are an expert virtual try-on AI. You will be given a 'model image' and a 'garment image'. Your task is to create a new photorealistic image where the person from the 'model image' is wearing the clothing from the 'garment image'.\n", "\n", "**Crucial Rules:**\n", "1.  **Complete Garment Replacement:** You MUST completely REMOVE and REPLACE the clothing item worn by the person in the 'model image' with the new garment. No part of the original clothing (e.g., collars, sleeves, patterns) should be visible in the final image.\n", "2.  **Preserve the Model:** The person's face, hair, body shape, and pose from the 'model image' MUST remain unchanged.\n", "3.  **Preserve the Background:** The entire background from the 'model image' MUST be preserved perfectly.\n", "4.  **Apply the Garment:** Realistically fit the new garment onto the person. It should adapt to their pose with natural folds, shadows, and lighting consistent with the original scene.\n", "5.  **Output:** Return ONLY the final, edited image. Do not include any text.\"\"\"\n", "\n", "# Sheet-related (Ops removed) kept only for Gen Log appends\n", "SPREADSHEET_ID = \"1Kbq9__sEUQiuDPuza5Xy_hRyIn8pUvmfFj6vhPBrp8Y\"\n", "GEN_LOG_SHEET  = \"Gen Log\"\n", "\n", "# Misc\n", "SHOW_VISUALS = True\n", "TIMEZONE     = \"Europe/Lisbon\"\n", "OPERATOR     = \"Ivan\"\n", "OUTPUT_DIR   = \"/content/drive/MyDrive/Dazzl/SikSilk/SS_OUTPUT_FOLDER/19oct\" #@param {type:\"string\"}\n", "\n", "\n", "# === Garment/type taxonomy (kept) ===\n", "ALLOWED_GARMENT_TYPES = [\n", "    \"hoodie\",\"jeans\",\"joggers\",\"shorts\",\"sweater\",\"swimwear\",\n", "    \"t-shirt\",\"shirts\",\"track top\",\"trousers\",\"twinset\",\"polo\",\"vests\",\"shirts\"\n", "]\n", "TOP_GARMENTS    = [\"t-shirt\",\"shirt\",\"sweater\",\"hoodie\",\"track top\",\"vest\"]\n", "BOTTOM_GARMENTS = [\"shorts\",\"jogger-trousers\",\"trousers\",\"jeans\",\"swimwear\"]\n", "TWINSET_TYPES   = [\"twinset\"]\n", "\n", "# === Details tokens ===\n", "ALLOWED_DETAIL_TYPES = [\"crest\",\"logo\",\"patch\"]\n", "\n", "# Angle sheet tokens (kept for compatibility, not used in v1.3)\n", "ANGLE_NEEDS_REGENERATE_TOKEN = \"Regenerate\"\n", "ENFORCE_BAN_SUBSTRINGS     = True\n", "BANNED_SUBSTRINGS_CSV      = \"wrong, pair, combo\"\n", "ENFORCE_REQUIRE_SUBSTRINGS = False\n", "REQUIRED_SUBSTRINGS_CSV    = \"\"\n", "REQUIRED_SUBSTRINGS_MODE   = \"ANY\"   # \"ANY\" | \"ALL\"\n", "\n", "print(\"âœ… Config ready for NanoBanana Pro v1.3\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "w3WTzeGw_nEv"}, "source": ["# UTILS"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kAeR9BUW-8Lp"}, "outputs": [], "source": ["# --- Core utilities: normalization, angles, walking, masks (agnostic-first) ---\n", "\n", "import os, re, fnmatch, math, uuid, pytz, random, gc, tempfile, traceback\n", "from datetime import datetime\n", "import numpy as np\n", "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter\n", "\n", "def normalize_sku_list(sku_csv: str) -> str:\n", "    skus = []\n", "    for raw in sku_csv.split(','):\n", "        sku = raw.strip().upper()\n", "        match = re.search(r'(\\d+)', sku)\n", "        if match:\n", "            sku_number = match.group(1)\n", "            skus.append(f\"SS-{sku_number}\")\n", "    # Return as CSV string\n", "    return \", \".join(skus)\n", "\n", "SKU_CSV = normalize_sku_list(SKU_CSV)\n", "\n", "# Parsers\n", "def _parse_csv_list(s):  return [x.strip().casefold() for x in (s or \"\").split(\",\") if x.strip()]\n", "BANNED_SUBSTRINGS       = _parse_csv_list(BANNED_SUBSTRINGS_CSV)\n", "REQUIRED_SUBSTRINGS     = _parse_csv_list(REQUIRED_SUBSTRINGS_CSV)\n", "SKIP_FILENAME_TOKENS    = set(_parse_csv_list(SKIP_FILENAME_TOKENS_CSV))\n", "SKIP_BASENAME_SUFFIXES  = tuple(_parse_csv_list(SKIP_BASENAME_SUFFIXES_CSV))\n", "\n", "# Normalizers\n", "def _norm_sku(s):\n", "    if s is None: return \"\"\n", "    s = str(s).replace(\"\\u00A0\",\" \")\n", "    s = \" \".join(s.split())\n", "    return s.casefold()\n", "\n", "def _norm_angle(s):\n", "    s = (s or \"\").strip().lower()\n", "    return s.strip(\"_ \").replace(\"-\", \"_\")\n", "\n", "# Angle aliases\n", "ANGLE_ALIASES = {\n", "    \"fr_cl\": [\"fr\", \"fr_\"],\n", "    #\"lft\":   [\"fr_lft\", \"bc_lft\"],\n", "    #\"rght\":  [\"fr_rght\", \"bc_rght\"],\n", "}\n", "\n", "# --- Helpers to keep outputs strict, sources flexible ---\n", "def expand_as_list(angles):\n", "    exp = list(expand_allowed_angles(angles))\n", "    exp = [_norm_angle(a) for a in exp]\n", "    exp.sort(key=len, reverse=True)  # prefer 'fr_cl' over 'fr'\n", "    return exp\n", "\n", "def pick_target_angle(source_angle: str, allowed_outputs: set) -> str | None:\n", "    s = _norm_angle(source_angle)\n", "    for target in allowed_outputs:\n", "        fam = {_norm_angle(x) for x in expand_allowed_angles([target])}\n", "        if s in fam:\n", "            return _norm_angle(target)\n", "    return None\n", "\n", "\n", "def expand_allowed_angles(angles):\n", "    expanded = set()\n", "    for a in (angles or []):\n", "        a_norm = _norm_angle(a)\n", "        expanded.add(a_norm)\n", "        for alt in ANGLE_ALIASES.get(a_norm, []):\n", "            expanded.add(_norm_angle(alt))\n", "    return expanded\n", "\n", "# Ignore set\n", "IGNORE_DIRS = {d.lower() for d in IGNORE_DIRS}\n", "\n", "# Walkers\n", "def _is_sku_folder(path: str) -> bool:\n", "    if os.path.basename(os.path.normpath(path)).lower() in IGNORE_DIRS:\n", "        return False\n", "    try:\n", "        for f in os.listdir(path):\n", "            if os.path.isfile(os.path.join(path, f)) and f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)):\n", "                return True\n", "    except Exception:\n", "        return False\n", "    return False\n", "\n", "def iter_sku_folders(root: str):\n", "    for dirpath, dirnames, filenames in os.walk(root):\n", "        dirnames[:] = [d for d in dirnames if d.lower() not in IGNORE_DIRS]\n", "        if any(f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)) for f in filenames):\n", "            yield dirpath\n", "\n", "def resolve_targets(idents_csv: str, garments_root: str):\n", "    \"\"\"\n", "    Accepts:\n", "      â€¢ Plain SKU names, relative paths (Category/Subcategory/SKU), or absolute dirs\n", "      â€¢ Glob patterns (e.g., 'Hoodies/*' or 'SKSLK_12*')\n", "      â€¢ Directories that are NOT SKU leaves â†’ expand to all descendant SKU leaves\n", "    \"\"\"\n", "    idents = [s.strip() for s in idents_csv.replace(\"\\n\", \",\").split(\",\") if s.strip()]\n", "    if not idents: return [], []\n", "\n", "    all_sku_dirs = list(iter_sku_folders(garments_root))\n", "    rel_map = {p: os.path.relpath(p, garments_root) for p in all_sku_dirs}\n", "    base_map = {p: os.path.basename(p) for p in all_sku_dirs}\n", "\n", "    seen, out, unmatched = set(), [], []\n", "    def add_path(p):\n", "        ap = os.path.abspath(p)\n", "        if os.path.isdir(ap):\n", "            if _is_sku_folder(ap):\n", "                if ap not in seen:\n", "                    seen.add(ap); out.append(ap)\n", "            else:\n", "                # Expand directory to all descendant SKU leaves\n", "                for leaf in iter_sku_folders(ap):\n", "                    a = os.path.abspath(leaf)\n", "                    if a not in seen:\n", "                        seen.add(a); out.append(a)\n", "\n", "    for ident in idents:\n", "        before = len(out)\n", "        # Absolute directory or SKU path\n", "        if os.path.isabs(ident) and os.path.isdir(ident):\n", "            add_path(ident)\n", "\n", "        # Relative under garments root (dir or SKU)\n", "        rel_candidate = os.path.join(garments_root, ident)\n", "        if os.path.exists(rel_candidate):\n", "            add_path(rel_candidate)\n", "\n", "        # Glob/pattern over known SKU leaves (by basename or relative path)\n", "        for p in all_sku_dirs:\n", "            if fnmatch.fnmatch(base_map[p], ident) or fnmatch.fnmatch(rel_map[p], ident):\n", "                add_path(p)\n", "\n", "        if len(out) == before:\n", "            unmatched.append(ident)\n", "\n", "    out.sort()\n", "    return out, unmatched\n", "\n", "# Base/mask location resolution\n", "def resolve_base_mask_dir(sku_folder: str,\n", "                          garments_root: str = GARMENTS_ROOT,\n", "                          base_root: str = BASE_PHOTOS_ROOT):\n", "    \"\"\"\n", "    Map .../GARMENTS_ROOT/Category/Subcategory/SKU â†’ .../BASE_ROOT/Category/Subcategory\n", "    With robust fallbacks.\n", "    \"\"\"\n", "    abs_sku = os.path.abspath(sku_folder)\n", "    abs_gar = os.path.abspath(garments_root)\n", "    try:\n", "        rel = os.path.relpath(abs_sku, abs_gar)\n", "    except Exception:\n", "        rel = None\n", "\n", "    if rel and not rel.startswith(\"..\"):\n", "        rel_parent = os.path.dirname(rel)\n", "        cand = os.path.join(base_root, rel_parent)\n", "        if os.path.isdir(cand): return cand\n", "\n", "    subcat = os.path.basename(os.path.dirname(abs_sku))\n", "    cat    = os.path.basename(os.path.dirname(os.path.dirname(abs_sku)))\n", "    cand2  = os.path.join(base_root, cat, subcat)\n", "    if os.path.isdir(cand2): return cand2\n", "\n", "    cand3  = os.path.join(base_root, subcat)\n", "    if os.path.isdir(cand3): return cand3\n", "    return None\n", "\n", "def _valid_ext(fname): return fname.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS))\n", "\n", "def _file_prefix_or_none(filename: str):\n", "    low = filename.lower()\n", "    for base in ALLOWED_BASES:\n", "        if low.startswith(base): return base\n", "    return None\n", "\n", "def _find_image_with_stem_and_suffix(directory, stem, suffix=\"\"):\n", "    if not directory or not os.path.isdir(directory):\n", "        return None\n", "    stem = stem.lower()\n", "    for file in os.listdir(directory):\n", "        fname, fext = os.path.splitext(file)\n", "        if fext.lower() in (\".png\",\".jpg\",\".jpeg\") and fname.lower() == f\"{stem}{suffix}\":\n", "            return os.path.join(directory, file)\n", "    return None\n", "\n", "# --- Existence check in Google Drive by Colab-style path ---\n", "def drive_file_exists_any_ext_at_colab_path(target_colab_path: str,\n", "                                            exts=(\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")) -> bool:\n", "    \"\"\"\n", "    Given a Colab-style *file* path (incl. a filename with any extension),\n", "    checks if a file with the SAME stem exists in the same folder with any of the allowed extensions.\n", "    \"\"\"\n", "    try:\n", "        parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(target_colab_path)\n", "        stem, _ = os.path.splitext(desired_name)\n", "        files = _list_children(parent_id, q_extra=\"\")  # list once; filter locally\n", "        allowed = {e.lower() for e in exts}\n", "        for f in files:\n", "            fname = f.get(\"name\", \"\")\n", "            s, e = os.path.splitext(fname)\n", "            if s == stem and e.lower() in allowed:\n", "                return True\n", "        return False\n", "    except Exception as e:\n", "        print(f\"âš ï¸ Ext-agnostic existence check failed for {target_colab_path}: {e}\")\n", "        return False\n", "\n", "\n", "# === NEW: mask finding with AGNOSTIC priority ===\n", "def find_mask_path(base_subcat_dir: str, stem_no_cut: str):\n", "    \"\"\"\n", "    Priority:\n", "      1) {stem}_mask_agnostic.(png|jpg|jpeg)\n", "      2) {stem}_mask.(png|jpg|jpeg)\n", "    \"\"\"\n", "    if not base_subcat_dir or not os.path.isdir(base_subcat_dir):\n", "        return None\n", "\n", "    candidates = []\n", "    if PREFER_AGNOSTIC_MASKS:\n", "      for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n", "          candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask_agnostic{ext}\"))\n", "    for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n", "        candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask{ext}\"))\n", "\n", "    for p in candidates:\n", "        if os.path.exists(p):\n", "            return p\n", "    return None\n", "\n", "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n", "# --- Aspect-ratio bbox (replaces square bbox usage) ---\n", "\n", "\n", "def find_aspect_bbox(\n", "    mask: Image.Image,\n", "    aspect: tuple[int,int] = (1,1),   # width:height, e.g. (1280,1600)\n", "    padding: int = 40,\n", "    upper_padding: int | None = None,\n", "    horiz_padding: int = 0,\n", "    min_margin: int | None = None,\n", "):\n", "    \"\"\"\n", "    Return a rectangular bbox [x0, y0, x1, y1] that fully contains the mask + padding\n", "    and matches the requested aspect ratio by expanding outward only.\n", "    \"\"\"\n", "    if min_margin is None:\n", "        try:\n", "            min_margin = int(MASK_EXPAND_PX + 3 * MASK_FEATHER_PX + 5)\n", "        except Exception:\n", "            min_margin = 40\n", "\n", "    m = np.array(mask.convert(\"L\"))\n", "    h, w = m.shape\n", "    ys, xs = np.where(m > 128)\n", "    if xs.size == 0:\n", "        raise ValueError(\"Mask has no white pixels!\")\n", "\n", "    x_min, x_max = int(xs.min()), int(xs.max())\n", "    y_min, y_max = int(ys.min()), int(ys.max())\n", "\n", "    if upper_padding is None:\n", "        upper_padding = padding\n", "\n", "    # Initial padded bbox\n", "    x0 = max(0, x_min - horiz_padding - min_margin)\n", "    x1 = min(w, x_max + horiz_padding + min_margin)\n", "    y0 = max(0, y_min - upper_padding - min_margin)\n", "    y1 = min(h, y_max + padding + min_margin)\n", "\n", "    bw, bh = (x1 - x0), (y1 - y0)\n", "    # Desired aspect as float\n", "    aw, ah = aspect\n", "    target_ar = float(aw) / float(max(1, ah))\n", "\n", "    # First pass: try to match aspect by expanding one dimension only\n", "    def expand_to_aspect(x0, y0, x1, y1):\n", "        bw = x1 - x0; bh = y1 - y0\n", "        cur_ar = bw / float(max(1, bh))\n", "        if cur_ar < target_ar:\n", "            # too tall â†’ need wider\n", "            need_w = int(np.ceil(target_ar * bh))\n", "            grow = max(0, need_w - bw)\n", "            left_grow  = min(x0, grow // 2)\n", "            right_grow = min(w - x1, grow - left_grow)\n", "            x0 -= left_grow; x1 += right_grow\n", "        elif cur_ar > target_ar:\n", "            # too wide â†’ need taller\n", "            need_h = int(np.ceil(bw / target_ar))\n", "            grow = max(0, need_h - bh)\n", "            top_grow    = min(y0, grow // 2)\n", "            bottom_grow = min(h - y1, grow - top_grow)\n", "            y0 -= top_grow; y1 += bottom_grow\n", "        return max(0,x0), max(0,y0), min(w,x1), min(h,y1)\n", "\n", "    x0, y0, x1, y1 = expand_to_aspect(x0, y0, x1, y1)\n", "\n", "    # Second pass: if a border capped us, re-try by expanding the other dimension\n", "    bw, bh = (x1 - x0), (y1 - y0)\n", "    cur_ar = bw / float(max(1, bh))\n", "    if abs(cur_ar - target_ar) > 1e-3:\n", "        if cur_ar < target_ar:\n", "            # could not widen enough â†’ try to grow height\n", "            need_h = int(np.ceil(bw / target_ar))\n", "            grow = max(0, need_h - bh)\n", "            top_grow    = min(y0, grow // 2)\n", "            bottom_grow = min(h - y1, grow - top_grow)\n", "            y0 -= top_grow; y1 += bottom_grow\n", "        else:\n", "            # could not heighten enough â†’ try to grow width\n", "            need_w = int(np.ceil(target_ar * bh))\n", "            grow = max(0, need_w - bw)\n", "            left_grow  = min(x0, grow // 2)\n", "            right_grow = min(w - x1, grow - left_grow)\n", "            x0 -= left_grow; x1 += right_grow\n", "\n", "    # Final clamp\n", "    x0, y0 = max(0, int(x0)), max(0, int(y0))\n", "    x1, y1 = min(w, int(x1)), min(h, int(y1))\n", "    return [x0, y0, x1, y1]\n", "\n", "\n", "# Alpha/white utilities for garment panel\n", "WHITE_RGB = (255,255,255)\n", "def flatten_alpha_to_white(img: Image.Image) -> Image.Image:\n", "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n", "        bg = Image.new(\"RGB\", img.size, WHITE_RGB)\n", "        bg.paste(img, mask=img.split()[-1])\n", "        return bg\n", "    return img.convert(\"RGB\")\n", "\n", "def _tight_bbox_nonwhite_or_opaque(img: Image.Image):\n", "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n", "        arr = np.asarray(img.convert(\"RGBA\"))\n", "        alpha = arr[...,3]\n", "        fg = alpha > 0\n", "    else:\n", "        arr = np.asarray(img.convert(\"RGB\"))\n", "        fg = ~((arr[...,0]==255)&(arr[...,1]==255)&(arr[...,2]==255))\n", "    if not np.any(fg): return None\n", "    ys, xs = np.where(fg)\n", "    x0, x1 = int(xs.min()), int(xs.max())+1\n", "    y0, y1 = int(ys.min()), int(ys.max())+1\n", "    return (x0,y0,x1,y1)\n", "\n", "def crop_garment_keep_aspect(img: Image.Image) -> Image.Image:\n", "    bbox = _tight_bbox_nonwhite_or_opaque(img)\n", "    base = flatten_alpha_to_white(img)\n", "    if bbox is None: return base\n", "    full_bbox = (0,0,base.width,base.height)\n", "    if bbox == full_bbox: return base\n", "    return base.crop(bbox)\n", "\n", "def to_centered_square(gar: Image.Image, fill=WHITE_RGB) -> Image.Image:\n", "    w,h = gar.size; side = max(w,h)\n", "    sq = Image.new(\"RGB\", (side, side), fill)\n", "    ox, oy = (side-w)//2, (side-h)//2\n", "    sq.paste(gar, (ox,oy)); return sq\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_MqzqMK-h9Qo"}, "outputs": [], "source": ["# GPT evaluation removed for NanoBanana Pro (v1.3)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "MbJ9VKUEzhaP"}, "outputs": [], "source": ["# --- Visualisation helpers (restored) ---\n", "\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "from PIL import Image, ImageOps\n", "\n", "def open_upright(path) -> Image.Image:\n", "    # EXIF-aware loader (same as before)\n", "    with Image.open(path) as im:\n", "        return ImageOps.exif_transpose(im)\n", "\n", "def show_gallery(img_list, titles=None, cols=3, w=4):\n", "    \"\"\"\n", "    Display PIL images in a flexible grid (identical behaviour to your original).\n", "    Only renders if SHOW_VISUALS is True.\n", "    \"\"\"\n", "    if not globals().get(\"SHOW_VISUALS\", False):\n", "        return\n", "\n", "    n = len(img_list)\n", "    rows = math.ceil(n / cols)\n", "    plt.figure(figsize=(cols * w, rows * w))\n", "\n", "    for i, img in enumerate(img_list):\n", "        plt.subplot(rows, cols, i + 1)\n", "        # Accept PIL, torch tensors or numpy arrays (4-D batch â‡’ pick first)\n", "        if isinstance(img, np.ndarray) and img.ndim == 4:\n", "            img = img[0]  # (B,H,W,C) â†’ (H,W,C)\n", "        # Torch tensors are printed via duck-typing check to avoid hard import\n", "        if \"Tensor\" in str(type(img)):\n", "            img = img.detach().cpu().permute(1, 2, 0).numpy()\n", "        plt.imshow(img)\n", "        if titles and i < len(titles):\n", "            plt.title(titles[i])\n", "        plt.axis('off')\n", "\n", "    plt.tight_layout()\n", "    plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "QOhtSL7B-_-f"}, "outputs": [], "source": ["# --- Paste-back (mask-aware) + crop helpers ---\n", "\n", "import cv2\n", "\n", "def paste_crop_back(\n", "    full_img: Image.Image,\n", "    edited_crop: Image.Image,\n", "    crop_box,               # (x0,y0,x1,y1) in full_img coords\n", "    crop_mask: np.ndarray,  # HÃ—W uint8/bool, garment=white within crop_box\n", "    expand_px: int = 20,    # outward dilation before feather (mask-aware)\n", "    feather_px: int = 10,   # Gaussian Ïƒ for feathering (outside only)\n", "    *,\n", "    bin_thresh: int = 127,\n", "    edge_kill_px: int | None = None,  # will be capped by actual margin\n", "    retry_expand_px: int = 30,        # how much to enlarge bbox if needed\n", "):\n", "    x0, y0, x1, y1 = map(int, crop_box)\n", "    tgt_w, tgt_h   = (x1 - x0), (y1 - y0)\n", "\n", "    # Resize edit to crop size\n", "    edit_rs = edited_crop.resize((tgt_w, tgt_h), Image.Resampling.LANCZOS)\n", "\n", "    # --- 1) Binary mask in crop coordinates\n", "    mask_np = crop_mask\n", "    if isinstance(mask_np, Image.Image):\n", "        mask_np = np.array(mask_np.convert(\"L\"))\n", "    if mask_np.ndim == 3:\n", "        mask_np = mask_np[..., 0]\n", "    mask_np = cv2.resize(mask_np, (tgt_w, tgt_h), interpolation=cv2.INTER_NEAREST)\n", "    mask_bin = (mask_np > bin_thresh).astype(np.uint8)\n", "\n", "    # --- 2) Build outside-only feather band\n", "    if expand_px > 0:\n", "        ksize = max(1, expand_px * 2 + 1)\n", "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n", "        dil = cv2.dilate(mask_bin, kernel, iterations=1)\n", "    else:\n", "        dil = mask_bin.copy()\n", "\n", "    outside = (dil - mask_bin).clip(0, 1).astype(np.float32) * 255.0\n", "    if feather_px > 0:\n", "        outside = cv2.GaussianBlur(outside, (0, 0), sigmaX=feather_px, sigmaY=feather_px)\n", "\n", "    # --- 3) Edge-kill: CAP by actual margin so we only taper near rectangle borders\n", "    margin_left   = x0\n", "    margin_right  = full_img.width  - x1\n", "    margin_top    = y0\n", "    margin_bottom = full_img.height - y1\n", "    max_safe_edgekill = max(2, min(margin_left, margin_right, margin_top, margin_bottom))\n", "\n", "    if edge_kill_px is None:\n", "        edge_kill_px = int(expand_px + 3 * feather_px)\n", "    edge_kill_px = min(int(edge_kill_px), int(max_safe_edgekill))\n", "\n", "    # apply taper only within edge_kill band near the crop edges (no global attenuation)\n", "    yy, xx = np.mgrid[0:tgt_h, 0:tgt_w]\n", "    dist_edge = np.minimum.reduce([xx, tgt_w - 1 - xx, yy, tgt_h - 1 - yy]).astype(np.float32)\n", "\n", "    edge_factor = np.ones_like(dist_edge, np.float32)\n", "    band = dist_edge < float(edge_kill_px)\n", "    edge_factor[band] = dist_edge[band] / float(max(1.0, edge_kill_px))\n", "    outside *= edge_factor\n", "\n", "    # --- 4) Final alpha: solid interior + tapered outside (no inward feathering)\n", "    alpha = np.zeros((tgt_h, tgt_w), np.float32)\n", "    alpha[mask_bin > 0] = 255.0\n", "    alpha += outside\n", "    alpha = np.clip(alpha, 0, 255).astype(np.uint8)\n", "\n", "    # --- 5) Leakage check:\n", "    leaks_top    = alpha[0, :].max() > 0\n", "    leaks_bottom = alpha[-1, :].max() > 0\n", "    leaks_left   = alpha[:, 0].max() > 0\n", "    leaks_right  = alpha[:, -1].max() > 0\n", "\n", "    needs_retry = False\n", "    if leaks_top    and y0 > 0:                      needs_retry = True\n", "    if leaks_bottom and y1 < full_img.height:        needs_retry = True\n", "    if leaks_left   and x0 > 0:                      needs_retry = True\n", "    if leaks_right  and x1 < full_img.width:         needs_retry = True\n", "\n", "    if needs_retry:\n", "        print(\"âš ï¸  alpha touches crop border (inside canvas) â†’ retrying with expanded bbox...\")\n", "        x0n = max(0, x0 - retry_expand_px)\n", "        y0n = max(0, y0 - retry_expand_px)\n", "        x1n = min(full_img.width,  x1 + retry_expand_px)\n", "        y1n = min(full_img.height, y1 + retry_expand_px)\n", "        new_box = [x0n, y0n, x1n, y1n]\n", "\n", "        full_mask = np.zeros((full_img.height, full_img.width), np.uint8)\n", "        full_mask[y0:y1, x0:x1] = (mask_bin * 255).astype(np.uint8)\n", "        region_mask_crop = full_mask[y0n:y1n, x0n:x1n]\n", "\n", "        return paste_crop_back(\n", "            full_img   = full_img,\n", "            edited_crop= edited_crop,\n", "            crop_box   = new_box,\n", "            crop_mask  = region_mask_crop,\n", "            expand_px  = expand_px,\n", "            feather_px = feather_px,\n", "            bin_thresh = bin_thresh,\n", "            edge_kill_px = edge_kill_px + 10,\n", "            retry_expand_px = retry_expand_px\n", "        )\n", "\n", "    # --- 6) Composite\n", "    mask_img = Image.fromarray(alpha, mode=\"L\")\n", "    region   = full_img.crop((x0, y0, x1, y1))\n", "    comp     = Image.composite(edit_rs, region, mask_img)\n", "    full_img.paste(comp, (x0, y0))\n", "    return full_img\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Br9dskEm_CBL"}, "outputs": [], "source": ["\n", "# --- NanoBanana Pro try-on (Google Cloud GenAI) ---\n", "\n", "import os, io\n", "from google import genai\n", "from google.genai import types\n", "from PIL import Image\n", "\n", "def _load_gemini_api_key():\n", "    key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GEMINI_APIKEY\")\n", "    try:\n", "        from google.colab import userdata\n", "        key = key or userdata.get(\"GEMINI_API_KEY\")\n", "    except Exception:\n", "        pass\n", "    if not key:\n", "        raise ValueError(\"Set GEMINI_API_KEY in environment or Colab userdata.\")\n", "    return key\n", "\n", "genai_client = genai.Client(api_key=_load_gemini_api_key())\n", "\n", "def _extract_first_image(resp):\n", "    parts = []\n", "    if hasattr(resp, \"parts\"):\n", "        parts.extend(resp.parts)\n", "    for cand in getattr(resp, \"candidates\", []):\n", "        content = getattr(cand, \"content\", None)\n", "        if content and hasattr(content, \"parts\"):\n", "            parts.extend(content.parts or [])\n", "\n", "    for part in parts:\n", "        if isinstance(part, Image.Image):\n", "            return part\n", "        as_image = getattr(part, \"as_image\", None)\n", "        if callable(as_image):\n", "            img = as_image()\n", "            if isinstance(img, Image.Image):\n", "                return img\n", "        inline = getattr(part, \"inline_data\", None)\n", "        data = getattr(inline, \"data\", None) if inline else None\n", "        if data:\n", "            try:\n", "                return Image.open(io.BytesIO(data))\n", "            except Exception:\n", "                pass\n", "    raise ValueError(\"No image returned from NanoBanana Pro response.\")\n", "\n", "def run_nanobanana_tryon(model_image: Image.Image, garment_image: Image.Image,\n", "                         *, aspect_ratio: str = GEN_ASPECT_RATIO,\n", "                         image_size: str = GEN_IMAGE_SIZE,\n", "                         prompt: str | None = None):\n", "    prompt_text = prompt or TRYON_PROMPT\n", "    resp = genai_client.models.generate_content(\n", "        model=NANOBANANA_MODEL_ID,\n", "        contents=[\n", "            prompt_text,\n", "            \"Model image:\",\n", "            model_image.convert(\"RGB\"),\n", "            \"Garment image:\",\n", "            garment_image.convert(\"RGB\"),\n", "        ],\n", "        config=types.GenerateContentConfig(\n", "            response_modalities=[\"IMAGE\"],\n", "            image_config=types.ImageConfig(\n", "                aspect_ratio=aspect_ratio,\n", "                image_size=image_size,\n", "            ),\n", "        ),\n", "    )\n", "    img = _extract_first_image(resp)\n", "    return img.convert(\"RGB\")\n", "\n", "print(\"âœ… NanoBanana Pro client ready.\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "YfNf3NHxjMaj"}, "outputs": [], "source": ["# Colab cell â€” output routing + metadata helpers\n", "import os, json\n", "from PIL import PngImagePlugin\n", "\n", "def ensure_dir(p):\n", "    os.makedirs(p, exist_ok=True); return p\n", "\n", "ensure_dir(OUTPUT_DIR)\n", "\n", "def build_output_filename(sku_name: str, angle_code: str, ext=\".png\") -> str:\n", "    # Examples: SS-12345-fr_rght or SS-12345-bc_lft\n", "    angle_clean = _norm_angle(angle_code)\n", "    return f\"{sku_name}-{angle_clean}{ext}\"\n", "\n", "\n", "import json, piexif\n", "from PIL import Image\n", "\n", "def save_png_with_metadata(img, out_path, details_payload=None, quality=95):\n", "    if details_payload:\n", "        # Encode JSON as UTF-8 with an ASCII prefix per EXIF spec for UserComment\n", "        payload = json.dumps(details_payload, ensure_ascii=False).encode(\"utf-8\")\n", "        user_comment = b\"ASCII\\x00\\x00\\x00\" + payload  # indicates undefined/UTF-8\n", "        exif_dict = {\"0th\": {}, \"Exif\": {piexif.ExifIFD.UserComment: user_comment}, \"1st\": {}, \"GPS\": {}, \"Interop\": {}}\n", "        exif_bytes = piexif.dump(exif_dict)\n", "        img.save(out_path, format=\"PNG\", exif=exif_bytes)\n", "    else:\n", "        img.save(out_path, format=\"PNG\")\n", "\n", "import json, piexif\n", "from PIL import Image, ImageOps\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "9oGZnx_6_EP7"}, "outputs": [], "source": ["# HYPIR removed in NanoBanana Pro v1.3\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "sBN_kgOo_GRI"}, "outputs": [], "source": ["# --- Google APIs: gspread + Drive upload (Operations sync removed) ---\n", "\n", "import google.auth\n", "SCOPES = [\"https://www.googleapis.com/auth/drive\", \"https://www.googleapis.com/auth/spreadsheets\"]\n", "creds, _ = google.auth.default(scopes=SCOPES)\n", "\n", "import gspread\n", "gs = gspread.authorize(creds)\n", "\n", "from googleapiclient.discovery import build\n", "from googleapiclient.http import MediaFileUpload\n", "drive_svc = build(\"drive\", \"v3\", credentials=creds)\n", "\n", "FOLDER_MIME   = \"application/vnd.google-apps.folder\"\n", "SHORTCUT_MIME = \"application/vnd.google-apps.shortcut\"\n", "PATH_PREFIX   = \"/content/drive/MyDrive/\"\n", "\n", "def _escape_name(name: str) -> str: return name.replace(\"'\", r\"'\")\n", "\n", "def _maybe_follow_shortcut(file_obj):\n", "    if file_obj.get(\"mimeType\") == SHORTCUT_MIME:\n", "        sd = file_obj.get(\"shortcutDetails\", {}) or {}\n", "        return sd.get(\"targetId\"), sd.get(\"targetMimeType\")\n", "    return file_obj.get(\"id\"), file_obj.get(\"mimeType\")\n", "\n", "def _list_children(parent_id: str, q_extra: str, page_size: int = 1000):\n", "    q = f\"'{parent_id}' in parents and trashed = false\"\n", "    if q_extra: q += f\" and ({q_extra})\"\n", "    resp = drive_svc.files().list(\n", "        q=q, spaces=\"drive\", pageSize=page_size,\n", "        fields=\"files(id,name,mimeType,shortcutDetails)\",\n", "        includeItemsFromAllDrives=True, supportsAllDrives=True,\n", "    ).execute()\n", "    return resp.get(\"files\", [])\n", "\n", "def _find_folder_id(parent_id: str, name: str):\n", "    files = _list_children(\n", "        parent_id,\n", "        q_extra=(\n", "            f\"name = '{_escape_name(name)}' and \"\n", "            f\"(mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\"\n", "        ),\n", "    )\n", "\n", "    # Direct folder match\n", "    for f in files:\n", "        if f[\"mimeType\"] == FOLDER_MIME:\n", "            return f[\"id\"]\n", "\n", "    # Shortcut to folder\n", "    for f in files:\n", "        if f[\"mimeType\"] == SHORTCUT_MIME:\n", "            tid, tmime = _maybe_follow_shortcut(f)\n", "            if tmime == FOLDER_MIME:\n", "                return tid\n", "\n", "    # Fallback: match by case-insensitive name\n", "    files = _list_children(\n", "        parent_id,\n", "        q_extra=(\n", "            f\"(mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\"\n", "        ),\n", "    )\n", "    needle = name.strip().casefold()\n", "\n", "    for f in files:\n", "        if f.get(\"name\", \"\").strip().casefold() == needle:\n", "            tid, tmime = _maybe_follow_shortcut(f)\n", "            if tmime == FOLDER_MIME:\n", "                return tid\n", "\n", "    return None\n", "\n", "\n", "def _resolve_parent_id_and_filename_from_colab_path(colab_path: str):\n", "    if not colab_path.startswith(PATH_PREFIX):\n", "        raise ValueError(\n", "            f\"This helper supports only '{PATH_PREFIX}...'. Got: {colab_path}\"\n", "        )\n", "\n", "    parts = colab_path[len(PATH_PREFIX):].strip(\"/\").split(\"/\")\n", "    if not parts:\n", "        raise ValueError(\"Path must include a file name.\")\n", "\n", "    parent_id = \"root\"\n", "\n", "    for part in parts[:-1]:\n", "        next_id = _find_folder_id(parent_id, part)\n", "        if not next_id:\n", "            raise FileNotFoundError(f\"Folder not found in path: '{part}'\")\n", "        parent_id = next_id\n", "\n", "    desired_name = parts[-1]\n", "    return parent_id, desired_name\n", "\n", "\n", "def upload_to_drive_folder(\n", "    local_path: str,\n", "    parent_folder_id: str,\n", "    desired_name: str | None = None\n", "):\n", "    media = MediaFileUpload(local_path, resumable=True)\n", "    body = {\n", "        \"name\": desired_name or os.path.basename(local_path),\n", "        \"parents\": [parent_folder_id],\n", "    }\n", "\n", "    file = (\n", "        drive_svc.files()\n", "        .create(\n", "            body=body,\n", "            media_body=media,\n", "            fields=\"id, webViewLink, name, parents\",\n", "            supportsAllDrives=True,\n", "        )\n", "        .execute()\n", "    )\n", "\n", "    drive_svc.permissions().create(\n", "        fileId=file[\"id\"],\n", "        body={\"type\": \"anyone\", \"role\": \"reader\"},\n", "        fields=\"id\",\n", "        supportsAllDrives=True,\n", "    ).execute()\n", "\n", "    return file\n", "\n", "\n", "def upload_file_and_append_to_sheet(\n", "    local_path: str,\n", "    target_colab_path: str,\n", "    sku_name: str,\n", "    angle: str,\n", "    spreadsheet_id: str | None,\n", "    worksheet_name: str | None,\n", "):\n", "    parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(\n", "        target_colab_path\n", "    )\n", "\n", "    uploaded = upload_to_drive_folder(local_path, parent_id, desired_name)\n", "    file_id = uploaded[\"id\"]\n", "\n", "    file_url = (\n", "        uploaded.get(\"webViewLink\")\n", "        or f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n", "    )\n", "    folder_url = f\"https://drive.google.com/drive/folders/{parent_id}\"\n", "\n", "    if spreadsheet_id and worksheet_name:\n", "        ts = datetime.now(pytz.timezone(TIMEZONE)).strftime(\"%m-%d %H:%M:%S\")\n", "        uid = str(uuid.uuid4())\n", "\n", "        sh = gs.open_by_key(spreadsheet_id)\n", "        ws = sh.worksheet(worksheet_name)\n", "\n", "        sku_cell = f'=HYPERLINK(\"{folder_url}\"; \"{sku_name}\")'\n", "\n", "        ws.append_row(\n", "            [\n", "                sku_cell,\n", "                angle,\n", "                ts,\n", "                file_url,\n", "                uid,\n", "                \"Girls need to check\",\n", "                OPERATOR,\n", "            ],\n", "            value_input_option=\"USER_ENTERED\",\n", "        )\n", "\n", "    return {\"file_url\": file_url}\n"]}, {"cell_type": "markdown", "metadata": {"id": "HFMF2dYwIlyZ"}, "source": ["#build pipe"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "5ZxAbw0MGeIp", "outputId": "6fbda67e-e626-4a37-cd07-b4f638db8035"}, "outputs": [], "source": ["print(\"NanoBanana Pro client initialized above; no local diffusion pipe needed.\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "IU5VwKPQIody"}, "source": ["# BATCH HELPERS"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "yQi5CzF2_O92"}, "outputs": [], "source": ["# --- Batch processor  ---\n", "\n", "def process_one_garment_folder(folder_path: str, allowed_angles=None):\n", "    allowed_outputs = {_norm_angle(a) for a in (allowed_angles or [])}\n", "    allowed_sources = expand_as_list(allowed_angles) if allowed_angles else None\n", "\n", "    base_subcat_dir = resolve_base_mask_dir(folder_path)\n", "    if not base_subcat_dir:\n", "        print(f\"âš ï¸ Cannot resolve base/mask dir for SKU: {folder_path}\")\n", "    files_sorted = sorted(os.listdir(folder_path))\n", "\n", "    worklist = []\n", "    for file in files_sorted:\n", "        low = file.lower()\n", "\n", "        if allowed_sources and not any(low.startswith(src) for src in allowed_sources):\n", "            continue\n", "        if SKIP_FILENAME_TOKENS and any(tok in low for tok in SKIP_FILENAME_TOKENS):\n", "            continue\n", "        if REQUIRE_CUT_IN_FILENAME and (\"cut\" not in low):\n", "            continue\n", "        if not _valid_ext(file):\n", "            continue\n", "\n", "        matched_source = next((src for src in allowed_sources if low.startswith(src)), None) if allowed_sources else None\n", "        if not matched_source:\n", "            continue\n", "        target_angle = pick_target_angle(matched_source, allowed_outputs) if allowed_outputs else _norm_angle(matched_source)\n", "        if allowed_outputs and not target_angle:\n", "            continue\n", "\n", "        base_img_path = _find_image_with_stem_and_suffix(base_subcat_dir, target_angle)\n", "        if not base_img_path:\n", "            print(f\"âš ï¸ Missing BASE for target '{target_angle}' â†’ skipping {file}\")\n", "            continue\n", "\n", "        mask_path = find_mask_path(base_subcat_dir, target_angle)\n", "        if not mask_path:\n", "            print(f\"âš ï¸ Missing MASK for target '{target_angle}' â†’ skipping {file}\")\n", "            continue\n", "\n", "        worklist.append((file, target_angle, base_img_path, mask_path))\n", "\n", "    sku_name = os.path.basename(folder_path)\n", "    if allowed_sources:\n", "        print(f\"â–¶ï¸  {sku_name}: {len(worklist)} image(s) to generate (outputs={sorted(list(allowed_outputs))}, sources={sorted(list(set(allowed_sources)))})\")\n", "    else:\n", "        print(f\"â–¶ï¸  {sku_name}: {len(worklist)} image(s) to generate\")\n", "\n", "    if not worklist:\n", "        return\n", "\n", "    for idx, (file, target_angle, base_img_path, mask_path) in enumerate(worklist, start=1):\n", "        print(f\"   {idx:>3}/{len(worklist):<3}  {file}  | USING STRICT base/mask='{target_angle}'\")\n", "        garment_path = os.path.join(folder_path, file)\n", "\n", "        sku_name = os.path.basename(folder_path)\n", "        angle_code = _norm_angle(target_angle)\n", "        out_stem   = f\"{sku_name}_{angle_code}\"\n", "        dest_check = os.path.join(OUTPUT_DIR, out_stem + \".png\")\n", "\n", "        if drive_file_exists_any_ext_at_colab_path(dest_check):\n", "            print(f\"      â­ï¸  Skip: {out_stem}.(png/jpg/jpeg) already exists in {OUTPUT_DIR}\")\n", "            continue\n", "\n", "        try:\n", "            garment_img = flatten_alpha_to_white(open_upright(garment_path))\n", "            base_full   = Image.open(base_img_path).convert(\"RGB\")\n", "            mask_full   = Image.open(mask_path).convert(\"L\")\n", "\n", "            show_gallery(\n", "                [garment_img, base_full, mask_full.convert(\"RGB\")],\n", "                [\"Source garment (white BG)\", f\"Base photo [{angle_code}]\", f\"Mask [{os.path.basename(mask_path)}]\"]\n", "            )\n", "\n", "            bbox = find_aspect_bbox(\n", "                mask_full,\n", "                aspect=TARGET_ASPECT,\n", "                padding=CROP_PADDING,\n", "                upper_padding=UPPER_PADDING,\n", "                horiz_padding=HORIZ_PADDING,\n", "                min_margin=CROP_MIN_MARGIN\n", "            )\n", "            base_crop = base_full.crop(tuple(bbox))\n", "            mask_crop = mask_full.crop(tuple(bbox))\n", "            show_gallery([base_crop, mask_crop.convert(\"RGB\"), garment_img], [\"Cropped base (1:1)\", \"Cropped mask\", \"Garment (white BG)\"])\n", "\n", "            tryon_gen = run_nanobanana_tryon(\n", "                model_image=base_crop,\n", "                garment_image=garment_img,\n", "                aspect_ratio=GEN_ASPECT_RATIO,\n", "                image_size=GEN_IMAGE_SIZE,\n", "                prompt=TRYON_PROMPT,\n", "            )\n", "            tryon_sq = tryon_gen.resize(base_crop.size, Image.LANCZOS)\n", "\n", "            final_img = paste_crop_back(\n", "                full_img   = base_full.copy(),\n", "                edited_crop= tryon_sq,\n", "                crop_box   = bbox,\n", "                crop_mask  = np.array(mask_crop),\n", "                expand_px  = MASK_EXPAND_PX,\n", "                feather_px = MASK_FEATHER_PX\n", "            )\n", "            show_gallery([final_img], [\"Final paste-back\"])\n", "\n", "            out_name = build_output_filename(sku_name, angle_code, ext=\".png\")\n", "            tmp_path = os.path.join(\"/tmp\", out_name)\n", "\n", "            save_png_with_metadata(final_img, tmp_path, details_payload=None)\n", "            target_path_for_drive = os.path.join(OUTPUT_DIR, out_name)\n", "\n", "            info = upload_file_and_append_to_sheet(\n", "                local_path       = tmp_path,\n", "                target_colab_path= target_path_for_drive,\n", "                sku_name         = sku_name,\n", "                angle            = angle_code,\n", "                spreadsheet_id   = SPREADSHEET_ID,\n", "                worksheet_name   = GEN_LOG_SHEET,\n", "            )\n", "            print(f\"      âœ… Uploaded â†’ {info['file_url']}\")\n", "\n", "        except Exception as e:\n", "            print(f\"      âŒ Error: {e}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "zzFKYPQO_RPb"}, "outputs": [], "source": ["# --- Sheet-driven angle selection + runners ---\n", "\n", "def build_sku_folder_index(garments_root: str):\n", "    return { _norm_sku(os.path.basename(p)) : p for p in iter_sku_folders(garments_root) }\n", "\n", "def run_list():\n", "    targets, unmatched = resolve_targets(SKU_CSV, GARMENTS_ROOT)\n", "    if not targets:\n", "        print(\"âš ï¸ No matching SKU folders found.\")\n", "        if unmatched: print(\"Unmatched:\", \", \".join(unmatched))\n", "        return\n", "    print(f\"âž¡ï¸  Will process {len(targets)} SKU(s).\")\n", "    for i, p in enumerate(targets, start=1):\n", "        name = os.path.basename(p)\n", "        print(f\"\\nSKU {i}/{len(targets)} â–¶ï¸  {name}\")\n", "        try:\n", "            process_one_garment_folder(p, allowed_angles=ALLOWED_BASES)\n", "            print(f\"âœ… Finished: {name}\")\n", "        except Exception as e:\n", "            print(f\"âŒ Error in {name}: {e}\")\n", "    if unmatched:\n", "        print(\"\\nâ„¹ï¸  Unmatched identifiers:\")\n", "        for u in unmatched: print(\"   -\", u)\n", "    print(\"\\nðŸ List run complete.\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "sinwLlWu_aCm"}, "source": ["# DISPATCH"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"background_save": true, "base_uri": "https://localhost:8080/", "height": 974, "referenced_widgets": ["42847e933f2342d5b001324d78fa41ba", "3ef487a23fa9411cbeb2c630dc358f80", "07103d96144e433b8a0dec5bd6484757", "11643dbc95be4d0f8ea22e6c4a3dcc8c", "fa063be14ccb427bb2040cddb43fd503", "2c40d233898c4dbda46119818a35436f", "ee7070ccbdaf4e3aa757c2a5ce84176e", "64da8b050b0a476ea28bf518acdc0c76", "7f270ea1b62e4342b74bd5aa6d837688", "0e5bbb730420408481535e6642f07103", "ea5c3cb9e1384e0a83368ec0a9722a08", "35738ab2bb0b4590887a664d4aa26f81", "6fe776ef2ac04ed091e68da7542f8a08", "667bafd29929415299a43d3b05badedd", "8dccf0878499432191fdda978ee08717", "c49a3468678a49fbbf9b9a6a5043e8ad", "9e4d49277f394596a2113d87da135768", "42ecabbfa2b04facb5666aa2fb1c33ef", "007af91ddf3b4405822cb75f7af6719a", "3237d2f74c1343f5bb24f7b47209f2db", "38d6a9c0ea094219b04fb159a53b4c7d", "ee822cdabe9b404e8b32d6904deabf45", "6ef65ee6043c4670b0d3170f2b861c4b", "c2b2951c47674792b0dd7b2bce037270", "9e489d6b38fb4f5bbba1a55ed1df0f76", "6591bcf9baa64e149e749a71a75aaff3", "fa543e5f37524774b9305493d21ef976"]}, "id": "FJAcEUnW_Y7f", "outputId": "3ccb008d-6106-45e0-80de-3c1fcc1b25c8"}, "outputs": [], "source": ["# Dispatch\n", "run_list()\n"]}, {"cell_type": "markdown", "metadata": {"id": "CK_cKHr9Uutd"}, "source": ["#UNASSIGN"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "TXlc2wlNyNDm"}, "outputs": [], "source": ["from google.colab import runtime\n", "runtime.unassign()"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": ["O4KzsXGV_ee1", "umaizfLYv7ww", "D7pcfKd0_iB_", "w3WTzeGw_nEv", "HFMF2dYwIlyZ", "IU5VwKPQIody"], "gpuType": "A100", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}