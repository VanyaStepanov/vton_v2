{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4KzsXGV_ee1"
      },
      "source": [
        "# SETUP (restart after this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7ieaWBKfOOB",
        "outputId": "3f8cbb3e-7d55-4688-e1f5-43b9b1ad909e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting piexif\n",
            "  Downloading piexif-1.1.3-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading piexif-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: piexif\n",
            "Successfully installed piexif-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install piexif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3aXFfY54iAH",
        "outputId": "deb0c1a4-1456-4da5-97e3-1122bbb46431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into 'catvton-flux'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 331 (delta 61), reused 57 (delta 57), pack-reused 249 (from 1)\u001b[K\n",
            "Receiving objects: 100% (331/331), 17.24 MiB | 12.46 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n",
            "Cloning into 'HYPIR'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 175 (delta 57), reused 57 (delta 43), pack-reused 91 (from 1)\u001b[K\n",
            "Receiving objects: 100% (175/175), 7.26 MiB | 25.54 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Setup done.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# --- Setup: Google auth + Drive + repos + deps ---\n",
        "\n",
        "import sys, os, subprocess, textwrap, importlib\n",
        "\n",
        "# Colab auth + Drive\n",
        "from google.colab import auth, drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Repos (only what we actually need)\n",
        "!git clone https://github.com/nftblackmagic/catvton-flux || true\n",
        "!git clone https://github.com/XPixelGroup/HYPIR.git || true\n",
        "\n",
        "# Deps (pin modern, stable)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"diffusers>=0.31.0\" \"accelerate>=0.33.0\" peft==0.17.0 \\\n",
        "                pillow torchvision safetensors einops numpy==1.26.4 \\\n",
        "                gspread google-auth google-auth-oauthlib google-api-python-client \\\n",
        "                pytz\n",
        "!pip -q install --force-reinstall \"transformers==4.46.2\"\n",
        "\n",
        "# Optional: HYPIR weight (kept small & lazy-used)\n",
        "!wget -q https://huggingface.co/lxq007/HYPIR/resolve/main/HYPIR_sd2.pth -O /content/HYPIR_sd2.pth\n",
        "\n",
        "# Colab cell — install OpenAI SDK with Responses API\n",
        "!pip -q install --upgrade \"openai>=1.52.0\"\n",
        "\n",
        "\n",
        "print(\"✅ Setup done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umaizfLYv7ww"
      },
      "source": [
        "# Select angles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4VplVzrviJT"
      },
      "outputs": [],
      "source": [
        "fr_lft = True #@param {type:\"boolean\"}\n",
        "fr_rght = True #@param {type:\"boolean\"}\n",
        "fr_cl = True #@param {type:\"boolean\"}\n",
        "bc_lft = True #@param {type:\"boolean\"}\n",
        "bc_rght = True #@param {type:\"boolean\"}\n",
        "lft = True #@param {type:\"boolean\"}\n",
        "rght = True #@param {type:\"boolean\"}\n",
        "bc_ = True #@param {type:\"boolean\"}\n",
        "fr_ = True #@param {type:\"boolean\"}\n",
        "fr_cl_btm = False #@param {type:\"boolean\"}\n",
        "fr_cl_tp = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "names = [\"fr_lft\",\"fr_rght\",\"fr_cl\",\"bc_lft\",\"bc_rght\",\"lft\",\"rght\",\"bc_\",\"fr_\",\"fr_cl_btm\",\"fr_cl_tp\"]\n",
        "ALLOWED_BASES = [n for n in names if locals()[n]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7pcfKd0_iB_"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq87guNN-V3D",
        "outputId": "b20df0bb-4466-42ce-f698-e3ee7e6612d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GPT config & folders ready.\n",
            "✅ Few-shot eval messages loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- Unified CONFIG ---\n",
        "\n",
        "# Selection mode: \"sheet\" | \"sku_list\" | \"dir\"\n",
        "RUN_MODE = \"sku_list\"     #@param [\"sheet\", \"sku_list\", \"dir\"]\n",
        "\n",
        "# For RUN_MODE == \"sku_list\"\n",
        "SKU_CSV = \"28748, 28920, 28747, 29018, 29095, 29094\"  #@param {type:\"string\"}\n",
        "\n",
        "# For RUN_MODE == \"dir\": an absolute or relative (under GARMENTS_ROOT) directory\n",
        "TARGET_DIR = \"/content/drive/MyDrive/Dazzl/SikSilk/AlexGens/SikSilk\"  #@param {type:\"string\"}\n",
        "\n",
        "# Paths\n",
        "BASE_PHOTOS_ROOT  = \"/content/drive/MyDrive/Dazzl/SikSilk/SKSLK_MODELS/\"\n",
        "GARMENTS_ROOT     = \"/content/drive/MyDrive/Dazzl/SikSilk/AlexGens/SikSilk/\"\n",
        "\n",
        "\n",
        "# Filename/dir policy\n",
        "VALID_EXTENSIONS  = (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")\n",
        "IGNORE_DIRS       = {\"old\", \"__MACOSX\", \".ds_store\", \"Ricardo\", \"toweling\"}\n",
        "SKIP_FILENAME_TOKENS_CSV   = \"mask, generated, freelance, _sec, _backup\"   # substrings to skip\n",
        "SKIP_BASENAME_SUFFIXES_CSV = \"_sec\"                             # stem endings to skip\n",
        "REQUIRE_CUT_IN_FILENAME    = True   #@param {type:\"boolean\"}\n",
        "\n",
        "# Cropping / paste-back\n",
        "CROP_PADDING      = 100        # px above selection\n",
        "UPPER_PADDING     = 200        # extra padding below bbox\n",
        "MASK_EXPAND_PX    = 100        # outward growth before feather\n",
        "MASK_FEATHER_PX   = 40         # Gaussian sigma for feathering\n",
        "BRIGHTNESS_FACTOR = 0.95\n",
        "\n",
        "# Inference (CatVTON+LoRA over Flux Fill)\n",
        "INFERENCE_MODE    = \"lora\"\n",
        "WIDTH, HEIGHT     = 1280, 1600\n",
        "STEPS             = 75 #@param {type:\"number\"}\n",
        "GUIDANCE          = 47 #@param {type:\"number\"}\n",
        "FILL_MODEL_ID     = \"black-forest-labs/FLUX.1-dev\"\n",
        "CATVTON_XFM       = \"xiaozaa/catvton-flux-beta\"\n",
        "#LORA_PATH         = \"/content/drive/MyDrive/Dazzl/SikSilk/Jeans_LORA/LORA_models/jeans_LORA_4_lowcfg_step400\" #@param {type: \"string\"}\n",
        "#LORA_PATH         = \"/content/drive/MyDrive/Dazzl/SikSilk/their_dataset_LORA/1280_their_ds_LORA_10_w_jitter/last\" #@param {type: \"string\"}\n",
        "LORA_PATH         = \"/content/drive/MyDrive/Dazzl/SikSilk/their_dataset_LORA/4x5_1280_their_ds_LORA_13_w_jitter/4x5_1280_their_ds_LORA_13_w_jitter_best\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "PREFER_AGNOSTIC_MASKS = True #@param {type:\"boolean\"}\n",
        "\n",
        "# LoRA schedule\n",
        "LORA_START = 1 #@param {type:\"number\"}\n",
        "LORA_MID   = 1 #@param {type:\"number\"}\n",
        "LORA_END   = 1 #@param {type:\"number\"}\n",
        "SPIKE_AT_STEP      = 1 #@param {type:\"number\"}\n",
        "TAIL_START_AT_STEP = 5 #@param {type:\"number\"}\n",
        "\n",
        "TARGET_ASPECT = (WIDTH, HEIGHT)\n",
        "\n",
        "# Prompt\n",
        "CATVTON_PROMPT = (\n",
        "    \"The pair of images highlights a clothing and its styling on a model, high resolution, 4K, 8K; \"\n",
        "    \"[IMAGE1] Detailed product shot of a clothing\"\n",
        "    \"[IMAGE2] The same cloth is worn by STVBLDMN in a studio setting.\"\n",
        ")\n",
        "\n",
        "# HYPIR enhancement (optional overlay)\n",
        "ENABLE_HYPIR_ENHANCE   = False   #@param {type:\"boolean\"}\n",
        "HYPIR_OVERLAY_OPACITY  = 0.15    #@param {type:\"number\"}\n",
        "HYPIR_PROMPT           = \"macro, defined fabric texture, 4K, professional fashion photography\" #@param {type:\"string\"}\n",
        "HYPIR_UPSCALE          = 1\n",
        "HYPIR_WEIGHT_PATH      = \"/content/HYPIR_sd2.pth\"\n",
        "HYPIR_BASE_MODEL       = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "\n",
        "GENERATED_SUFFIX  = \"\" #@param {type:\"string\"}\n",
        "if ENABLE_HYPIR_ENHANCE:\n",
        "    GENERATED_SUFFIX += \"_enhanced\"\n",
        "\n",
        "# Spreadsheet (Gen Log + Operations)\n",
        "SPREADSHEET_ID = \"1Kbq9__sEUQiuDPuza5Xy_hRyIn8pUvmfFj6vhPBrp8Y\"\n",
        "GEN_LOG_SHEET  = \"Gen Log\"\n",
        "OPS_SHEET_NAME = \"Operations\"\n",
        "\n",
        "# Sheet-driven angle selection\n",
        "USE_SHEET_SELECTION = (RUN_MODE == \"sheet\")\n",
        "ANGLE_NEEDS_REGENERATE_TOKEN = \"Regenerate\"\n",
        "ENFORCE_BAN_SUBSTRINGS     = True\n",
        "BANNED_SUBSTRINGS_CSV      = \"wrong, pair, combo\"\n",
        "\n",
        "ENFORCE_REQUIRE_SUBSTRINGS = False\n",
        "REQUIRED_SUBSTRINGS_CSV    = \"\"\n",
        "REQUIRED_SUBSTRINGS_MODE   = \"ANY\"   # \"ANY\" | \"ALL\"\n",
        "\n",
        "# Misc\n",
        "SHOW_VISUALS = True   # fewer inline plots in a batch notebook\n",
        "TIMEZONE     = \"Europe/Lisbon\"\n",
        "OPERATOR     = \"Ivan\"\n",
        "\n",
        "\n",
        "# === Garment/type taxonomy (from you) ===\n",
        "ALLOWED_GARMENT_TYPES = [\n",
        "    \"hoodie\",\"jeans\",\"joggers\",\"shorts\",\"sweater\",\"swimwear\",\n",
        "    \"t-shirt\",\"shirts\",\"track top\",\"trousers\",\"twinset\",\"polo\",\"vests\",\"shirts\"\n",
        "]\n",
        "TOP_GARMENTS    = [\"t-shirt\",\"shirt\",\"sweater\",\"hoodie\",\"track top\",\"vest\"]\n",
        "BOTTOM_GARMENTS = [\"shorts\",\"jogger-trousers\",\"trousers\",\"jeans\",\"swimwear\"]\n",
        "TWINSET_TYPES   = [\"twinset\"]\n",
        "\n",
        "# === Details tokens from you (canonicalized) ===\n",
        "ALLOWED_DETAIL_TYPES = [\"crest\",\"logo\",\"patch\"]\n",
        "\n",
        "# === GPT switches/models/threshold ===\n",
        "GPT_EVAL_ENABLED   = False\n",
        "GPT_EVAL_MODEL     = \"gpt-4.1-mini\"      # vision-capable, cost-effective\n",
        "GPT_PASS_THRESHOLD = 7                   # stop retrying at ≥\n",
        "\n",
        "# === Your DETAIL prompt (kept essence; only token names harmonized) ===\n",
        "GPT_DETAIL_ANALYZE_PROMPT = f\"\"\"\n",
        "You are a vision AI for fashion. Look at the main garment in the attached image.\n",
        "Output STRICTLY the following JSON schema and nothing else:\n",
        "\n",
        "{{\n",
        "  \"details\": [\n",
        "    {{\"type\": \"logo\"|\"patch\"|\"crest\", \"color\": \"<color if appropriate>\"}},\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "RULES:\n",
        "- \"details\": list EVERY decorative detail (logo, patch, crest).\n",
        "- Allowed detail tokens are exactly: {ALLOWED_DETAIL_TYPES}\n",
        "- For logo/patch: include a COLOR string (e.g., \"white\", \"red\").\n",
        "- If there are NO details, return \"details\": [].\n",
        "Return your answer STRICTLY as a JSON object; no commentary.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# === LoRA retry candidates (first one mirrors your current global triplet) ===\n",
        "LORA_SCHEDULE_CANDIDATES = [\n",
        "    (LORA_START, LORA_MID, LORA_END),\n",
        "    (2.7, 0.9, 1.3),\n",
        "    (3.0, 0.5, 1.2),\n",
        "    (2.2, 1.0, 1.8),\n",
        "]\n",
        "\n",
        "# === Output routing (single all-output folder + detailer queue) ===\n",
        "OUTPUT_DIR          = \"/content/drive/MyDrive/Dazzl/SikSilk/SS_OUTPUT_FOLDER/19oct\" #@param {type:\"string\"}\n",
        "#DETAILER_QUEUE_DIR  = \"/content/drive/MyDrive/Dazzl/SikSilk/DETAILER_QUEUE_FOLDER/14oct\" #@param {type:\"string\"}\n",
        "print(\"✅ GPT config & folders ready.\")\n",
        "\n",
        "# ==== CONFIG: Few-shot eval messages (from your \"Eval\") ====\n",
        "\n",
        "\n",
        "EVAL_DEVELOPER_PROMPT = \"You are a professional stylist and fashion construction expert with 15 years at Vogue and multiple industry awards. Your task is to rate the structural faithfulness of an AI-generated fashion photo of a garment compared to the real flat-lay/ghost-mannequin image of that same garment. You are given a side-by-side composite (reference garment + AI generation). Your output MUST be an integer from 1–10, where: 1 = the garment generated is not the source garment (e.g. a dress instead of a hoodie); 2 = The provided garment is effectively not the same: major structural differences (missing/extra panels, different neckline/collar, wrong closure type) or obviously different pattern/structure.; 4 = The generated garment is clearly the intended item, but has noticeable structural differences (e.g., added chest pocket, missing coin pocket, different fly/closure construction, visibly different seam placements, added drawstrings where none exist, extra/missing seams/panels); 7 = A very good structural clone; the pattern and construction read correctly, with only small inconsistencies (e.g., slightly different sleeve length, minor hood volume difference, subtle seam placement drift, extra bar-tack impression, small hem shape change).; 8 = Near-perfect structure; a trained expert can find tiny off-details in construction or proportions, but overall pattern and elements match.; 9 = Essentially perfect structure even for a trained eye. at most a minute proportional quirk.; 10 = Impeccable structural cloning; every element (seam placements, paneling, pockets, closures, hems, cuffs, collar/neckline, waistband shape, hood size, sleeve/leg lengths, pleat counts, slit positions) is exactly right. Critically focus ONLY on structure and construction. Evaluate: Presence/absence and placement of elements (pockets, zippers, buttons/snaps, drawstrings, pleats, slits, waistbands, yokes, panels, darts, gussets). Seam placement and count, stitch-line paths, hems/cuffs, collar/neckline shape and depth, hood size/shape. Proportions and lengths (sleeve/leg/hem), vent/slit positions, panel geometry. Overall fit category on the body: baggy/oversize vs. regular vs. slim (coarse classification only). Artifacts like random fabric pieces, phantom seams, warped or duplicated elements. Explicitly exclude any judgment about color, shade, fabric texture/hand, prints or logos/graphics. Ignore materials and surface finishes entirely. Provide a brief, technical justification broken into segments (e.g., “Neckline & Collar”, “Seams & Panels”, “Pockets & Closures”, “Sleeves & Lengths”, “Hood & Fit”, etc.). Do not give recommendations. End with the numeric score in curly braces (e.g., {7}) and nothing after that. Be precise and never hallucinate.\"\n",
        "\n",
        "\n",
        "\n",
        "EVAL_FEWSHOT_MESSAGES = [\n",
        "    {\n",
        "      \"role\": \"developer\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"input_text\",\n",
        "          \"text\": EVAL_DEVELOPER_PROMPT\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_hoodie_neutral.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nHood & Opening: Reference hood has no drawstrings and a compact three-panel build; AI image adds drawstrings and shows a deeper hood. Added element and altered hood volume.\\nNeckline & Collar: Reference shows a standard hood-to-body seam without placket; AI is consistent here.\\nPockets & Closures: Single kangaroo pocket in both; pocket shape and entry placements align.\\nSeams & Panels: Shoulder and side seams read correctly; cuff/hem bands present.\\nSleeves & Lengths: Sleeve length equivalent; cuff proportion consistent.\\nFit Category: Reference regular/relaxed; AI reads regular/relaxed.\\nStructural Verdict: Added drawstrings and increased hood volume create meaningful structural deviation.\\n{4}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_jeans_front.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nWaistband & Fly: Reference shows classic waistband with button and zipper fly; AI reproduces waistband height and fly construction.\\nPockets: Five-pocket layout present in both (including coin pocket); pocket mouth shapes differ slightly, placements align.\\nSeams & Panels: Side seams, inseams, and front rise seam match in path and count; no phantom seams.\\nHem & Length: Full-length with straight hem; AI length and hem shape align.\\nFit Category: Reference slim; AI on-body reads slim.\\nStructural Verdict: Strong structural match with tiny pocket-mouth curvature drift.\\n{8}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_dress.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nNeckline & Collar: Reference is a high crew neckline; AI shows a V-neck—major structural change.\\nSeams & Panels: Reference bodice has princess seams; AI omits them, simplifying paneling.\\nSleeves & Lengths: Reference is sleeveless; AI adds capped sleeves—added elements.\\nSkirt & Slits: Reference A-line with no slit; AI suggests a front slit—extra element.\\nFit Category: Reference regular; AI reads regular.\\nStructural Verdict: Different neckline, added sleeves, altered paneling/slit—structurally a different garment.\\n{2}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_denim_jacket.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nChest Pockets: Reference has two flap chest pockets with pointed flaps; AI shows both flap pockets with matching flap geometry and placements.\\nBody Pockets: Reference includes vertical welt hand pockets; AI renders both welts in correct positions and angles.\\nYoke & Panels: Reference features classic front yoke seam; AI preserves the yoke line and height precisely; panel divisions match.\\nPlacket & Closures: Center-front button placket structure, spacing impression, and cuff plackets read correctly in AI; no extra placket or missing closure structure.\\nSeams & Toplines: Side seams, armhole seams, and sleeve paneling align with reference; no phantom seams or missing panels.\\nSleeves & Lengths: Sleeve length and cuff proportion match; body length and hem band shape are consistent with reference.\\nFit Category: Reference regular; AI on-body reads regular.\\nStructural Verdict: Perfect structural clone. All structural elements—including pocket count/type, yoke, placket, seams, and proportions—match exactly. No extra/missing elements or artifacts detected.\\n{10}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_pleated_skirt.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nWaistband: Reference shows a straight waistband; AI reproduces same waistband height and structure.\\nPleats: Reference uses dense micro-pleats; AI shows slightly wider spacing but consistent count across panels.\\nHem & Length: Straight hem in both; length reads equivalent.\\nClosures: Side-zip impression not visible in either—no conflict.\\nFit Category: Reference regular; AI reads regular.\\nStructural Verdict: Decent structural alignment with small pleat-spacing variance.\\n{7}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"input_text\", \"text\": \"Compare: https://example.com/flatlay_shirt.png\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"output_text\",\n",
        "          \"text\": \"Justification and Structural Analysis:\\nCollar & Stand: Reference has a point collar with stand; AI matches collar shape and stand height.\\nPlacket: Reference features a standard front placket; AI placket present and aligned.\\nChest Pocket: Reference has no chest pocket; AI adds a left chest patch pocket—added element.\\nYoke & Seams: Back yoke implied in reference; AI suggests same; side seams align.\\nSleeves & Cuffs: Long sleeves with single-button cuff in reference; AI keeps long sleeves with comparable cuff shape.\\nFit Category: Reference regular; AI reads slim-regular.\\nStructural Verdict: Added chest pocket changes structure despite other accurate elements.\\n{5}\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "]\n",
        "print(\"✅ Few-shot eval messages loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WTzeGw_nEv"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAeR9BUW-8Lp"
      },
      "outputs": [],
      "source": [
        "# --- Core utilities: normalization, angles, walking, masks (agnostic-first) ---\n",
        "\n",
        "import os, re, fnmatch, math, uuid, pytz, random, gc, tempfile, traceback\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter\n",
        "\n",
        "def normalize_sku_list(sku_csv: str) -> str:\n",
        "    skus = []\n",
        "    for raw in sku_csv.split(','):\n",
        "        sku = raw.strip().upper()\n",
        "        match = re.search(r'(\\d+)', sku)\n",
        "        if match:\n",
        "            sku_number = match.group(1)\n",
        "            skus.append(f\"SS-{sku_number}\")\n",
        "    # Return as CSV string\n",
        "    return \", \".join(skus)\n",
        "\n",
        "SKU_CSV = normalize_sku_list(SKU_CSV)\n",
        "\n",
        "# Parsers\n",
        "def _parse_csv_list(s):  return [x.strip().casefold() for x in (s or \"\").split(\",\") if x.strip()]\n",
        "BANNED_SUBSTRINGS       = _parse_csv_list(BANNED_SUBSTRINGS_CSV)\n",
        "REQUIRED_SUBSTRINGS     = _parse_csv_list(REQUIRED_SUBSTRINGS_CSV)\n",
        "SKIP_FILENAME_TOKENS    = set(_parse_csv_list(SKIP_FILENAME_TOKENS_CSV))\n",
        "SKIP_BASENAME_SUFFIXES  = tuple(_parse_csv_list(SKIP_BASENAME_SUFFIXES_CSV))\n",
        "\n",
        "# Normalizers\n",
        "def _norm_sku(s):\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).replace(\"\\u00A0\",\" \")\n",
        "    s = \" \".join(s.split())\n",
        "    return s.casefold()\n",
        "\n",
        "def _norm_angle(s):\n",
        "    s = (s or \"\").strip().lower()\n",
        "    return s.strip(\"_ \").replace(\"-\", \"_\")\n",
        "\n",
        "# Angle aliases\n",
        "ANGLE_ALIASES = {\n",
        "    \"fr_cl\": [\"fr\", \"fr_\"],\n",
        "    #\"lft\":   [\"fr_lft\", \"bc_lft\"],\n",
        "    #\"rght\":  [\"fr_rght\", \"bc_rght\"],\n",
        "}\n",
        "\n",
        "# --- Helpers to keep outputs strict, sources flexible ---\n",
        "def expand_as_list(angles):\n",
        "    exp = list(expand_allowed_angles(angles))\n",
        "    exp = [_norm_angle(a) for a in exp]\n",
        "    exp.sort(key=len, reverse=True)  # prefer 'fr_cl' over 'fr'\n",
        "    return exp\n",
        "\n",
        "def pick_target_angle(source_angle: str, allowed_outputs: set) -> str | None:\n",
        "    s = _norm_angle(source_angle)\n",
        "    for target in allowed_outputs:\n",
        "        fam = {_norm_angle(x) for x in expand_allowed_angles([target])}\n",
        "        if s in fam:\n",
        "            return _norm_angle(target)\n",
        "    return None\n",
        "\n",
        "\n",
        "def expand_allowed_angles(angles):\n",
        "    expanded = set()\n",
        "    for a in (angles or []):\n",
        "        a_norm = _norm_angle(a)\n",
        "        expanded.add(a_norm)\n",
        "        for alt in ANGLE_ALIASES.get(a_norm, []):\n",
        "            expanded.add(_norm_angle(alt))\n",
        "    return expanded\n",
        "\n",
        "# Ignore set\n",
        "IGNORE_DIRS = {d.lower() for d in IGNORE_DIRS}\n",
        "\n",
        "# Walkers\n",
        "def _is_sku_folder(path: str) -> bool:\n",
        "    if os.path.basename(os.path.normpath(path)).lower() in IGNORE_DIRS:\n",
        "        return False\n",
        "    try:\n",
        "        for f in os.listdir(path):\n",
        "            if os.path.isfile(os.path.join(path, f)) and f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)):\n",
        "                return True\n",
        "    except Exception:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "def iter_sku_folders(root: str):\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        dirnames[:] = [d for d in dirnames if d.lower() not in IGNORE_DIRS]\n",
        "        if any(f.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS)) for f in filenames):\n",
        "            yield dirpath\n",
        "\n",
        "def resolve_targets(idents_csv: str, garments_root: str):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      • Plain SKU names, relative paths (Category/Subcategory/SKU), or absolute dirs\n",
        "      • Glob patterns (e.g., 'Hoodies/*' or 'SKSLK_12*')\n",
        "      • Directories that are NOT SKU leaves → expand to all descendant SKU leaves\n",
        "    \"\"\"\n",
        "    idents = [s.strip() for s in idents_csv.replace(\"\\n\", \",\").split(\",\") if s.strip()]\n",
        "    if not idents: return [], []\n",
        "\n",
        "    all_sku_dirs = list(iter_sku_folders(garments_root))\n",
        "    rel_map = {p: os.path.relpath(p, garments_root) for p in all_sku_dirs}\n",
        "    base_map = {p: os.path.basename(p) for p in all_sku_dirs}\n",
        "\n",
        "    seen, out, unmatched = set(), [], []\n",
        "    def add_path(p):\n",
        "        ap = os.path.abspath(p)\n",
        "        if os.path.isdir(ap):\n",
        "            if _is_sku_folder(ap):\n",
        "                if ap not in seen:\n",
        "                    seen.add(ap); out.append(ap)\n",
        "            else:\n",
        "                # Expand directory to all descendant SKU leaves\n",
        "                for leaf in iter_sku_folders(ap):\n",
        "                    a = os.path.abspath(leaf)\n",
        "                    if a not in seen:\n",
        "                        seen.add(a); out.append(a)\n",
        "\n",
        "    for ident in idents:\n",
        "        before = len(out)\n",
        "        # Absolute directory or SKU path\n",
        "        if os.path.isabs(ident) and os.path.isdir(ident):\n",
        "            add_path(ident)\n",
        "\n",
        "        # Relative under garments root (dir or SKU)\n",
        "        rel_candidate = os.path.join(garments_root, ident)\n",
        "        if os.path.exists(rel_candidate):\n",
        "            add_path(rel_candidate)\n",
        "\n",
        "        # Glob/pattern over known SKU leaves (by basename or relative path)\n",
        "        for p in all_sku_dirs:\n",
        "            if fnmatch.fnmatch(base_map[p], ident) or fnmatch.fnmatch(rel_map[p], ident):\n",
        "                add_path(p)\n",
        "\n",
        "        if len(out) == before:\n",
        "            unmatched.append(ident)\n",
        "\n",
        "    out.sort()\n",
        "    return out, unmatched\n",
        "\n",
        "# Base/mask location resolution\n",
        "def resolve_base_mask_dir(sku_folder: str,\n",
        "                          garments_root: str = GARMENTS_ROOT,\n",
        "                          base_root: str = BASE_PHOTOS_ROOT):\n",
        "    \"\"\"\n",
        "    Map .../GARMENTS_ROOT/Category/Subcategory/SKU → .../BASE_ROOT/Category/Subcategory\n",
        "    With robust fallbacks.\n",
        "    \"\"\"\n",
        "    abs_sku = os.path.abspath(sku_folder)\n",
        "    abs_gar = os.path.abspath(garments_root)\n",
        "    try:\n",
        "        rel = os.path.relpath(abs_sku, abs_gar)\n",
        "    except Exception:\n",
        "        rel = None\n",
        "\n",
        "    if rel and not rel.startswith(\"..\"):\n",
        "        rel_parent = os.path.dirname(rel)\n",
        "        cand = os.path.join(base_root, rel_parent)\n",
        "        if os.path.isdir(cand): return cand\n",
        "\n",
        "    subcat = os.path.basename(os.path.dirname(abs_sku))\n",
        "    cat    = os.path.basename(os.path.dirname(os.path.dirname(abs_sku)))\n",
        "    cand2  = os.path.join(base_root, cat, subcat)\n",
        "    if os.path.isdir(cand2): return cand2\n",
        "\n",
        "    cand3  = os.path.join(base_root, subcat)\n",
        "    if os.path.isdir(cand3): return cand3\n",
        "    return None\n",
        "\n",
        "def _valid_ext(fname): return fname.lower().endswith(tuple(e.lower() for e in VALID_EXTENSIONS))\n",
        "\n",
        "def _file_prefix_or_none(filename: str):\n",
        "    low = filename.lower()\n",
        "    for base in ALLOWED_BASES:\n",
        "        if low.startswith(base): return base\n",
        "    return None\n",
        "\n",
        "def _find_image_with_stem_and_suffix(directory, stem, suffix=\"\"):\n",
        "    if not directory or not os.path.isdir(directory):\n",
        "        return None\n",
        "    stem = stem.lower()\n",
        "    for file in os.listdir(directory):\n",
        "        fname, fext = os.path.splitext(file)\n",
        "        if fext.lower() in (\".png\",\".jpg\",\".jpeg\") and fname.lower() == f\"{stem}{suffix}\":\n",
        "            return os.path.join(directory, file)\n",
        "    return None\n",
        "\n",
        "# --- Existence check in Google Drive by Colab-style path ---\n",
        "def drive_file_exists_any_ext_at_colab_path(target_colab_path: str,\n",
        "                                            exts=(\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")) -> bool:\n",
        "    \"\"\"\n",
        "    Given a Colab-style *file* path (incl. a filename with any extension),\n",
        "    checks if a file with the SAME stem exists in the same folder with any of the allowed extensions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(target_colab_path)\n",
        "        stem, _ = os.path.splitext(desired_name)\n",
        "        files = _list_children(parent_id, q_extra=\"\")  # list once; filter locally\n",
        "        allowed = {e.lower() for e in exts}\n",
        "        for f in files:\n",
        "            fname = f.get(\"name\", \"\")\n",
        "            s, e = os.path.splitext(fname)\n",
        "            if s == stem and e.lower() in allowed:\n",
        "                return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Ext-agnostic existence check failed for {target_colab_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# === NEW: mask finding with AGNOSTIC priority ===\n",
        "def find_mask_path(base_subcat_dir: str, stem_no_cut: str):\n",
        "    \"\"\"\n",
        "    Priority:\n",
        "      1) {stem}_mask_agnostic.(png|jpg|jpeg)\n",
        "      2) {stem}_mask.(png|jpg|jpeg)\n",
        "    \"\"\"\n",
        "    if not base_subcat_dir or not os.path.isdir(base_subcat_dir):\n",
        "        return None\n",
        "\n",
        "    candidates = []\n",
        "    if PREFER_AGNOSTIC_MASKS:\n",
        "      for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n",
        "          candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask_agnostic{ext}\"))\n",
        "    for ext in (\".png\",\".jpg\",\".jpeg\",\".PNG\",\".JPG\",\".JPEG\"):\n",
        "        candidates.append(os.path.join(base_subcat_dir, f\"{stem_no_cut}_mask{ext}\"))\n",
        "\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "# ──────────────────────────────────────────\n",
        "# --- Aspect-ratio bbox (replaces square bbox usage) ---\n",
        "\n",
        "\n",
        "def find_aspect_bbox(\n",
        "    mask: Image.Image,\n",
        "    aspect: tuple[int,int] = (4,5),   # width:height, e.g. (1280,1600)\n",
        "    padding: int = 40,\n",
        "    upper_padding: int | None = None,\n",
        "    horiz_padding: int = 0,\n",
        "    min_margin: int | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Return a rectangular bbox [x0, y0, x1, y1] that fully contains the mask + padding\n",
        "    and matches the requested aspect ratio by expanding outward only.\n",
        "    \"\"\"\n",
        "    if min_margin is None:\n",
        "        try:\n",
        "            min_margin = int(MASK_EXPAND_PX + 3 * MASK_FEATHER_PX + 5)\n",
        "        except Exception:\n",
        "            min_margin = 40\n",
        "\n",
        "    m = np.array(mask.convert(\"L\"))\n",
        "    h, w = m.shape\n",
        "    ys, xs = np.where(m > 128)\n",
        "    if xs.size == 0:\n",
        "        raise ValueError(\"Mask has no white pixels!\")\n",
        "\n",
        "    x_min, x_max = int(xs.min()), int(xs.max())\n",
        "    y_min, y_max = int(ys.min()), int(ys.max())\n",
        "\n",
        "    if upper_padding is None:\n",
        "        upper_padding = padding\n",
        "\n",
        "    # Initial padded bbox\n",
        "    x0 = max(0, x_min - horiz_padding - min_margin)\n",
        "    x1 = min(w, x_max + horiz_padding + min_margin)\n",
        "    y0 = max(0, y_min - upper_padding - min_margin)\n",
        "    y1 = min(h, y_max + padding + min_margin)\n",
        "\n",
        "    bw, bh = (x1 - x0), (y1 - y0)\n",
        "    # Desired aspect as float\n",
        "    aw, ah = aspect\n",
        "    target_ar = float(aw) / float(max(1, ah))\n",
        "\n",
        "    # First pass: try to match aspect by expanding one dimension only\n",
        "    def expand_to_aspect(x0, y0, x1, y1):\n",
        "        bw = x1 - x0; bh = y1 - y0\n",
        "        cur_ar = bw / float(max(1, bh))\n",
        "        if cur_ar < target_ar:\n",
        "            # too tall → need wider\n",
        "            need_w = int(np.ceil(target_ar * bh))\n",
        "            grow = max(0, need_w - bw)\n",
        "            left_grow  = min(x0, grow // 2)\n",
        "            right_grow = min(w - x1, grow - left_grow)\n",
        "            x0 -= left_grow; x1 += right_grow\n",
        "        elif cur_ar > target_ar:\n",
        "            # too wide → need taller\n",
        "            need_h = int(np.ceil(bw / target_ar))\n",
        "            grow = max(0, need_h - bh)\n",
        "            top_grow    = min(y0, grow // 2)\n",
        "            bottom_grow = min(h - y1, grow - top_grow)\n",
        "            y0 -= top_grow; y1 += bottom_grow\n",
        "        return max(0,x0), max(0,y0), min(w,x1), min(h,y1)\n",
        "\n",
        "    x0, y0, x1, y1 = expand_to_aspect(x0, y0, x1, y1)\n",
        "\n",
        "    # Second pass: if a border capped us, re-try by expanding the other dimension\n",
        "    bw, bh = (x1 - x0), (y1 - y0)\n",
        "    cur_ar = bw / float(max(1, bh))\n",
        "    if abs(cur_ar - target_ar) > 1e-3:\n",
        "        if cur_ar < target_ar:\n",
        "            # could not widen enough → try to grow height\n",
        "            need_h = int(np.ceil(bw / target_ar))\n",
        "            grow = max(0, need_h - bh)\n",
        "            top_grow    = min(y0, grow // 2)\n",
        "            bottom_grow = min(h - y1, grow - top_grow)\n",
        "            y0 -= top_grow; y1 += bottom_grow\n",
        "        else:\n",
        "            # could not heighten enough → try to grow width\n",
        "            need_w = int(np.ceil(target_ar * bh))\n",
        "            grow = max(0, need_w - bw)\n",
        "            left_grow  = min(x0, grow // 2)\n",
        "            right_grow = min(w - x1, grow - left_grow)\n",
        "            x0 -= left_grow; x1 += right_grow\n",
        "\n",
        "    # Final clamp\n",
        "    x0, y0 = max(0, int(x0)), max(0, int(y0))\n",
        "    x1, y1 = min(w, int(x1)), min(h, int(y1))\n",
        "    return [x0, y0, x1, y1]\n",
        "\n",
        "\n",
        "# Alpha/white utilities for garment panel\n",
        "WHITE_RGB = (255,255,255)\n",
        "def flatten_alpha_to_white(img: Image.Image) -> Image.Image:\n",
        "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n",
        "        bg = Image.new(\"RGB\", img.size, WHITE_RGB)\n",
        "        bg.paste(img, mask=img.split()[-1])\n",
        "        return bg\n",
        "    return img.convert(\"RGB\")\n",
        "\n",
        "def _tight_bbox_nonwhite_or_opaque(img: Image.Image):\n",
        "    if img.mode in (\"RGBA\",\"LA\") or (\"transparency\" in img.info):\n",
        "        arr = np.asarray(img.convert(\"RGBA\"))\n",
        "        alpha = arr[...,3]\n",
        "        fg = alpha > 0\n",
        "    else:\n",
        "        arr = np.asarray(img.convert(\"RGB\"))\n",
        "        fg = ~((arr[...,0]==255)&(arr[...,1]==255)&(arr[...,2]==255))\n",
        "    if not np.any(fg): return None\n",
        "    ys, xs = np.where(fg)\n",
        "    x0, x1 = int(xs.min()), int(xs.max())+1\n",
        "    y0, y1 = int(ys.min()), int(ys.max())+1\n",
        "    return (x0,y0,x1,y1)\n",
        "\n",
        "def crop_garment_keep_aspect(img: Image.Image) -> Image.Image:\n",
        "    bbox = _tight_bbox_nonwhite_or_opaque(img)\n",
        "    base = flatten_alpha_to_white(img)\n",
        "    if bbox is None: return base\n",
        "    full_bbox = (0,0,base.width,base.height)\n",
        "    if bbox == full_bbox: return base\n",
        "    return base.crop(bbox)\n",
        "\n",
        "def to_centered_square(gar: Image.Image, fill=WHITE_RGB) -> Image.Image:\n",
        "    w,h = gar.size; side = max(w,h)\n",
        "    sq = Image.new(\"RGB\", (side, side), fill)\n",
        "    ox, oy = (side-w)//2, (side-h)//2\n",
        "    sq.paste(gar, (ox,oy)); return sq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MqzqMK-h9Qo"
      },
      "outputs": [],
      "source": [
        "# ==== GPT scorer with few-shot pre-messages ====\n",
        "import io, json, base64, re\n",
        "from typing import Tuple, Dict, Any, Optional, List\n",
        "from PIL import Image\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "\n",
        "\n",
        "_client: Optional[OpenAI] = None\n",
        "def _client_once() -> OpenAI:\n",
        "    global _client\n",
        "    if _client is None:\n",
        "        _client = OpenAI(api_key=openai_api_key)\n",
        "    return _client\n",
        "\n",
        "def _pil_to_data_url(pil: Image.Image) -> str:\n",
        "    buf = io.BytesIO()\n",
        "    pil.convert(\"RGB\").save(buf, format=\"PNG\")\n",
        "    b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "    return f\"data:image/png;base64,{b64}\"\n",
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "_DETAILS_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"additionalProperties\": False,\n",
        "    \"required\": [\"details\"],\n",
        "    \"properties\": {\n",
        "        \"details\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"required\": [\"type\"],\n",
        "                \"properties\": {\n",
        "                    \"type\": {\"type\": \"string\",\n",
        "                             \"enum\": [\"crest\",\"logo\",\"patch\"]},\n",
        "                    \"color\": {\"type\": \"string\"}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def _normalize_detail_type(t: str) -> str:\n",
        "    t = (t or \"\").strip().lower()\n",
        "    mapping = {\n",
        "        \"waistband lettering\": \"waist text\",\n",
        "        \"sleeve lettering\": \"sleeve text\",\n",
        "        \"sleeve_text\": \"sleeve text\",\n",
        "        \"waist_text\": \"waist text\",\n",
        "    }\n",
        "    return mapping.get(t, t)\n",
        "\n",
        "def _postprocess_details(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    details = payload.get(\"details\", [])\n",
        "    fixed: List[Dict[str, Any]] = []\n",
        "    for d in details:\n",
        "        typ = _normalize_detail_type(d.get(\"type\"))\n",
        "        col = d.get(\"color\")\n",
        "        if typ == \"sleeve text\":\n",
        "            fixed.append({\"type\": \"sleeve text\"})\n",
        "        elif typ in [\"crest\",\"logo\",\"patch\"]:\n",
        "            ent = {\"type\": typ}\n",
        "            if col and isinstance(col, str) and col.strip():\n",
        "                ent[\"color\"] = col.strip()\n",
        "            fixed.append(ent)\n",
        "    return {\"details\": fixed}\n",
        "\n",
        "def concat_side_by_side(left: Image.Image, right: Image.Image, pad: int = 16, bg=(255,255,255)) -> Image.Image:\n",
        "    lh = max(left.height, right.height)\n",
        "    def _rs(im):\n",
        "        scale = lh / max(1, im.height)\n",
        "        return im.resize((max(1,int(im.width*scale)), lh), Image.LANCZOS)\n",
        "    L = _rs(left.convert(\"RGB\")); R = _rs(right.convert(\"RGB\"))\n",
        "    canvas = Image.new(\"RGB\", (L.width + pad + R.width, lh), bg)\n",
        "    canvas.paste(L, (0, 0))\n",
        "    canvas.paste(R, (L.width + pad, 0))\n",
        "    return canvas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gpt_score_tryon(garment_img: Image.Image, generated_img: Image.Image):\n",
        "    \"\"\"\n",
        "    Sends your developer prompt + few-shot exemplars, then asks the model\n",
        "    to write the review and put the numeric score in curly braces at the end.\n",
        "    Returns (score:int, response_id:str).\n",
        "    \"\"\"\n",
        "    pair = concat_side_by_side(garment_img, generated_img)\n",
        "    data_url = _pil_to_data_url(pair)\n",
        "    client = _client_once()\n",
        "\n",
        "    messages = []\n",
        "    # Your developer/system-style instruction for how to evaluate\n",
        "    messages.append({\n",
        "        \"role\": \"developer\",\n",
        "        \"content\": [{\"type\": \"input_text\", \"text\": EVAL_DEVELOPER_PROMPT.strip()}]\n",
        "    })\n",
        "    # Your long few-shot examples exactly as provided earlier\n",
        "    messages.extend(EVAL_FEWSHOT_MESSAGES)\n",
        "    # The actual comparison request for THIS pair\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"input_text\",\n",
        "             \"text\": (\n",
        "                 \"Compare:\"\n",
        "             )},\n",
        "            {\"type\": \"input_image\", \"image_url\": data_url}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    resp = client.responses.create(\n",
        "        model=GPT_EVAL_MODEL,\n",
        "        input=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    text = getattr(resp, \"output_text\", \"\") or \"\"\n",
        "    print(f\"GPT full eval: {text}\")\n",
        "    # Robustly extract the last {N} or 'Rating: {N}'\n",
        "    import re\n",
        "    score = None\n",
        "    # 1) last {...} containing 1-10\n",
        "    brace_hits = re.findall(r\"\\{[^{}]*?(10|[1-9])[^{}]*?\\}\", text)\n",
        "    if brace_hits:\n",
        "        score = int(brace_hits[-1])\n",
        "    else:\n",
        "        # 2) Rating: {N} or Rating: N (fallback)\n",
        "        m = re.search(r\"(?i)rating\\s*[:\\-]?\\s*\\{?\\s*(10|[1-9])\\s*\\}?\", text)\n",
        "        score = int(m.group(1)) if m else 1\n",
        "\n",
        "    return score, resp.id\n",
        "\n",
        "\n",
        "def gpt_detect_details(garment_img: Image.Image,\n",
        "                       generated_img: Image.Image,\n",
        "                       previous_response_id: str | None):\n",
        "    \"\"\"\n",
        "    Calls your GPT_DETAIL_ANALYZE_PROMPT verbatim. Expects pure JSON per your spec.\n",
        "    Returns the parsed dict (or {\"details\": []} on failure).\n",
        "    \"\"\"\n",
        "    pair = concat_side_by_side(garment_img, generated_img)\n",
        "    data_url = _pil_to_data_url(pair)\n",
        "    client = _client_once()\n",
        "\n",
        "    resp = client.responses.create(\n",
        "        model=GPT_EVAL_MODEL,\n",
        "        previous_response_id=previous_response_id,  # keep same thread\n",
        "        input=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"input_text\", \"text\": GPT_DETAIL_ANALYZE_PROMPT},\n",
        "                {\"type\": \"input_image\", \"image_url\": data_url}\n",
        "            ]\n",
        "        }],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    raw = getattr(resp, \"output_text\", \"\") or \"\"\n",
        "    import json, re\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except Exception:\n",
        "        # If the model wrapped JSON in prose or code fences, try to extract the first JSON object\n",
        "        m = re.search(r\"\\{[\\s\\S]*\\}\", raw)\n",
        "        if m:\n",
        "            try:\n",
        "                return json.loads(m.group(0))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return {\"details\": []}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbJ9VKUEzhaP"
      },
      "outputs": [],
      "source": [
        "# --- Visualisation helpers (restored) ---\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "def open_upright(path) -> Image.Image:\n",
        "    # EXIF-aware loader (same as before)\n",
        "    with Image.open(path) as im:\n",
        "        return ImageOps.exif_transpose(im)\n",
        "\n",
        "def show_gallery(img_list, titles=None, cols=3, w=4):\n",
        "    \"\"\"\n",
        "    Display PIL images in a flexible grid (identical behaviour to your original).\n",
        "    Only renders if SHOW_VISUALS is True.\n",
        "    \"\"\"\n",
        "    if not globals().get(\"SHOW_VISUALS\", False):\n",
        "        return\n",
        "\n",
        "    n = len(img_list)\n",
        "    rows = math.ceil(n / cols)\n",
        "    plt.figure(figsize=(cols * w, rows * w))\n",
        "\n",
        "    for i, img in enumerate(img_list):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        # Accept PIL, torch tensors or numpy arrays (4-D batch ⇒ pick first)\n",
        "        if isinstance(img, np.ndarray) and img.ndim == 4:\n",
        "            img = img[0]  # (B,H,W,C) → (H,W,C)\n",
        "        # Torch tensors are printed via duck-typing check to avoid hard import\n",
        "        if \"Tensor\" in str(type(img)):\n",
        "            img = img.detach().cpu().permute(1, 2, 0).numpy()\n",
        "        plt.imshow(img)\n",
        "        if titles and i < len(titles):\n",
        "            plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOhtSL7B-_-f"
      },
      "outputs": [],
      "source": [
        "# --- Paste-back (mask-aware) + pair/mask builders ---\n",
        "\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "_to_tensor = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
        "_to_tensor_mask = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# ──────────────────────────────────────────\n",
        "def paste_crop_back(\n",
        "    full_img: Image.Image,\n",
        "    edited_crop: Image.Image,\n",
        "    crop_box,               # (x0,y0,x1,y1) in full_img coords\n",
        "    crop_mask: np.ndarray,  # H×W uint8/bool, garment=white within crop_box\n",
        "    expand_px: int = 20,    # outward dilation before feather (mask-aware)\n",
        "    feather_px: int = 10,   # Gaussian σ for feathering (outside only)\n",
        "    *,\n",
        "    bin_thresh: int = 127,\n",
        "    edge_kill_px: int | None = None,  # will be capped by actual margin\n",
        "    retry_expand_px: int = 30,        # how much to enlarge bbox if needed\n",
        "):\n",
        "    x0, y0, x1, y1 = map(int, crop_box)\n",
        "    tgt_w, tgt_h   = (x1 - x0), (y1 - y0)\n",
        "\n",
        "    # Resize edit to crop size\n",
        "    edit_rs = edited_crop.resize((tgt_w, tgt_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "    # --- 1) Binary mask in crop coordinates\n",
        "    mask_np = crop_mask\n",
        "    if isinstance(mask_np, Image.Image):\n",
        "        mask_np = np.array(mask_np.convert(\"L\"))\n",
        "    if mask_np.ndim == 3:\n",
        "        mask_np = mask_np[..., 0]\n",
        "    mask_np = cv2.resize(mask_np, (tgt_w, tgt_h), interpolation=cv2.INTER_NEAREST)\n",
        "    mask_bin = (mask_np > bin_thresh).astype(np.uint8)\n",
        "\n",
        "    # --- 2) Build outside-only feather band\n",
        "    if expand_px > 0:\n",
        "        ksize = max(1, expand_px * 2 + 1)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n",
        "        dil = cv2.dilate(mask_bin, kernel, iterations=1)\n",
        "    else:\n",
        "        dil = mask_bin.copy()\n",
        "\n",
        "    outside = (dil - mask_bin).clip(0, 1).astype(np.float32) * 255.0\n",
        "    if feather_px > 0:\n",
        "        outside = cv2.GaussianBlur(outside, (0, 0), sigmaX=feather_px, sigmaY=feather_px)\n",
        "\n",
        "    # --- 3) Edge-kill: CAP by actual margin so we only taper near rectangle borders\n",
        "    # distance from crop border to full-image borders on each side\n",
        "    margin_left   = x0\n",
        "    margin_right  = full_img.width  - x1\n",
        "    margin_top    = y0\n",
        "    margin_bottom = full_img.height - y1\n",
        "    max_safe_edgekill = max(2, min(margin_left, margin_right, margin_top, margin_bottom))\n",
        "\n",
        "    if edge_kill_px is None:\n",
        "        edge_kill_px = int(expand_px + 3 * feather_px)\n",
        "    edge_kill_px = min(int(edge_kill_px), int(max_safe_edgekill))\n",
        "\n",
        "    # apply taper only within edge_kill band near the crop edges (no global attenuation)\n",
        "    yy, xx = np.mgrid[0:tgt_h, 0:tgt_w]\n",
        "    dist_edge = np.minimum.reduce([xx, tgt_w - 1 - xx, yy, tgt_h - 1 - yy]).astype(np.float32)\n",
        "\n",
        "    edge_factor = np.ones_like(dist_edge, np.float32)\n",
        "    band = dist_edge < float(edge_kill_px)\n",
        "    edge_factor[band] = dist_edge[band] / float(max(1.0, edge_kill_px))\n",
        "    outside *= edge_factor\n",
        "\n",
        "    # --- 4) Final alpha: solid interior + tapered outside (no inward feathering)\n",
        "    alpha = np.zeros((tgt_h, tgt_w), np.float32)\n",
        "    alpha[mask_bin > 0] = 255.0\n",
        "    alpha += outside\n",
        "    alpha = np.clip(alpha, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # --- 5) Leakage check:\n",
        "    # Only retry if alpha > 0 at a crop edge that is NOT coincident with the full image edge.\n",
        "    leaks_top    = alpha[0, :].max() > 0\n",
        "    leaks_bottom = alpha[-1, :].max() > 0\n",
        "    leaks_left   = alpha[:, 0].max() > 0\n",
        "    leaks_right  = alpha[:, -1].max() > 0\n",
        "\n",
        "    needs_retry = False\n",
        "    if leaks_top    and y0 > 0:                      needs_retry = True\n",
        "    if leaks_bottom and y1 < full_img.height:        needs_retry = True\n",
        "    if leaks_left   and x0 > 0:                      needs_retry = True\n",
        "    if leaks_right  and x1 < full_img.width:         needs_retry = True\n",
        "\n",
        "    if needs_retry:\n",
        "        print(\"⚠️  alpha touches crop border (inside canvas) → retrying with expanded bbox...\")\n",
        "        x0n = max(0, x0 - retry_expand_px)\n",
        "        y0n = max(0, y0 - retry_expand_px)\n",
        "        x1n = min(full_img.width,  x1 + retry_expand_px)\n",
        "        y1n = min(full_img.height, y1 + retry_expand_px)\n",
        "        new_box = [x0n, y0n, x1n, y1n]\n",
        "\n",
        "        # Build a full-canvas mask only to crop-align it to the new box:\n",
        "        full_mask = np.zeros((full_img.height, full_img.width), np.uint8)\n",
        "        full_mask[y0:y1, x0:x1] = (mask_bin * 255).astype(np.uint8)\n",
        "        region_mask_crop = full_mask[y0n:y1n, x0n:x1n]  # <<< keep mask aligned to new crop\n",
        "\n",
        "        # Recurse with slightly larger edge_kill to be conservative\n",
        "        return paste_crop_back(\n",
        "            full_img   = full_img,\n",
        "            edited_crop= edited_crop,\n",
        "            crop_box   = new_box,\n",
        "            crop_mask  = region_mask_crop,\n",
        "            expand_px  = expand_px,\n",
        "            feather_px = feather_px,\n",
        "            bin_thresh = bin_thresh,\n",
        "            edge_kill_px = edge_kill_px + 10,\n",
        "            retry_expand_px = retry_expand_px\n",
        "        )\n",
        "\n",
        "    # --- 6) Composite\n",
        "    mask_img = Image.fromarray(alpha, mode=\"L\")\n",
        "    region   = full_img.crop((x0, y0, x1, y1))\n",
        "    comp     = Image.composite(edit_rs, region, mask_img)\n",
        "    full_img.paste(comp, (x0, y0))\n",
        "    return full_img\n",
        "\n",
        "\n",
        "from diffusers.utils import load_image\n",
        "# --- Pair & mask builder with letterboxing for the garment panel ---\n",
        "\n",
        "from diffusers.utils import load_image\n",
        "from PIL import Image\n",
        "\n",
        "def _fit_to_canvas(im: Image.Image, size: tuple[int,int], *, fill=(255,255,255), resample=Image.LANCZOS) -> Image.Image:\n",
        "    W,H = size\n",
        "    if im.width == 0 or im.height == 0:\n",
        "        return Image.new(\"RGB\", (W,H), fill)\n",
        "    scale = min(W / im.width, H / im.height)\n",
        "    nw, nh = max(1, int(round(im.width * scale))), max(1, int(round(im.height * scale)))\n",
        "    rs = im.resize((nw, nh), resample)\n",
        "    canvas = Image.new(\"RGB\", (W, H), fill)\n",
        "    ox, oy = (W - nw) // 2, (H - nh) // 2\n",
        "    canvas.paste(rs, (ox, oy))\n",
        "    return canvas\n",
        "\n",
        "def make_pair_and_mask(steve_image_path, mask_path, garment_path, size=None):\n",
        "    # default to global WIDTH, HEIGHT\n",
        "    W = WIDTH if size is None else size[0]\n",
        "    H = HEIGHT if size is None else size[1]\n",
        "\n",
        "    # Right panel (model/base) & its mask are expected to be pre-sized already in main loop,\n",
        "    # but keep safe resizing here:\n",
        "    steve = load_image(steve_image_path).convert(\"RGB\").resize((W,H), Image.BICUBIC)\n",
        "    msk   = load_image(mask_path).convert(\"RGB\").resize((W,H), Image.NEAREST)\n",
        "\n",
        "    # Left panel (garment): crop to content, then LETTERBOX to W×H (no stretch)\n",
        "    gar_raw     = load_image(garment_path)\n",
        "    gar_cropped = crop_garment_keep_aspect(gar_raw)\n",
        "    gar_panel   = _fit_to_canvas(gar_cropped.convert(\"RGB\"), (W,H), fill=WHITE_RGB, resample=Image.LANCZOS)\n",
        "\n",
        "    steve_t = _to_tensor(steve)\n",
        "    gar_t   = _to_tensor(gar_panel)\n",
        "    msk_t   = _to_tensor_mask(msk)[:1]\n",
        "\n",
        "    inpaint_image = torch.cat([gar_t, steve_t], dim=2)  # (3,H,2W)\n",
        "    zeros = torch.zeros_like(msk_t)\n",
        "    extended_mask = torch.cat([zeros, msk_t], dim=2)    # (1,H,2W)\n",
        "    return inpaint_image, extended_mask, H, W\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br9dskEm_CBL"
      },
      "outputs": [],
      "source": [
        "# --- LoRA pipeline + schedule ---\n",
        "\n",
        "import torch\n",
        "from diffusers import FluxFillPipeline\n",
        "from diffusers.models import FluxTransformer2DModel\n",
        "from peft import PeftModel\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE  = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "def list_lora_adapters(model):\n",
        "    names=set()\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, LoraLayer):\n",
        "            names.update(m.lora_A.keys())\n",
        "    return sorted(names)\n",
        "\n",
        "def set_lora_global_scale(model, scale: float, adapter: str | None = None):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, LoraLayer):\n",
        "            adapters = [adapter] if adapter else list(m.lora_A.keys())\n",
        "            for a in adapters:\n",
        "                m.scaling[a] = (m.lora_alpha[a] / m.r[a]) * float(scale)\n",
        "\n",
        "def lora_weight_schedule_params(step_idx, total_steps, w_start, w_mid, w_end):\n",
        "    s_spike = max(0, min(int(SPIKE_AT_STEP), max(0, total_steps-1)))\n",
        "    s_tail  = max(s_spike+1, min(int(TAIL_START_AT_STEP), max(0, total_steps-1)))\n",
        "    if step_idx < s_spike: return float(w_start)\n",
        "    if step_idx < s_tail:\n",
        "        denom = max(1,(s_tail - s_spike))\n",
        "        f = (step_idx - s_spike)/denom\n",
        "        return float(w_start + (w_mid - w_start)*f)\n",
        "    denom_tail = max(1, (total_steps-1) - s_tail)\n",
        "    f = (step_idx - s_tail)/denom_tail\n",
        "    f = max(0.0, min(1.0, f))\n",
        "    return float(w_mid + (w_end - w_mid)*f)\n",
        "\n",
        "class LoRAStepHook:\n",
        "    def __init__(self, model, total_steps, w_start, w_mid, w_end):\n",
        "        self.model = model; self.total_steps = int(total_steps)\n",
        "        self.step_idx = -1; self.last_ts_val=None; self._handle=None\n",
        "        self.w_start, self.w_mid, self.w_end = float(w_start), float(w_mid), float(w_end)\n",
        "    def _extract_timestep_from(self, args, kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "        if \"timestep\" in kwargs: t = kwargs[\"timestep\"]\n",
        "        elif len(args)>=2: t = args[1]\n",
        "        else: return None\n",
        "        if torch.is_tensor(t):\n",
        "            t = t.flatten()[0].detach(); return float(t.item())\n",
        "        try: return float(t)\n",
        "        except: return None\n",
        "    def _maybe_update_scale(self, ts_val):\n",
        "        if ts_val is None: return\n",
        "        if (self.last_ts_val is None) or (ts_val != self.last_ts_val):\n",
        "            self.last_ts_val = ts_val\n",
        "            self.step_idx += 1\n",
        "            idx = min(self.step_idx, self.total_steps-1)\n",
        "            w = lora_weight_schedule_params(idx, self.total_steps, self.w_start, self.w_mid, self.w_end)\n",
        "            set_lora_global_scale(self.model, w)\n",
        "    def _pre_hook_kwargs(self, module, args, kwargs): self._maybe_update_scale(self._extract_timestep_from(args, kwargs))\n",
        "    def _pre_hook(self, module, args): self._maybe_update_scale(self._extract_timestep_from(args, None))\n",
        "    def attach(self):\n",
        "        try: self._handle = self.model.register_forward_pre_hook(self._pre_hook_kwargs, with_kwargs=True)\n",
        "        except TypeError: self._handle = self.model.register_forward_pre_hook(self._pre_hook)\n",
        "    def detach(self):\n",
        "        if self._handle is not None: self._handle.remove(); self._handle=None\n",
        "\n",
        "def _build_lora_pipe():\n",
        "    pipe = FluxFillPipeline.from_pretrained(FILL_MODEL_ID, torch_dtype=DTYPE, use_safetensors=True).to(DEVICE)\n",
        "    catvton_transformer = FluxTransformer2DModel.from_pretrained(CATVTON_XFM, torch_dtype=DTYPE, use_safetensors=True)\n",
        "    pipe.transformer = catvton_transformer\n",
        "    if LORA_PATH and os.path.isdir(LORA_PATH):\n",
        "        pipe.transformer = PeftModel.from_pretrained(pipe.transformer, LORA_PATH)\n",
        "        print(f\"Loaded LoRA from: {LORA_PATH}\")\n",
        "    else:\n",
        "        print(\"⚠️ LORA_PATH not found — running base CatVTON transformer.\")\n",
        "    pipe.transformer.to(DEVICE, dtype=DTYPE).eval()\n",
        "    try:\n",
        "        pipe.vae.enable_slicing(); pipe.vae.enable_tiling()\n",
        "    except Exception: pass\n",
        "    torch.cuda.empty_cache()\n",
        "    return pipe\n",
        "\n",
        "def run_with_lora_schedule(pipe, steps, schedule_triplet, **pipe_kwargs):\n",
        "    try: pipe.scheduler.set_timesteps(steps, device=pipe._execution_device)\n",
        "    except Exception: pass\n",
        "    w_start, w_mid, w_end = schedule_triplet\n",
        "    hook = LoRAStepHook(pipe.transformer, steps, w_start, w_mid, w_end); hook.attach()\n",
        "    try:\n",
        "        result = pipe(num_inference_steps=steps, **pipe_kwargs)\n",
        "        imgs = result.images\n",
        "    finally:\n",
        "        hook.detach(); set_lora_global_scale(pipe.transformer, 1.0)\n",
        "    return imgs\n",
        "\n",
        "def run_inference_lora(image_path, mask_path, garment_path, size=(WIDTH,HEIGHT), steps=STEPS, guidance_scale=GUIDANCE, seed=777, prompt=CATVTON_PROMPT, pipe=None):\n",
        "    if pipe is None: pipe = _build_lora_pipe()\n",
        "    inpaint_image, extended_mask, H, W = make_pair_and_mask(image_path, mask_path, garment_path, size=size)\n",
        "    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
        "    with torch.autocast(device_type=DEVICE, dtype=DTYPE, enabled=(DEVICE==\"cuda\")):\n",
        "        imgs = run_with_lora_schedule(\n",
        "            pipe, steps=steps,\n",
        "            height=H, width=W*2,\n",
        "            image=inpaint_image, mask_image=extended_mask,\n",
        "            generator=generator, max_sequence_length=512,\n",
        "            guidance_scale=guidance_scale, prompt=prompt\n",
        "        )\n",
        "    out = imgs[0]\n",
        "    garment_result = out.crop((0,0,W,H))\n",
        "    tryon_result   = out.crop((W,0,W*2,H))\n",
        "    return garment_result, tryon_result\n",
        "\n",
        "def generate_tryon(base_crop: Image.Image, mask_crop: Image.Image, garment_path: str, seed: int, pipe=None):\n",
        "    model_size = (WIDTH, HEIGHT)\n",
        "    base_for_model = base_crop.resize(model_size, Image.LANCZOS)\n",
        "    mask_for_model = mask_crop.resize(model_size, Image.NEAREST)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f_img, \\\n",
        "         tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f_msk:\n",
        "        tmp_img, tmp_msk = f_img.name, f_msk.name\n",
        "        base_for_model.save(tmp_img); mask_for_model.save(tmp_msk)\n",
        "    _, tryon = run_inference_lora(tmp_img, tmp_msk, garment_path, size=model_size, steps=STEPS, guidance_scale=GUIDANCE, seed=seed, prompt=CATVTON_PROMPT, pipe=pipe)\n",
        "    try: os.remove(tmp_img); os.remove(tmp_msk)\n",
        "    except: pass\n",
        "    return tryon.resize(base_crop.size, Image.LANCZOS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfNf3NHxjMaj"
      },
      "outputs": [],
      "source": [
        "# Colab cell — output routing + metadata helpers\n",
        "import os, json\n",
        "from PIL import PngImagePlugin\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True); return p\n",
        "\n",
        "ensure_dir(OUTPUT_DIR)\n",
        "\n",
        "def build_output_filename(sku_name: str, angle_code: str, ext=\".png\") -> str:\n",
        "    # Examples: SS-12345-fr_rght or SS-12345-bc_lft\n",
        "    angle_clean = _norm_angle(angle_code)\n",
        "    return f\"{sku_name}-{angle_clean}{ext}\"\n",
        "\n",
        "\n",
        "import json, piexif\n",
        "from PIL import Image\n",
        "\n",
        "def save_png_with_metadata(img, out_path, details_payload=None, quality=95):\n",
        "    if details_payload:\n",
        "        # Encode JSON as UTF-8 with an ASCII prefix per EXIF spec for UserComment\n",
        "        payload = json.dumps(details_payload, ensure_ascii=False).encode(\"utf-8\")\n",
        "        user_comment = b\"ASCII\\x00\\x00\\x00\" + payload  # indicates undefined/UTF-8\n",
        "        exif_dict = {\"0th\": {}, \"Exif\": {piexif.ExifIFD.UserComment: user_comment}, \"1st\": {}, \"GPS\": {}, \"Interop\": {}}\n",
        "        exif_bytes = piexif.dump(exif_dict)\n",
        "        img.save(out_path, format=\"PNG\", exif=exif_bytes)\n",
        "    else:\n",
        "        img.save(out_path, format=\"PNG\")\n",
        "\n",
        "import json, piexif\n",
        "from PIL import Image, ImageOps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oGZnx_6_EP7"
      },
      "outputs": [],
      "source": [
        "# --- HYPIR (optional overlay) ---\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/HYPIR\")\n",
        "try:\n",
        "    from HYPIR.enhancer.sd2 import SD2Enhancer\n",
        "except Exception:\n",
        "    SD2Enhancer = None\n",
        "\n",
        "_HYPIR = {\"model\": None}\n",
        "def _init_hypir_if_needed():\n",
        "    if not ENABLE_HYPIR_ENHANCE: return None\n",
        "    if SD2Enhancer is None: return None\n",
        "    if _HYPIR[\"model\"] is not None: return _HYPIR[\"model\"]\n",
        "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    m = SD2Enhancer(\n",
        "        base_model_path=HYPIR_BASE_MODEL,\n",
        "        weight_path=HYPIR_WEIGHT_PATH,\n",
        "        lora_modules=[\n",
        "            \"to_k\",\"to_q\",\"to_v\",\"to_out.0\",\n",
        "            \"conv\",\"conv1\",\"conv2\",\"conv_shortcut\",\"conv_out\",\n",
        "            \"proj_in\",\"proj_out\",\"ff.net.2\",\"ff.net.0.proj\"\n",
        "        ],\n",
        "        lora_rank=256, model_t=200, coeff_t=200, device=dev\n",
        "    )\n",
        "    m.init_models()\n",
        "    _HYPIR[\"model\"] = m\n",
        "    return m\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from accelerate.utils import set_seed\n",
        "_to_tensor_vis = T.ToTensor()\n",
        "\n",
        "def hypir_enhance_pil(img_pil, prompt=None, upscale=None, seed=-1):\n",
        "    model = _init_hypir_if_needed()\n",
        "    if model is None: return img_pil\n",
        "    if seed == -1: seed = random.randint(0, 2**32-1)\n",
        "    set_seed(seed)\n",
        "    prompt  = HYPIR_PROMPT if prompt  is None else prompt\n",
        "    upscale = HYPIR_UPSCALE if upscale is None else upscale\n",
        "    tens = _to_tensor_vis(img_pil.convert(\"RGB\")).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        out_list = model.enhance(lq=tens, prompt=prompt, upscale=upscale, return_type=\"pil\")\n",
        "    return out_list[0].convert(\"RGB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBN_kgOo_GRI"
      },
      "outputs": [],
      "source": [
        "# --- Google APIs: gspread + Drive upload + Operations sync ---\n",
        "\n",
        "import google.auth\n",
        "SCOPES = [\"https://www.googleapis.com/auth/drive\", \"https://www.googleapis.com/auth/spreadsheets\"]\n",
        "creds, _ = google.auth.default(scopes=SCOPES)\n",
        "\n",
        "import gspread\n",
        "gs = gspread.authorize(creds)\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "drive_svc = build(\"drive\", \"v3\", credentials=creds)\n",
        "\n",
        "FOLDER_MIME   = \"application/vnd.google-apps.folder\"\n",
        "SHORTCUT_MIME = \"application/vnd.google-apps.shortcut\"\n",
        "PATH_PREFIX   = \"/content/drive/MyDrive/\"\n",
        "\n",
        "def _escape_name(name: str) -> str: return name.replace(\"'\", r\"\\'\")\n",
        "\n",
        "def _maybe_follow_shortcut(file_obj):\n",
        "    if file_obj.get(\"mimeType\") == SHORTCUT_MIME:\n",
        "        sd = file_obj.get(\"shortcutDetails\", {}) or {}\n",
        "        return sd.get(\"targetId\"), sd.get(\"targetMimeType\")\n",
        "    return file_obj.get(\"id\"), file_obj.get(\"mimeType\")\n",
        "\n",
        "def _list_children(parent_id: str, q_extra: str, page_size: int = 1000):\n",
        "    q = f\"'{parent_id}' in parents and trashed = false\"\n",
        "    if q_extra: q += f\" and ({q_extra})\"\n",
        "    resp = drive_svc.files().list(\n",
        "        q=q, spaces=\"drive\", pageSize=page_size,\n",
        "        fields=\"files(id,name,mimeType,shortcutDetails)\",\n",
        "        includeItemsFromAllDrives=True, supportsAllDrives=True,\n",
        "    ).execute()\n",
        "    return resp.get(\"files\", [])\n",
        "\n",
        "def _find_folder_id(parent_id: str, name: str):\n",
        "    files = _list_children(parent_id, q_extra=f\"name = '{_escape_name(name)}' and (mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\")\n",
        "    for f in files:\n",
        "        if f[\"mimeType\"] == FOLDER_MIME:\n",
        "            return f[\"id\"]\n",
        "    for f in files:\n",
        "        if f[\"mimeType\"] == SHORTCUT_MIME:\n",
        "            tid, tmime = _maybe_follow_shortcut(f)\n",
        "            if tmime == FOLDER_MIME: return tid\n",
        "    files = _list_children(parent_id, q_extra=f\"(mimeType = '{FOLDER_MIME}' or mimeType = '{SHORTCUT_MIME}')\")\n",
        "    needle = name.strip().casefold()\n",
        "    for f in files:\n",
        "        if (f.get(\"name\",\"\").strip().casefold()) == needle:\n",
        "            tid, tmime = _maybe_follow_shortcut(f)\n",
        "            if tmime == FOLDER_MIME: return tid\n",
        "    return None\n",
        "\n",
        "def _resolve_parent_id_and_filename_from_colab_path(colab_path: str):\n",
        "    if not colab_path.startswith(PATH_PREFIX):\n",
        "        raise ValueError(f\"This helper supports only '{PATH_PREFIX}...'. Got: {colab_path}\")\n",
        "    parts = colab_path[len(PATH_PREFIX):].strip(\"/\").split(\"/\")\n",
        "    if not parts: raise ValueError(\"Path must include a file name.\")\n",
        "    parent_id = \"root\"\n",
        "    for part in parts[:-1]:\n",
        "        next_id = _find_folder_id(parent_id, part)\n",
        "        if not next_id: raise FileNotFoundError(f\"Folder not found in path: '{part}'\")\n",
        "        parent_id = next_id\n",
        "    desired_name = parts[-1]\n",
        "    return parent_id, desired_name\n",
        "\n",
        "def upload_to_drive_folder(local_path: str, parent_folder_id: str, desired_name: str | None = None):\n",
        "    media = MediaFileUpload(local_path, resumable=True)\n",
        "    body = {\"name\": desired_name or os.path.basename(local_path), \"parents\": [parent_folder_id]}\n",
        "    file = drive_svc.files().create(\n",
        "        body=body, media_body=media,\n",
        "        fields=\"id, webViewLink, name, parents\",\n",
        "        supportsAllDrives=True,\n",
        "    ).execute()\n",
        "    # anyone with link reader\n",
        "    drive_svc.permissions().create(fileId=file[\"id\"], body={\"type\":\"anyone\",\"role\":\"reader\"},\n",
        "                                   fields=\"id\", supportsAllDrives=True).execute()\n",
        "    return file\n",
        "\n",
        "def update_operations_status(spreadsheet_id: str, sku_name: str, angle_code: str,\n",
        "                             ops_sheet_name: str = OPS_SHEET_NAME,\n",
        "                             status_value: str = \"Girls need to check\",\n",
        "                             update_all: bool = False):\n",
        "    sh = gs.open_by_key(spreadsheet_id)\n",
        "    ws_ops = sh.worksheet(ops_sheet_name)\n",
        "    col_c = ws_ops.col_values(3)\n",
        "    target_sku = _norm_sku(sku_name)\n",
        "    target_rows = [idx for idx, val in enumerate(col_c, start=1) if idx>1 and _norm_sku(val)==target_sku]\n",
        "    updated_rows = []\n",
        "    for r in target_rows:\n",
        "        angle_here = (ws_ops.cell(r,5).value or \"\").strip()\n",
        "        if angle_here == (angle_code or \"\").strip():\n",
        "            ws_ops.update_cell(r,9,status_value)\n",
        "            updated_rows.append(r)\n",
        "            if not update_all: break\n",
        "    return {\"updated\": len(updated_rows), \"rows\": updated_rows}\n",
        "\n",
        "def upload_file_and_append_to_sheet(local_path: str, target_colab_path: str,\n",
        "                                    sku_name: str, angle: str,\n",
        "                                    spreadsheet_id: str, worksheet_name: str):\n",
        "    parent_id, desired_name = _resolve_parent_id_and_filename_from_colab_path(target_colab_path)\n",
        "    uploaded = upload_to_drive_folder(local_path, parent_id, desired_name)\n",
        "    file_id  = uploaded[\"id\"]\n",
        "    file_url = uploaded.get(\"webViewLink\") or f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "    folder_url = f\"https://drive.google.com/drive/folders/{parent_id}\"\n",
        "\n",
        "    ts  = datetime.now(pytz.timezone(TIMEZONE)).strftime(\"%m-%d %H:%M:%S\")\n",
        "    uid = str(uuid.uuid4())\n",
        "\n",
        "    sh = gs.open_by_key(spreadsheet_id); ws = sh.worksheet(worksheet_name)\n",
        "    sku_cell = f'=HYPERLINK(\"{folder_url}\"; \"{sku_name}\")'\n",
        "    ws.append_row([sku_cell, angle, ts, file_url, uid, \"Girls need to check\", OPERATOR], value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "    try: update_operations_status(spreadsheet_id, sku_name, angle, OPS_SHEET_NAME, \"Girls need to check\", update_all=False)\n",
        "    except Exception as e: print(f\"⚠️ Ops update failed: {e}\")\n",
        "\n",
        "    return {\"file_url\": file_url}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFMF2dYwIlyZ"
      },
      "source": [
        "#build pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZxAbw0MGeIp",
        "outputId": "6fbda67e-e626-4a37-cd07-b4f638db8035"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3fdbe944dcf49d6b2f139da274d5e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model_index.json:   0%|          | 0.00/536 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43dfa4043e9a4759831a24bdf9cac2dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd58a315ad74c688706a9309e161d90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "scheduler_config.json:   0%|          | 0.00/273 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "997fb3b9cc8641e79b97163f8b7dabb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "189b59a45ead4c09982f0acede34a34f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "text_encoder_2/model-00002-of-00002.safe(…):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a836de18dec49c18dbf04573040d096",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22c7c3e26f0941f8a9a1e2a58a7968df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f668b34b2374165a9152319e309348c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "text_encoder_2/model-00001-of-00002.safe(…):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6bba97490634e5fa582c2bed61d0bba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "text_encoder/model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f512f589c2442c6b98b015c89507a2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87fb6c726b83485e9062ecd492b72125",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "586047203e534e94bac5a120caf9f463",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0afb1ac4ef30431aa1a4da77e3f1f1f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e07d82b3b50244619a5065834e34ce6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df97f4a874f4e01bef018a41c516853",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_2/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b4ca6461af54e7b82239cee2341329a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbcfd50b6df2441abb6f82297984f029",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b55038579d314c2887b025a878a19d35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/378 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7726ac7c69d1428c933e8e3fea0f9c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "transformer/diffusion_pytorch_model-0000(…):   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e95154ba4fe6458dbe3a4bbe97de4e38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "transformer/diffusion_pytorch_model-0000(…):   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5a349d4a4284817826235e23ee0196c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "transformer/diffusion_pytorch_model-0000(…):   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e17ec87b47c4f36a2ccd914d4363afa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/121k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e2ee96eb98242d5aa73d65e5f838b30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fad58dc96a6844d18b52a74939db9990",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b693678edf4580aad67dbf9f95c6cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4246cc1ac9c94aaa9a5a18b0debac74d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb4046bb0a434eb887c89da7a48d0eed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59aca372b3d447b692209d94c23c5739",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/442 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32dd052acbb7427380545d9cff49b546",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)ion_pytorch_model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0be446ff24c94fa2a97e18003eb06c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42f5b7adaada44f285bdcc7177bf61a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "diffusion_pytorch_model-00002-of-00003.s(…):   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6afaf1a5a89c4457a72052e74cdc99b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "diffusion_pytorch_model-00001-of-00003.s(…):   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb9f95cbeafd4d2f8a9c28e2973d4e2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "diffusion_pytorch_model-00003-of-00003.s(…):   0%|          | 0.00/3.83G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a7bf5e5364b4bde88091cf39ef040bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded LoRA from: /content/drive/MyDrive/Dazzl/SikSilk/their_dataset_LORA/4x5_1280_their_ds_LORA_13_w_jitter/4x5_1280_their_ds_LORA_13_w_jitter_best\n"
          ]
        }
      ],
      "source": [
        "local_pipe = _build_lora_pipe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU5VwKPQIody"
      },
      "source": [
        "# BATCH HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQi5CzF2_O92"
      },
      "outputs": [],
      "source": [
        "# --- Batch processor  ---\n",
        "\n",
        "# Colab cell — replace the whole function to insert GPT eval + retry + routing\n",
        "\n",
        "\n",
        "def process_one_garment_folder(folder_path: str, pipe=None, allowed_angles=None):\n",
        "    # STRICT targets (what to PRODUCE), e.g. {'fr_cl'}\n",
        "    allowed_outputs = {_norm_angle(a) for a in (allowed_angles or [])}\n",
        "    # Flexible sources (what garment filenames may START WITH), e.g. ['fr_cl','fr','fr_']\n",
        "    allowed_sources = expand_as_list(allowed_angles) if allowed_angles else None\n",
        "\n",
        "    base_subcat_dir = resolve_base_mask_dir(folder_path)\n",
        "    if not base_subcat_dir:\n",
        "        print(f\"⚠️ Cannot resolve base/mask dir for SKU: {folder_path}\")\n",
        "    files_sorted = sorted(os.listdir(folder_path))\n",
        "\n",
        "    worklist = []\n",
        "    for file in files_sorted:\n",
        "        low = file.lower()\n",
        "\n",
        "        # 1) Garment filename gates\n",
        "        if allowed_sources and not any(low.startswith(src) for src in allowed_sources):  # aliasing allowed ONLY here\n",
        "            continue\n",
        "        if SKIP_FILENAME_TOKENS and any(tok in low for tok in SKIP_FILENAME_TOKENS):\n",
        "            continue\n",
        "        if REQUIRE_CUT_IN_FILENAME and (\"cut\" not in low):\n",
        "            continue\n",
        "        if not _valid_ext(file):\n",
        "            continue\n",
        "\n",
        "        # 2) Determine matched SOURCE and map → STRICT TARGET\n",
        "        matched_source = next((src for src in allowed_sources if low.startswith(src)), None) if allowed_sources else None\n",
        "        if not matched_source:\n",
        "            continue\n",
        "        target_angle = pick_target_angle(matched_source, allowed_outputs) if allowed_outputs else _norm_angle(matched_source)\n",
        "        if allowed_outputs and not target_angle:\n",
        "            continue  # garment belongs to no requested target\n",
        "\n",
        "        # 3) STRICT base/mask resolution: **NO aliasing, do not try stem_nocut**\n",
        "        base_img_path = _find_image_with_stem_and_suffix(base_subcat_dir, target_angle)\n",
        "        if not base_img_path:\n",
        "            print(f\"⚠️ Missing BASE for target '{target_angle}' → skipping {file}\")\n",
        "            continue\n",
        "\n",
        "        mask_path = find_mask_path(base_subcat_dir, target_angle)\n",
        "        if not mask_path:\n",
        "            print(f\"⚠️ Missing MASK for target '{target_angle}' → skipping {file}\")\n",
        "            continue\n",
        "\n",
        "        # 4) Queue job; bind the OUTPUT ANGLE = target_angle (never the source)\n",
        "        worklist.append((file, target_angle, base_img_path, mask_path))\n",
        "\n",
        "    sku_name = os.path.basename(folder_path)\n",
        "    if allowed_sources:\n",
        "        print(f\"▶️  {sku_name}: {len(worklist)} image(s) to generate (outputs={sorted(list(allowed_outputs))}, sources={sorted(list(set(allowed_sources)))})\")\n",
        "    else:\n",
        "        print(f\"▶️  {sku_name}: {len(worklist)} image(s) to generate\")\n",
        "\n",
        "    if not worklist:\n",
        "        return\n",
        "\n",
        "    # === Run ===\n",
        "    for idx, (file, target_angle, base_img_path, mask_path) in enumerate(worklist, start=1):\n",
        "        print(f\"   {idx:>3}/{len(worklist):<3}  {file}  | USING STRICT base/mask='{target_angle}'\")\n",
        "        garment_path = os.path.join(folder_path, file)\n",
        "\n",
        "        # Skip if output already exists\n",
        "        sku_name   = os.path.basename(folder_path)\n",
        "        angle_code = _norm_angle(target_angle)\n",
        "        out_stem   = f\"{sku_name}_{angle_code}\"\n",
        "        dest_check = os.path.join(OUTPUT_DIR, out_stem + \".png\")\n",
        "\n",
        "        if drive_file_exists_any_ext_at_colab_path(dest_check):\n",
        "            print(f\"      ⏭️  Skip: {out_stem}.(png/jpg/jpeg) already exists in {OUTPUT_DIR}\")\n",
        "            continue\n",
        "        elif drive_file_exists_any_ext_at_colab_path(dest_check):\n",
        "            print(f\"      ⏭️  Skip: {out_stem}.(png/jpg/jpeg) already exists in {OUTPUT_DIR}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            garment_img = flatten_alpha_to_white(open_upright(garment_path))\n",
        "            base_full   = Image.open(base_img_path).convert(\"RGB\")\n",
        "            mask_full   = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "            show_gallery(\n",
        "                [garment_img, base_full, mask_full.convert(\"RGB\")],\n",
        "                [\"Source garment (white BG)\", f\"Base photo [{angle_code}]\", f\"Mask [{os.path.basename(mask_path)}]\"]\n",
        "            )\n",
        "\n",
        "            # Crop & prepare\n",
        "            bbox = find_aspect_bbox(mask_full, aspect=TARGET_ASPECT, padding=CROP_PADDING, upper_padding=UPPER_PADDING, horiz_padding=CROP_PADDING, min_margin=10)\n",
        "            base_crop = base_full.crop(tuple(bbox))\n",
        "            mask_crop = mask_full.crop(tuple(bbox))\n",
        "            show_gallery([base_crop, mask_crop.convert(\"RGB\"), garment_img], [\"Cropped base\", \"Cropped mask\", \"Garment (white BG)\"])\n",
        "\n",
        "            # === Try generation with schedule candidates until score >= threshold ===\n",
        "            best_final = None\n",
        "            best_score = -1\n",
        "            last_resp_id = None\n",
        "\n",
        "            for attempt, sched in enumerate(LORA_SCHEDULE_CANDIDATES, start=1):\n",
        "                seed = random.randint(1, 2**32 - 1)\n",
        "                # 1) Generate cropped try-on with chosen schedule\n",
        "                model_size = (WIDTH, HEIGHT)\n",
        "                base_for_model = base_crop.resize(model_size, Image.LANCZOS)\n",
        "                mask_for_model = mask_crop.resize(model_size, Image.NEAREST)\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f_img, \\\n",
        "                     tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f_msk:\n",
        "                    tmp_img, tmp_msk = f_img.name, f_msk.name\n",
        "                    base_for_model.save(tmp_img); mask_for_model.save(tmp_msk)\n",
        "\n",
        "                try:\n",
        "                    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
        "                    with torch.autocast(device_type=DEVICE, dtype=DTYPE, enabled=(DEVICE==\"cuda\")):\n",
        "                        imgs = run_with_lora_schedule(\n",
        "                            local_pipe, steps=STEPS, schedule_triplet=sched,\n",
        "                            height=model_size[1], width=model_size[0]*2,\n",
        "                            image=make_pair_and_mask(tmp_img, tmp_msk, garment_path, size=model_size)[0],\n",
        "                            mask_image=make_pair_and_mask(tmp_img, tmp_msk, garment_path, size=model_size)[1],\n",
        "                            generator=generator, max_sequence_length=512,\n",
        "                            guidance_scale=GUIDANCE, prompt=CATVTON_PROMPT\n",
        "                        )\n",
        "                    out = imgs[0]\n",
        "                    W,H = model_size\n",
        "                    tryon_sq = out.crop((W,0,W*2,H))  # right half is the try-on\n",
        "                finally:\n",
        "                    try: os.remove(tmp_img); os.remove(tmp_msk)\n",
        "                    except: pass\n",
        "\n",
        "                # Optional HYPIR blend\n",
        "                if ENABLE_HYPIR_ENHANCE and float(HYPIR_OVERLAY_OPACITY) > 0.0:\n",
        "                    hyp = hypir_enhance_pil(tryon_sq, prompt=HYPIR_PROMPT, upscale=HYPIR_UPSCALE)\n",
        "                    tryon_sq = Image.blend(tryon_sq.convert(\"RGB\"), hyp, float(HYPIR_OVERLAY_OPACITY))\n",
        "\n",
        "                # Paste back to full frame\n",
        "                final_img = paste_crop_back(\n",
        "                    full_img   = base_full.copy(),\n",
        "                    edited_crop= tryon_sq,\n",
        "                    crop_box   = bbox,\n",
        "                    crop_mask  = np.array(mask_crop),\n",
        "                    expand_px  = MASK_EXPAND_PX,\n",
        "                    feather_px = MASK_FEATHER_PX\n",
        "                )\n",
        "                show_gallery([final_img], [f\"Attempt {attempt} with schedule={sched}\"])\n",
        "\n",
        "                # 2) GPT scoring on [garment | final]\n",
        "                if GPT_EVAL_ENABLED:\n",
        "                    try:\n",
        "                        score, resp_id = gpt_score_tryon(garment_img, final_img)\n",
        "                        last_resp_id = resp_id\n",
        "                        print(f\"      🔎 GPT score = {score} (threshold {GPT_PASS_THRESHOLD})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"      ⚠️ GPT score failed: {e}\")\n",
        "                        score = 10  # fail-open to avoid endless loops\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_final = final_img.copy()\n",
        "\n",
        "                    if score >= GPT_PASS_THRESHOLD:\n",
        "                        final_img = best_final\n",
        "                        break\n",
        "                else:\n",
        "                    best_final = final_img\n",
        "                    best_score = 10\n",
        "                    break  # no eval, accept first\n",
        "\n",
        "            if best_final is None:\n",
        "                best_final = final_img\n",
        "\n",
        "            # 3) If passing, optionally ask for details on *same* conversation\n",
        "            details = {\"details\": []}\n",
        "            if GPT_EVAL_ENABLED and best_score >= GPT_PASS_THRESHOLD:\n",
        "                try:\n",
        "                    details = gpt_detect_details(garment_img, best_final, previous_response_id=last_resp_id)\n",
        "                except Exception as e:\n",
        "                    print(f\"      ⚠️ GPT details failed: {e}\")\n",
        "                    details = {\"details\": []}\n",
        "\n",
        "            # 4) Route & save with required filename pattern: {SKU}_{angle}.jpg\n",
        "            out_name = build_output_filename(sku_name, angle_code, ext=\".png\")\n",
        "            tmp_path = os.path.join(\"/tmp\", out_name)\n",
        "            has_details = bool(details.get(\"details\"))\n",
        "\n",
        "\n",
        "            # Save to the consolidated output folder\n",
        "            save_png_with_metadata(best_final, tmp_path, details_payload=details if has_details else None)\n",
        "            target_path_for_drive = os.path.join(OUTPUT_DIR, out_name)\n",
        "\n",
        "            # Drive upload + sheet append (kept from your original flow)\n",
        "            info = upload_file_and_append_to_sheet(\n",
        "                local_path       = tmp_path,\n",
        "                target_colab_path= target_path_for_drive,\n",
        "                sku_name         = sku_name,\n",
        "                angle            = angle_code,\n",
        "                spreadsheet_id   = SPREADSHEET_ID,\n",
        "                worksheet_name   = GEN_LOG_SHEET,\n",
        "            )\n",
        "            print(f\"      ✅ Uploaded → {info['file_url']}  (details: {has_details and details})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ❌ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzFKYPQO_RPb"
      },
      "outputs": [],
      "source": [
        "# --- Sheet-driven angle selection + runners ---\n",
        "\n",
        "# Sheet columns (0-based)\n",
        "COL_C_SKU, COL_E_ANGLE, COL_H_NOTE, COL_I_STATE, COL_J_FLAG = 2,4,7,8,9\n",
        "\n",
        "def angle_row_satisfies_conditions(row, sku_rows, *, regen_token, enforce_j_bans, banned_list, enforce_j_req, required_list, required_mode):\n",
        "    def _h_has_token(r, token): return (token or \"\").casefold() in ((r[COL_H_NOTE] if len(r)>COL_H_NOTE else \"\") or \"\").casefold()\n",
        "    def _i_empty(r):             return ((r[COL_I_STATE] if len(r)>COL_I_STATE else \"\") or \"\").strip() == \"\"\n",
        "    def _sku_banned(rows, banned):\n",
        "        if not banned: return False\n",
        "        for rr in rows:\n",
        "            j = ((rr[COL_J_FLAG] if len(rr)>COL_J_FLAG else \"\") or \"\").casefold()\n",
        "            if any(b in j for b in banned): return True\n",
        "        return False\n",
        "    def _sku_required(rows, reqs, mode):\n",
        "        if not reqs: return True\n",
        "        j_concat = \" \".join([((r[COL_J_FLAG] if len(r)>COL_J_FLAG else \"\") or \"\") for r in rows]).casefold()\n",
        "        return all(r in j_concat for r in reqs) if mode==\"ALL\" else any(r in j_concat for r in reqs)\n",
        "\n",
        "    if not _h_has_token(row, regen_token): return False\n",
        "    if not _i_empty(row): return False\n",
        "    if enforce_j_bans and _sku_banned(sku_rows, banned_list): return False\n",
        "    if enforce_j_req  and not _sku_required(sku_rows, required_list, required_mode): return False\n",
        "    angle = (row[COL_E_ANGLE] if len(row)>COL_E_ANGLE else \"\").strip()\n",
        "    return angle != \"\"\n",
        "\n",
        "def fetch_sku_angle_pairs_from_ops(spreadsheet_id: str, ops_sheet_name: str,\n",
        "                                   *, regen_token, enforce_j_bans, banned_list, enforce_j_requires, required_list, required_mode):\n",
        "    sh = gs.open_by_key(spreadsheet_id)\n",
        "    ws = sh.worksheet(ops_sheet_name)\n",
        "    all_vals = ws.get_all_values() or []\n",
        "    rows = all_vals[1:] if len(all_vals)>1 else []\n",
        "\n",
        "    by_sku = {}\n",
        "    for r in rows:\n",
        "        sku_raw = (r[COL_C_SKU] if len(r)>COL_C_SKU else \"\").strip()\n",
        "        if not sku_raw: continue\n",
        "        key = _norm_sku(sku_raw)\n",
        "        by_sku.setdefault(key, {\"sku_display\": sku_raw, \"rows\": []})[\"rows\"].append(r)\n",
        "\n",
        "    pairs = []\n",
        "    for key, bundle in by_sku.items():\n",
        "        sku_rows = bundle[\"rows\"]; display = bundle[\"sku_display\"]\n",
        "        for r in sku_rows:\n",
        "            angle = (r[COL_E_ANGLE] if len(r)>COL_E_ANGLE else \"\").strip()\n",
        "            if not angle: continue\n",
        "            if angle_row_satisfies_conditions(\n",
        "                r, sku_rows,\n",
        "                regen_token=regen_token,\n",
        "                enforce_j_bans=enforce_j_bans,\n",
        "                banned_list=banned_list,\n",
        "                enforce_j_req=enforce_j_requires,\n",
        "                required_list=required_list,\n",
        "                required_mode=(required_mode or \"ANY\").upper(),\n",
        "            ):\n",
        "                pairs.append({\"sku\": display, \"angle\": angle})\n",
        "\n",
        "    seen=set(); uniq=[]\n",
        "    for p in pairs:\n",
        "        k = (_norm_sku(p[\"sku\"]), _norm_angle(p[\"angle\"]))\n",
        "        if k not in seen:\n",
        "            seen.add(k); uniq.append(p)\n",
        "    print(f\"Found {len(uniq)} eligible (SKU, angle) pair(s).\")\n",
        "    return uniq\n",
        "\n",
        "def build_sku_folder_index(garments_root: str):\n",
        "    return { _norm_sku(os.path.basename(p)) : p for p in iter_sku_folders(garments_root) }\n",
        "\n",
        "# --- Entrypoints ---\n",
        "def run_sheet():\n",
        "    eligible = fetch_sku_angle_pairs_from_ops(\n",
        "        SPREADSHEET_ID, OPS_SHEET_NAME,\n",
        "        regen_token=ANGLE_NEEDS_REGENERATE_TOKEN,\n",
        "        enforce_j_bans=ENFORCE_BAN_SUBSTRINGS,\n",
        "        banned_list=BANNED_SUBSTRINGS,\n",
        "        enforce_j_requires=ENFORCE_REQUIRE_SUBSTRINGS,\n",
        "        required_list=REQUIRED_SUBSTRINGS,\n",
        "        required_mode=REQUIRED_SUBSTRINGS_MODE,\n",
        "    )\n",
        "    per_sku_angles = {}\n",
        "    display_name = {}\n",
        "    for item in eligible:\n",
        "        k = _norm_sku(item[\"sku\"])\n",
        "        per_sku_angles.setdefault(k, set()).add(_norm_angle(item[\"angle\"]))\n",
        "        display_name[k] = item[\"sku\"]\n",
        "\n",
        "    sku_index = build_sku_folder_index(GARMENTS_ROOT)\n",
        "    total = len(per_sku_angles)\n",
        "    print(f\"➡️  Will process {total} SKU(s) from sheet.\")\n",
        "    if total == 0: return\n",
        "    shared_pipe = local_pipe\n",
        "    for i, (sku_key, angles) in enumerate(per_sku_angles.items(), start=1):\n",
        "        folder = sku_index.get(sku_key)\n",
        "        disp   = display_name.get(sku_key, sku_key)\n",
        "        if not folder:\n",
        "            print(f\"⚠️  Missing folder for SKU '{disp}' under GARMENTS_ROOT.\")\n",
        "            continue\n",
        "        print(f\"\\nSKU {i}/{total} ▶️  {disp}  angles={sorted(list(angles))}\")\n",
        "        try:\n",
        "            process_one_garment_folder(folder, pipe=shared_pipe, allowed_angles=angles)\n",
        "            print(f\"✅ Finished: {disp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in {disp}: {e}\")\n",
        "    print(\"\\n🏁 Sheet run complete.\")\n",
        "\n",
        "def run_dir():\n",
        "    if not TARGET_DIR:\n",
        "        print(\"⚠️ TARGET_DIR is empty.\")\n",
        "        return\n",
        "    targets, unmatched = resolve_targets(TARGET_DIR, GARMENTS_ROOT)\n",
        "    if not targets:\n",
        "        print(f\"⚠️ No SKU leaves found under: {TARGET_DIR}\")\n",
        "        if unmatched: print(\"Unmatched:\", unmatched)\n",
        "        return\n",
        "    print(f\"➡️  Will process {len(targets)} SKU(s) from directory.\")\n",
        "    shared_pipe = local_pipe\n",
        "    for i, p in enumerate(targets, start=1):\n",
        "        name = os.path.basename(p)\n",
        "        print(f\"\\nSKU {i}/{len(targets)} ▶️  {name}\")\n",
        "        try:\n",
        "            process_one_garment_folder(p, pipe=shared_pipe)\n",
        "            print(f\"✅ Finished: {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in {name}: {e}\")\n",
        "    print(\"\\n🏁 Directory run complete.\")\n",
        "\n",
        "def run_list():\n",
        "    targets, unmatched = resolve_targets(SKU_CSV, GARMENTS_ROOT)\n",
        "    if not targets:\n",
        "        print(\"⚠️ No matching SKU folders found.\")\n",
        "        if unmatched: print(\"Unmatched:\", \", \".join(unmatched))\n",
        "        return\n",
        "    print(f\"➡️  Will process {len(targets)} SKU(s).\")\n",
        "    shared_pipe = local_pipe\n",
        "    for i, p in enumerate(targets, start=1):\n",
        "        name = os.path.basename(p)\n",
        "        print(f\"\\nSKU {i}/{len(targets)} ▶️  {name}\")\n",
        "        try:\n",
        "            process_one_garment_folder(p, pipe=shared_pipe, allowed_angles=ALLOWED_BASES)\n",
        "            print(f\"✅ Finished: {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in {name}: {e}\")\n",
        "    if unmatched:\n",
        "        print(\"\\nℹ️  Unmatched identifiers:\")\n",
        "        for u in unmatched: print(\"   -\", u)\n",
        "    print(\"\\n🏁 List run complete.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sinwLlWu_aCm"
      },
      "source": [
        "# DISPATCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 974,
          "referenced_widgets": [
            "42847e933f2342d5b001324d78fa41ba",
            "3ef487a23fa9411cbeb2c630dc358f80",
            "07103d96144e433b8a0dec5bd6484757",
            "11643dbc95be4d0f8ea22e6c4a3dcc8c",
            "fa063be14ccb427bb2040cddb43fd503",
            "2c40d233898c4dbda46119818a35436f",
            "ee7070ccbdaf4e3aa757c2a5ce84176e",
            "64da8b050b0a476ea28bf518acdc0c76",
            "7f270ea1b62e4342b74bd5aa6d837688",
            "0e5bbb730420408481535e6642f07103",
            "ea5c3cb9e1384e0a83368ec0a9722a08",
            "35738ab2bb0b4590887a664d4aa26f81",
            "6fe776ef2ac04ed091e68da7542f8a08",
            "667bafd29929415299a43d3b05badedd",
            "8dccf0878499432191fdda978ee08717",
            "c49a3468678a49fbbf9b9a6a5043e8ad",
            "9e4d49277f394596a2113d87da135768",
            "42ecabbfa2b04facb5666aa2fb1c33ef",
            "007af91ddf3b4405822cb75f7af6719a",
            "3237d2f74c1343f5bb24f7b47209f2db",
            "38d6a9c0ea094219b04fb159a53b4c7d",
            "ee822cdabe9b404e8b32d6904deabf45",
            "6ef65ee6043c4670b0d3170f2b861c4b",
            "c2b2951c47674792b0dd7b2bce037270",
            "9e489d6b38fb4f5bbba1a55ed1df0f76",
            "6591bcf9baa64e149e749a71a75aaff3",
            "fa543e5f37524774b9305493d21ef976"
          ]
        },
        "id": "FJAcEUnW_Y7f",
        "outputId": "3ccb008d-6106-45e0-80de-3c1fcc1b25c8"
      },
      "outputs": [],
      "source": [
        "# Dispatch\n",
        "if RUN_MODE == \"sheet\":\n",
        "    run_sheet()\n",
        "elif RUN_MODE == \"dir\":\n",
        "    run_dir()\n",
        "else:\n",
        "    run_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK_cKHr9Uutd"
      },
      "source": [
        "#UNASSIGN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXlc2wlNyNDm"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "O4KzsXGV_ee1",
        "umaizfLYv7ww",
        "D7pcfKd0_iB_",
        "w3WTzeGw_nEv",
        "HFMF2dYwIlyZ",
        "IU5VwKPQIody"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07103d96144e433b8a0dec5bd6484757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64da8b050b0a476ea28bf518acdc0c76",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f270ea1b62e4342b74bd5aa6d837688",
            "value": 5
          }
        },
        "0e5bbb730420408481535e6642f07103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11643dbc95be4d0f8ea22e6c4a3dcc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e5bbb730420408481535e6642f07103",
            "placeholder": "​",
            "style": "IPY_MODEL_ea5c3cb9e1384e0a83368ec0a9722a08",
            "value": " 5/75 [00:11&lt;02:47,  2.39s/it]"
          }
        },
        "2c40d233898c4dbda46119818a35436f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef487a23fa9411cbeb2c630dc358f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c40d233898c4dbda46119818a35436f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee7070ccbdaf4e3aa757c2a5ce84176e",
            "value": "  7%"
          }
        },
        "42847e933f2342d5b001324d78fa41ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ef487a23fa9411cbeb2c630dc358f80",
              "IPY_MODEL_07103d96144e433b8a0dec5bd6484757",
              "IPY_MODEL_11643dbc95be4d0f8ea22e6c4a3dcc8c"
            ],
            "layout": "IPY_MODEL_fa063be14ccb427bb2040cddb43fd503"
          }
        },
        "64da8b050b0a476ea28bf518acdc0c76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f270ea1b62e4342b74bd5aa6d837688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea5c3cb9e1384e0a83368ec0a9722a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee7070ccbdaf4e3aa757c2a5ce84176e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa063be14ccb427bb2040cddb43fd503": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
