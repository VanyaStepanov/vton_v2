{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "collapsed_sections": [
    "Qh3dRdojPCy1",
    "3WXWS5m9Ozyt",
    "SdvcT9PEO5ye",
    "0nznhr0CO8xT"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ce3dbfa3a114d988fc62b6c9ed98aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11517e487afa473abac383b9efaa5c7b",
       "IPY_MODEL_7028b59f4d5c4166b1769e140ef386fd",
       "IPY_MODEL_0a73f3bacea845379e0b6a927c9dea7e"
      ],
      "layout": "IPY_MODEL_1fcf083bbf954ff1bf37ada3cd3c30d5"
     }
    },
    "11517e487afa473abac383b9efaa5c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fd93a4cd2de46bd9d15a4755ae12102",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_a51521be386846eda04d2bdf520d52f7",
      "value": "Loading\u2007weights:\u2007100%"
     }
    },
    "7028b59f4d5c4166b1769e140ef386fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0888c84d331d4452a5cce6d6afda14e0",
      "max": 1468,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c5e63df2c3c4144a95b83877d2e622a",
      "value": 1468
     }
    },
    "0a73f3bacea845379e0b6a927c9dea7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b14e06815a64828b6c1bed44b72e7d4",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_f5962d2c47cd4fe286c17bd62f62aadb",
      "value": "\u20071468/1468\u2007[00:01&lt;00:00,\u20071358.79it/s,\u2007Materializing\u2007param=vision_encoder.neck.fpn_layers.3.proj2.weight]"
     }
    },
    "1fcf083bbf954ff1bf37ada3cd3c30d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fd93a4cd2de46bd9d15a4755ae12102": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a51521be386846eda04d2bdf520d52f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0888c84d331d4452a5cce6d6afda14e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c5e63df2c3c4144a95b83877d2e622a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b14e06815a64828b6c1bed44b72e7d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5962d2c47cd4fe286c17bd62f62aadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dccaf990377b4690aae829d03ed302de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_908d7b1661bc4a5aaf3108983819b5a9",
       "IPY_MODEL_71175ff97e9a43c3bac1cbc72e5060bd",
       "IPY_MODEL_48f01c245c4542a6868c340fb08382f7"
      ],
      "layout": "IPY_MODEL_7c585665999144f2b0611eff8248cdd8"
     }
    },
    "908d7b1661bc4a5aaf3108983819b5a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1df25f9b36f45698c6efb70f8310fb7",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_24bfa48a0cc74f8ca850c0d6217f1d1d",
      "value": "Loading\u2007pipeline\u2007components...:\u2007100%"
     }
    },
    "71175ff97e9a43c3bac1cbc72e5060bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_100f33513f2246ae89d0da72f0fb5108",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf58709f69434605a871dbd8eed256a9",
      "value": 7
     }
    },
    "48f01c245c4542a6868c340fb08382f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d55fff92fd05430cac0d0dca873efa3d",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_ae4b4886a23e4230b44446164474360a",
      "value": "\u20077/7\u2007[00:01&lt;00:00,\u2007\u20074.89it/s]"
     }
    },
    "7c585665999144f2b0611eff8248cdd8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1df25f9b36f45698c6efb70f8310fb7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24bfa48a0cc74f8ca850c0d6217f1d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "100f33513f2246ae89d0da72f0fb5108": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf58709f69434605a871dbd8eed256a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d55fff92fd05430cac0d0dca873efa3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae4b4886a23e4230b44446164474360a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9541658b7f32499eb83d1f34f1a99829": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81b2e62a17d04660a6bf0131d0184d78",
       "IPY_MODEL_535f81d037ed40859bfb0855f1a7fac1",
       "IPY_MODEL_16ac3d25bc8a4fb4aba749a92251bde6"
      ],
      "layout": "IPY_MODEL_f0b8247247aa4cf7974b9f502ba79c5d"
     }
    },
    "81b2e62a17d04660a6bf0131d0184d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2cdfa3fcae2497085e7e3ed23742de6",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2ab9416431a44ad9abc207f994d3b0b5",
      "value": "Loading\u2007weights:\u2007100%"
     }
    },
    "535f81d037ed40859bfb0855f1a7fac1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad7a42121b584cd0bd83f8dcf3360f69",
      "max": 196,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4cb80f7ca3c4669b8eb97018473de19",
      "value": 196
     }
    },
    "16ac3d25bc8a4fb4aba749a92251bde6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56ded5613f224feaaf90f20bb65c76a9",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_52564325ca684d0c9ed39bb4e75d5829",
      "value": "\u2007196/196\u2007[00:00&lt;00:00,\u20071068.63it/s,\u2007Materializing\u2007param=text_model.final_layer_norm.weight]"
     }
    },
    "f0b8247247aa4cf7974b9f502ba79c5d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2cdfa3fcae2497085e7e3ed23742de6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ab9416431a44ad9abc207f994d3b0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad7a42121b584cd0bd83f8dcf3360f69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4cb80f7ca3c4669b8eb97018473de19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56ded5613f224feaaf90f20bb65c76a9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52564325ca684d0c9ed39bb4e75d5829": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b09b1b42cdb491aa3c75dd8570ff564": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e4a887fd9e64c209346b02c675e44a1",
       "IPY_MODEL_c11c0f061ccc4a04a8c3bdd12a190eda",
       "IPY_MODEL_011ebf3383414757aeaaff9e37c0d3c0"
      ],
      "layout": "IPY_MODEL_e646a855f00c430ab191a01a223abe77"
     }
    },
    "5e4a887fd9e64c209346b02c675e44a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63db910c289f4bc09566d763ba2332ec",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_c3f3b06c97d94390ac064e6d91969cad",
      "value": "Loading\u2007weights:\u2007100%"
     }
    },
    "c11c0f061ccc4a04a8c3bdd12a190eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cd604395db940f792a8fe97d7e959a0",
      "max": 219,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b82a6335abb74cb5b4985a9da8ae2018",
      "value": 219
     }
    },
    "011ebf3383414757aeaaff9e37c0d3c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_944a41be57094812b61f7e84bcf3fbd7",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_a957690fe34f4d60b9b75df9e9205396",
      "value": "\u2007219/219\u2007[00:00&lt;00:00,\u2007464.99it/s,\u2007Materializing\u2007param=shared.weight]"
     }
    },
    "e646a855f00c430ab191a01a223abe77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63db910c289f4bc09566d763ba2332ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3f3b06c97d94390ac064e6d91969cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cd604395db940f792a8fe97d7e959a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b82a6335abb74cb5b4985a9da8ae2018": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "944a41be57094812b61f7e84bcf3fbd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a957690fe34f4d60b9b75df9e9205396": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4723b5d76a5f4406a454c777e040d321": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce9d5c9bb4434dd6b6108293e895e87c",
       "IPY_MODEL_106b9309c97943c393427cfb230e4bd1",
       "IPY_MODEL_19e597fba5b042a7bd4f98ef29c6a1c1"
      ],
      "layout": "IPY_MODEL_7cfbfb55d60240e28751df0f8f988f5a"
     }
    },
    "ce9d5c9bb4434dd6b6108293e895e87c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d269690bf0724663a56053d16447472b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_4e8984a1507e4049903e8068fe0ba6fb",
      "value": "Loading\u2007pipeline\u2007components...:\u2007100%"
     }
    },
    "106b9309c97943c393427cfb230e4bd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85197b3ff708441fb76621de24d9bdef",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a807732abb23439ebf7ee0f50eb430ee",
      "value": 3
     }
    },
    "19e597fba5b042a7bd4f98ef29c6a1c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71a39ba304254e748a816935b23dc331",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2f2eca9f0a8744c89d7eb58a79972823",
      "value": "\u20073/3\u2007[00:00&lt;00:00,\u2007\u20072.76it/s]"
     }
    },
    "7cfbfb55d60240e28751df0f8f988f5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d269690bf0724663a56053d16447472b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e8984a1507e4049903e8068fe0ba6fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85197b3ff708441fb76621de24d9bdef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a807732abb23439ebf7ee0f50eb430ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71a39ba304254e748a816935b23dc331": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f2eca9f0a8744c89d7eb58a79972823": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9526c597587746f4a26be50d4697f2e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68f54fb0fe1d4b1b8f8a7a803a5748cd",
       "IPY_MODEL_6e889fcd02dd4bababd5337a57cca486",
       "IPY_MODEL_f8c9c744b9eb450292cfaad5b3831c4e"
      ],
      "layout": "IPY_MODEL_5d240d5e53c849d6b022ba87c1714837"
     }
    },
    "68f54fb0fe1d4b1b8f8a7a803a5748cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab9b840ed6a0435f8b7db9e7e209f935",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_16ba9ff3771a427abdf15eb6ea40aec8",
      "value": "Loading\u2007weights:\u2007100%"
     }
    },
    "6e889fcd02dd4bababd5337a57cca486": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf4a4eb38715412980ab268c3d98c427",
      "max": 448,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df4fb92d216e42c488109fafb0d67c52",
      "value": 448
     }
    },
    "f8c9c744b9eb450292cfaad5b3831c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2ee3ddd56d04477b19e9282b7d9ad87",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_5bb4b3ef23a84e8f9c9f93af3e3a63a3",
      "value": "\u2007448/448\u2007[00:00&lt;00:00,\u2007934.75it/s,\u2007Materializing\u2007param=vision_model.post_layernorm.weight]"
     }
    },
    "5d240d5e53c849d6b022ba87c1714837": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab9b840ed6a0435f8b7db9e7e209f935": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16ba9ff3771a427abdf15eb6ea40aec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf4a4eb38715412980ab268c3d98c427": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df4fb92d216e42c488109fafb0d67c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2ee3ddd56d04477b19e9282b7d9ad87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bb4b3ef23a84e8f9c9f93af3e3a63a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#CONFIG"
   ],
   "metadata": {
    "id": "Qh3dRdojPCy1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UdtbzFhJCxn",
    "outputId": "5caad81e-4713-421f-a07f-df7e47b99d5b"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Y7pOn3vu5dQd"
   },
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "\n",
    "# Queued targets that REQUIRE the detailer (already pre-filtered by your generator)\n",
    "DETAILER_QUEUE_FOLDER = \"/content/drive/MyDrive/DETAILER_TODO/plisse\"  # @param {type:\"string\"}\n",
    "\n",
    "# Where to save (1) the source garment and (2) the inpainted results\n",
    "TARGET_DIR = \"/content/drive/MyDrive/DETAILER_DONE/plisse\"               # @param {type:\"string\"}\n",
    "\n",
    "# Root that contains your SKU trees (used to locate the source)\n",
    "WORKING_DIR = \"/content/drive/MyDrive/SikSilk\"                  # @param {type:\"string\"}\n",
    "\n",
    "# Root for (subcategory-wide) garment masks to constrain the detector\n",
    "MASKS_ROOT = \"/content/drive/MyDrive/SKSLK_MODELS\"              # @param {type:\"string\"}\n",
    "\n",
    "# Model/runtime knobs\n",
    "DEVICE_STR = \"cuda\"\n",
    "INPAINT_GENEROUS_PAD = 150                                      # @param {type:\"integer\"}\n",
    "INPAINT_TINY_PAD = 6                                            # @param {type:\"integer\"}\n",
    "INPAINT_SEED = 2025                                             # @param {type:\"integer\"}\n",
    "VISUALIZE = True                                                # @param {type:\"boolean\"}\n",
    "\n",
    "# File patterns\n",
    "VALID_EXTS = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\")\n",
    "\n",
    "# Allowed detail tokens (normalized)\n",
    "ALLOWED_DETAIL_TYPES = [\"crest\", \"logo\", \"patch\", \"waist text\", \"sleeve text\"]\n",
    "\n",
    "# Allowed garment tokens (for prompting DINO)\n",
    "ALLOWED_GARMENT_TYPES = [\n",
    "    \"hoodie\",\"jeans\",\"joggers\",\"shorts\",\"sweater\",\"swimwear\",\"t-shirt\",\"shirts\",\n",
    "    \"track top\",\"trousers\",\"twinset\",\"polo\",\"vests\",\"shirts\"\n",
    "]\n",
    "TOP_GARMENTS = [\"t-shirt\", \"shirt\", \"sweater\", \"hoodie\", \"track top\", \"vest\"]\n",
    "BOTTOM_GARMENTS = [\"shorts\", \"jogger-trousers\", \"trousers\", \"jeans\", \"swimwear\"]\n",
    "TWINSET_TYPES = [\"twinset\"]\n",
    "\n",
    "# Angle parsing / source lookup helpers\n",
    "BASE_NAMES = [\"fr_rght\", \"fr_lft\", \"fr_cl\",\n",
    "              #\"bc_rght\", \"bc_lft\", \"bc_cl\", \"bc\",\n",
    "              \"fr\", \"lft\", \"rght\"]\n",
    "ACCEPTABLE_SUFFIXES = [\"cut\"]\n",
    "\n",
    "# Skip if already have any inpainted output in TARGET_DIR for this SKU+angle\n",
    "SKIP_IF_ALREADY_INPAINTED = True # @param {type:\"boolean\"}\n",
    "\n",
    "USE_BF16_INFERENCE = True  # global toggle\n",
    "\n",
    "\n",
    "# Create target dir if missing\n",
    "import os, pathlib\n",
    "pathlib.Path(TARGET_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#INSTALLS (restart & reinstall again after this)"
   ],
   "metadata": {
    "id": "3WXWS5m9Ozyt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 via Hugging Face transformers\n",
    "!pip install -q \"git+https://github.com/huggingface/transformers.git\"\n"
   ],
   "metadata": {
    "id": "0ZUgwThxBmUv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "166da4bd-6295-4bc3-9072-91d530800dc8"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m516.1/516.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# GroundingDINO setup removed \u2014 SAM3 now handles detection + segmentation in one model.\n"
   ],
   "metadata": {
    "id": "MHz7WW6oBpDz"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%pip -q install open_clip_torch ninja wheel transformers accelerate \\\n",
    "                 sentencepiece protobuf huggingface_hub opencv-python\n",
    "!pip install -U --no-deps --force-reinstall \"git+https://github.com/huggingface/diffusers.git@main\"\n",
    "#%pip -q install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install --upgrade open_clip_torch"
   ],
   "metadata": {
    "collapsed": true,
    "id": "CjRycBjyBsdk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c46ccd3b-1454-41b8-ed2a-7d60b86d25c5"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting git+https://github.com/huggingface/diffusers.git@main\n",
      "  Cloning https://github.com/huggingface/diffusers.git (to revision main) to /tmp/pip-req-build-y5fmwbkv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-y5fmwbkv\n",
      "  Resolved https://github.com/huggingface/diffusers.git to commit 152f7ca357c066c4af3d1a58cdf17662ef5a2f87\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for diffusers: filename=diffusers-0.36.0.dev0-py3-none-any.whl size=4522015 sha256=c5ba8899b4f9247d010483a15993bde7900c36fe56401efd9362b2dab0512baf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b8o2j6ms/wheels/57/4f/c6/afad4149f5a3d7eae6412509d80a18b16df89dd1210ed4f326\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.35.2\n",
      "    Uninstalling diffusers-0.35.2:\n",
      "      Successfully uninstalled diffusers-0.35.2\n",
      "Successfully installed diffusers-0.36.0.dev0\n",
      "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.24.0+cu126)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.1.6)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n",
      "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (0.20.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub->open_clip_torch) (8.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub->open_clip_torch) (1.3.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install piexif"
   ],
   "metadata": {
    "id": "pm9LoEbd5fn9"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/\n",
    "!git clone --depth 1 https://github.com/song-wensong/insert-anything.git"
   ],
   "metadata": {
    "id": "kufDQpabB4h7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "803c6239-e109-41ce-c7bb-0c99c1156dec"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "Cloning into 'insert-anything'...\n",
      "remote: Enumerating objects: 109, done.\u001b[K\n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 109 (delta 3), reused 96 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (109/109), 67.31 MiB | 15.53 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.2.0+torch2.6-cp312-cp312-linux_x86_64.whl\n",
    "!pip install torch==2.6 torchvision==0.21 torchaudio==2.6\n",
    "!pip install ninja wheel diffusers transformers accelerate sentencepiece protobuf huggingface_hub\n",
    "!git clone https://huggingface.co/aha2023/insert-anything-lora-for-nunchaku"
   ],
   "metadata": {
    "id": "kkLr9zB3CJEl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "00fb6f7f-c8cd-4973-acb0-3892e7f9f441"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting nunchaku==0.2.0+torch2.6\n",
      "  Downloading https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.2.0+torch2.6-cp312-cp312-linux_x86_64.whl (114.0 MB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m114.0/114.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: diffusers>=0.32.2 in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (0.36.0.dev0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (5.0.0.dev0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (5.29.5)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from nunchaku==0.2.0+torch2.6) (1.1.6)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (3.20.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (0.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (0.7.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (11.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->nunchaku==0.2.0+torch2.6) (4.15.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->nunchaku==0.2.0+torch2.6) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->nunchaku==0.2.0+torch2.6) (2.9.0+cu126)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->nunchaku==0.2.0+torch2.6) (0.22.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (3.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (2.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub->nunchaku==0.2.0+torch2.6) (8.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->diffusers>=0.32.2->nunchaku==0.2.0+torch2.6) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate->nunchaku==0.2.0+torch2.6) (3.0.3)\n",
      "Installing collected packages: nunchaku\n",
      "Successfully installed nunchaku-0.2.0+torch2.6\n",
      "Collecting torch==2.6\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.21\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.6\n",
      "  Downloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.6)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6) (3.0.3)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.24.0+cu126\n",
      "    Uninstalling torchvision-0.24.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.24.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.9.0+cu126\n",
      "    Uninstalling torchaudio-2.9.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.36.0.dev0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0.dev0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (1.1.6)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.20.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.7.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers) (0.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->diffusers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Cloning into 'insert-anything-lora-for-nunchaku'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 23 (delta 7), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (23/23), 17.65 KiB | 3.53 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 2.85 GiB | 157.29 MiB/s, done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#SETUP"
   ],
   "metadata": {
    "id": "SdvcT9PEO5ye"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, torch, numpy as np, cv2, base64, gc, json\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageOps\n",
    "import piexif\n",
    "\n",
    "CPU_DEVICE = torch.device(\"cpu\")\n",
    "GPU_DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else CPU_DEVICE\n",
    "\n",
    "device = torch.device(DEVICE_STR if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\u2705 Torch device:\", device)\n"
   ],
   "metadata": {
    "id": "HNmBx946B-JK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d7a0b5e1-b424-4d84-c4d5-58fb1f726244"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Torch device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Sam3Processor, Sam3Model\n",
    "\n",
    "HF_SAM3_ID = \"facebook/sam3\"\n",
    "SAM3_CONFIDENCE = 0.05   # permissive to catch small logos; raise if predictions get noisy\n",
    "SAM3_RESOLUTION = 1024\n",
    "SAM3_PREFERRED_DEVICE = GPU_DEVICE\n",
    "SAM3_DEVICE = CPU_DEVICE  # keep SAM3 on CPU until needed to leave VRAM for insert-anything\n",
    "\n",
    "sam3_processor = Sam3Processor.from_pretrained(HF_SAM3_ID)\n",
    "sam3_model = Sam3Model.from_pretrained(HF_SAM3_ID).to(SAM3_DEVICE)\n",
    "sam3_model.eval()\n",
    "print(f\"\u2705 HF SAM3 ready (current device: {SAM3_DEVICE}, preferred: {SAM3_PREFERRED_DEVICE})\")\n"
   ],
   "metadata": {
    "id": "azEqfSb-5p5F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "0ce3dbfa3a114d988fc62b6c9ed98aab",
      "11517e487afa473abac383b9efaa5c7b",
      "7028b59f4d5c4166b1769e140ef386fd",
      "0a73f3bacea845379e0b6a927c9dea7e",
      "1fcf083bbf954ff1bf37ada3cd3c30d5",
      "4fd93a4cd2de46bd9d15a4755ae12102",
      "a51521be386846eda04d2bdf520d52f7",
      "0888c84d331d4452a5cce6d6afda14e0",
      "7c5e63df2c3c4144a95b83877d2e622a",
      "0b14e06815a64828b6c1bed44b72e7d4",
      "f5962d2c47cd4fe286c17bd62f62aadb"
     ]
    },
    "outputId": "05277307-ad6b-4572-ed41-3b6d82663ec2"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/1468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ce3dbfa3a114d988fc62b6c9ed98aab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 HF SAM3 ready (text \u2192 boxes \u2192 masks)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 replaces the old SAM2 predictor \u2014 no extra setup needed.\n"
   ],
   "metadata": {
    "id": "vuJxHufYTNKj"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SAM3 initialized above.\n"
   ],
   "metadata": {
    "id": "jCTA2cBPTQNJ"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Insert_anything on nunchaku\n",
    "%cd /content/insert-anything\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import FluxFillPipeline, FluxPriorReduxPipeline\n",
    "from utils.utils import get_bbox_from_mask, expand_bbox, pad_to_square, box2squre, expand_image_mask\n",
    "from nunchaku.models.transformers.transformer_flux import NunchakuFluxTransformer2dModel\n",
    "from datetime import datetime\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "IA_DEVICE = GPU_DEVICE\n",
    "dtype = torch.bfloat16\n",
    "size = (1024, 1024)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model and LoRA-for-nunchaku weights\n",
    "# Please replace the paths with your own paths\n",
    "transformer = NunchakuFluxTransformer2dModel.from_pretrained(\"mit-han-lab/svdq-int4-flux.1-fill-dev\")\n",
    "\n",
    "pipe = FluxFillPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Fill-dev\",\n",
    "    transformer=transformer,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "\n",
    "transformer.update_lora_params(\n",
    "    path_or_state_dict=\"/content/drive/MyDrive/insert-anything-lora/insert-anything_extracted_lora_rank_256-bf16.safetensors\"\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust the LoRA strength\n",
    "transformer.set_lora_strength(1)\n",
    "\n",
    "redux = FluxPriorReduxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Redux-dev\").to(dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "# The purpose of this code is to reduce the GPU memory usage to 26GB, but it will increase the inference time accordingly.\n",
    "pipe.to(IA_DEVICE)\n",
    "redux.to(IA_DEVICE)\n",
    "os.environ[\"NNCF_GROUP_SIZE\"] = \"-1\"      # disable token merging\n",
    "print(f\"\u2705 Insert-anything pipelines ready on {IA_DEVICE}\")\n"
   ],
   "metadata": {
    "id": "f4e99gLxT3TM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "dccaf990377b4690aae829d03ed302de",
      "908d7b1661bc4a5aaf3108983819b5a9",
      "71175ff97e9a43c3bac1cbc72e5060bd",
      "48f01c245c4542a6868c340fb08382f7",
      "7c585665999144f2b0611eff8248cdd8",
      "e1df25f9b36f45698c6efb70f8310fb7",
      "24bfa48a0cc74f8ca850c0d6217f1d1d",
      "100f33513f2246ae89d0da72f0fb5108",
      "bf58709f69434605a871dbd8eed256a9",
      "d55fff92fd05430cac0d0dca873efa3d",
      "ae4b4886a23e4230b44446164474360a",
      "9541658b7f32499eb83d1f34f1a99829",
      "81b2e62a17d04660a6bf0131d0184d78",
      "535f81d037ed40859bfb0855f1a7fac1",
      "16ac3d25bc8a4fb4aba749a92251bde6",
      "f0b8247247aa4cf7974b9f502ba79c5d",
      "a2cdfa3fcae2497085e7e3ed23742de6",
      "2ab9416431a44ad9abc207f994d3b0b5",
      "ad7a42121b584cd0bd83f8dcf3360f69",
      "e4cb80f7ca3c4669b8eb97018473de19",
      "56ded5613f224feaaf90f20bb65c76a9",
      "52564325ca684d0c9ed39bb4e75d5829",
      "2b09b1b42cdb491aa3c75dd8570ff564",
      "5e4a887fd9e64c209346b02c675e44a1",
      "c11c0f061ccc4a04a8c3bdd12a190eda",
      "011ebf3383414757aeaaff9e37c0d3c0",
      "e646a855f00c430ab191a01a223abe77",
      "63db910c289f4bc09566d763ba2332ec",
      "c3f3b06c97d94390ac064e6d91969cad",
      "4cd604395db940f792a8fe97d7e959a0",
      "b82a6335abb74cb5b4985a9da8ae2018",
      "944a41be57094812b61f7e84bcf3fbd7",
      "a957690fe34f4d60b9b75df9e9205396",
      "4723b5d76a5f4406a454c777e040d321",
      "ce9d5c9bb4434dd6b6108293e895e87c",
      "106b9309c97943c393427cfb230e4bd1",
      "19e597fba5b042a7bd4f98ef29c6a1c1",
      "7cfbfb55d60240e28751df0f8f988f5a",
      "d269690bf0724663a56053d16447472b",
      "4e8984a1507e4049903e8068fe0ba6fb",
      "85197b3ff708441fb76621de24d9bdef",
      "a807732abb23439ebf7ee0f50eb430ee",
      "71a39ba304254e748a816935b23dc331",
      "2f2eca9f0a8744c89d7eb58a79972823",
      "9526c597587746f4a26be50d4697f2e8",
      "68f54fb0fe1d4b1b8f8a7a803a5748cd",
      "6e889fcd02dd4bababd5337a57cca486",
      "f8c9c744b9eb450292cfaad5b3831c4e",
      "5d240d5e53c849d6b022ba87c1714837",
      "ab9b840ed6a0435f8b7db9e7e209f935",
      "16ba9ff3771a427abdf15eb6ea40aec8",
      "bf4a4eb38715412980ab268c3d98c427",
      "df4fb92d216e42c488109fafb0d67c52",
      "f2ee3ddd56d04477b19e9282b7d9ad87",
      "5bb4b3ef23a84e8f9c9f93af3e3a63a3"
     ]
    },
    "outputId": "eed1411f-1f13-4373-da3d-c05b805365da"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/insert-anything\n",
      "Injecting quantized module\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dccaf990377b4690aae829d03ed302de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9541658b7f32499eb83d1f34f1a99829"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/219 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b09b1b42cdb491aa3c75dd8570ff564"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Converting LoRAs to nunchaku format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [00:04<00:00, 14.06it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4723b5d76a5f4406a454c777e040d321"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/448 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9526c597587746f4a26be50d4697f2e8"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UTILS"
   ],
   "metadata": {
    "id": "0nznhr0CO8xT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "q5N9LysfFp9c"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def open_upright(path) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return ImageOps.exif_transpose(im).convert(\"RGB\")\n",
    "\n",
    "def open_source_with_black_bg(path: str) -> Image.Image:\n",
    "    im = Image.open(path)\n",
    "    im = ImageOps.exif_transpose(im)\n",
    "    name_low = os.path.basename(path).lower()\n",
    "    if \"_cut\" in name_low and im.mode in (\"RGBA\",\"LA\"):\n",
    "        rgb = im.convert(\"RGB\")\n",
    "        alpha = im.getchannel(\"A\")\n",
    "        black = Image.new(\"RGB\", im.size, (0,0,0))\n",
    "        return Image.composite(rgb, black, alpha)\n",
    "    return im.convert(\"RGB\")\n",
    "\n",
    "\n",
    "# NEW \u2014 root of subcategory-wide garment masks\n",
    "MASKS_ROOT = '/content/drive/MyDrive/SKSLK_MODELS'\n",
    "MASK_EXTS = ('.png', '.jpg', '.jpeg', '.webp', '.PNG', '.JPG', '.JPEG', '.WEBP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "\n",
    "# --- Helper: get <Category>/<Subcategory> from the *source* path ------------\n",
    "_SKU_DIR_RE = re.compile(r\"SS-\\d{3,7}\", re.IGNORECASE)\n",
    "\n",
    "def _category_subcategory_from_source(src_path: str) -> tuple[str, str] | None:\n",
    "    \"\"\"\n",
    "    Resolve (Category, Subcategory) from the garment *source* path.\n",
    "    Preferred: relative to WORKING_DIR \u2192 parts[0], parts[1].\n",
    "    Fallback: find the SKU folder in the path and take the two parents.\n",
    "    Returns None if not resolvable.\n",
    "    \"\"\"\n",
    "    p = Path(src_path).resolve()\n",
    "    wr = Path(WORKING_DIR).resolve()\n",
    "\n",
    "    # Preferred: relative to WORKING_DIR\n",
    "    try:\n",
    "        rel = p.relative_to(wr)\n",
    "        parts = rel.parts\n",
    "        # Expect: Category/Subcategory/SKU/<file>\n",
    "        if len(parts) >= 3:\n",
    "            return parts[0], parts[1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: locate the SKU dir and take its two parents as Cat/Subcat\n",
    "    parts = p.parts\n",
    "    sku_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if _SKU_DIR_RE.fullmatch(part or \"\"):\n",
    "            sku_idx = i\n",
    "            break\n",
    "    if sku_idx is not None and sku_idx >= 2:\n",
    "        return parts[sku_idx - 2], parts[sku_idx - 1]\n",
    "\n",
    "    # Last resort: try after an explicit 'SikSilk' anchor\n",
    "    if \"SikSilk\" in parts:\n",
    "        j = parts.index(\"SikSilk\")\n",
    "        if len(parts) >= j + 3:\n",
    "            return parts[j + 1], parts[j + 2]\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- New: derive mask basename from *angle*, not from filename heuristics ----\n",
    "def _mask_basename_from_angle(angle_code: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Map 'fr' -> 'fr_mask', 'fr_lft' -> 'fr_lft_mask', 'bc_cl' -> 'bc_cl_mask', etc.\n",
    "    If angle_code is missing, return None (\u2192 no mask).\n",
    "    \"\"\"\n",
    "    if not angle_code:\n",
    "        return None\n",
    "    angle = angle_code.strip().lower()\n",
    "    return f\"{angle}_mask\"\n",
    "\n",
    "# --- Exact-only mask finder ---------------------------------------------------\n",
    "\n",
    "def find_mask_for_generated_exact(gen_path: str, source_path: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    EXACT lookup (no fuzzy fallbacks):\n",
    "      angle  = parsed from queued filename/path (e.g., SS-12345_fr_cl.* -> 'fr_cl')\n",
    "      (cat, subcat) = derived from source_path\n",
    "      priority: <angle>_mask_agnostic.<ext>  \u2192  <angle>_mask.<ext>\n",
    "      searched in: MASKS_ROOT / cat / subcat\n",
    "    \"\"\"\n",
    "    # 1) angle from queued filename\n",
    "    _, angle = extract_sku_and_angle_from_path(gen_path)\n",
    "    if not angle:\n",
    "        print(\"\u26a0\ufe0f  No angle parsed \u2014 proceeding without a mask.\")\n",
    "        return None\n",
    "    angle = angle.strip().lower()\n",
    "\n",
    "    # 2) category/subcategory from source path\n",
    "    cat_sub = _category_subcategory_from_source(source_path)\n",
    "    if not cat_sub:\n",
    "        print(\"\u26a0\ufe0f  Could not resolve Category/Subcategory from source path \u2014 no mask.\")\n",
    "        return None\n",
    "    category, subcategory = cat_sub\n",
    "\n",
    "    mask_dir = Path(MASKS_ROOT) / category / subcategory\n",
    "\n",
    "    # 3) Try agnostic first, then regular; exact names only\n",
    "    candidates = [f\"{angle}_mask_agnostic\", f\"{angle}_mask\"]\n",
    "\n",
    "    for name in candidates:\n",
    "        for ext in MASK_EXTS:\n",
    "            cand = mask_dir / f\"{name}{ext}\"\n",
    "            if cand.exists():\n",
    "                which = \"agnostic\" if name.endswith(\"_agnostic\") else \"regular\"\n",
    "                print(f\"\u2705 Found {which} mask: {cand}\")\n",
    "                return cand\n",
    "\n",
    "    print(f\"\u26a0\ufe0f  No exact mask found in {mask_dir} for angle '{angle}' \"\n",
    "          f\"(tried {candidates} with MASK_EXTS). Proceeding without mask.\")\n",
    "    return None\n",
    "\n",
    "def load_binary_mask_for_generated(gen_path: str, source_path: str, gen_img: Image.Image) -> np.ndarray | None:\n",
    "    mp = find_mask_for_generated_exact(gen_path, source_path)\n",
    "    if mp is None:\n",
    "        return None\n",
    "    with Image.open(mp) as m:\n",
    "        m = ImageOps.exif_transpose(m)\n",
    "        return align_mask_to_image(m, gen_img)\n",
    "\n",
    "# --- Align (unchanged) --------------------------------------------------------\n",
    "def align_mask_to_image(mask_img: Image.Image, target_img: Image.Image) -> np.ndarray:\n",
    "    mw, mh = mask_img.size\n",
    "    tw, th = target_img.size\n",
    "    if mh == th and mw > 0 and (tw % mw) == 0 and 1 < (tw // mw) <= 3:\n",
    "        k = tw // mw\n",
    "        tiled = Image.new('L', (tw, th), 0)\n",
    "        src = mask_img.convert('L')\n",
    "        for i in range(k):\n",
    "            tiled.paste(src, (i * mw, 0))\n",
    "        M = np.array(tiled, dtype=np.uint8)\n",
    "    else:\n",
    "        if mw == 0 or mh == 0:\n",
    "            return np.zeros((th, tw), np.uint8)\n",
    "        scale = max(mw / tw, mh / th)\n",
    "        new_w = int(round(mw / scale)); new_h = int(round(mh / scale))\n",
    "        m_resized = mask_img.convert('L').resize((new_w, new_h), Image.NEAREST)\n",
    "        M = np.zeros((th, tw), np.uint8)\n",
    "        x0 = (tw - new_w) // 2; y0 = (th - new_h) // 2\n",
    "        M[y0:y0+new_h, x0:x0+new_w] = np.array(m_resized, dtype=np.uint8)\n",
    "    return ((M > 127).astype(np.uint8) * 255)\n",
    "\n",
    "\n",
    "def enlarge_mask(mask_np: np.ndarray, scale: float = 1.05) -> np.ndarray:\n",
    "    \"\"\"Dilate mask outward based on object size (\u2248scale of the foreground).\"\"\"\n",
    "    if mask_np is None:\n",
    "        return None\n",
    "    mask = (mask_np > 0).astype(np.uint8)\n",
    "    ys, xs = np.where(mask)\n",
    "    if ys.size == 0 or scale <= 1.0:\n",
    "        return (mask_np > 0).astype(np.uint8) * 255\n",
    "    h_obj = ys.max() - ys.min() + 1\n",
    "    w_obj = xs.max() - xs.min() + 1\n",
    "    grow = max(1, int(round(max(h_obj, w_obj) * (scale - 1.0))))\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (grow * 2 + 1, grow * 2 + 1))\n",
    "    out = cv2.dilate(mask, kernel, iterations=1)\n",
    "    return (out > 0).astype(np.uint8) * 255\n",
    "\n",
    "\n",
    "\n",
    "# ---- visuals\n",
    "def _draw_bbox(img: Image.Image, bb_xyxy, color=\"lime\", width=4):\n",
    "    out = img.copy()\n",
    "    if bb_xyxy is None: return out\n",
    "    draw = ImageDraw.Draw(out)\n",
    "    draw.rectangle(bb_xyxy, outline=color, width=width)\n",
    "    return out\n",
    "\n",
    "def _show_images(pairs, cols=3, figsize=(16,12)):\n",
    "    rows = int(np.ceil(len(pairs) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows*cols>1 else [axes]\n",
    "    for ax,(title,img) in zip(axes, pairs):\n",
    "        ax.imshow(img); ax.set_title(title, fontsize=10); ax.axis(\"off\")\n",
    "    for ax in axes[len(pairs):]: ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def resize_and_pad(image, target_size=1024):\n",
    "    w, h = image.size\n",
    "    scale = target_size / max(1, max(w, h))\n",
    "    new_w, new_h = int(round(w * scale)), int(round(h * scale))\n",
    "    image_resized = image.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    pad_w = (target_size - new_w) // 2\n",
    "    pad_h = (target_size - new_h) // 2\n",
    "    padding = (pad_w, pad_h, target_size - new_w - pad_w, target_size - new_h - pad_h)\n",
    "\n",
    "    # \u2705 Match fill type to mode\n",
    "    mode = image_resized.mode\n",
    "    if mode in (\"L\", \"1\", \"I\", \"F\"):\n",
    "        fill_color = 0                      # int for single-channel\n",
    "    elif mode == \"RGBA\":\n",
    "        fill_color = (0, 0, 0, 0)           # transparent for RGBA\n",
    "    else:\n",
    "        fill_color = (0, 0, 0)              # RGB tuple for RGB/others\n",
    "\n",
    "    return ImageOps.expand(image_resized, padding, fill=fill_color)\n",
    "\n",
    "def box_1024_to_original(box_xyxy_1024, original_w, original_h):\n",
    "    x1_1024, y1_1024, x2_1024, y2_1024 = [float(v) for v in box_xyxy_1024]\n",
    "    target_size = 1024\n",
    "    w, h = original_w, original_h\n",
    "    scale = target_size / max(w, h)\n",
    "    new_w, new_h = int(round(w*scale)), int(round(h*scale))\n",
    "    pad_w = (target_size - new_w)//2\n",
    "    pad_h = (target_size - new_h)//2\n",
    "    x1 = (x1_1024 - pad_w) / scale; x2 = (x2_1024 - pad_w) / scale\n",
    "    y1 = (y1_1024 - pad_h) / scale; y2 = (y2_1024 - pad_h) / scale\n",
    "    x1 = min(max(int(round(x1)),0), w); x2 = min(max(int(round(x2)),0), w)\n",
    "    y1 = min(max(int(round(y1)),0), h); y2 = min(max(int(round(y2)),0), h)\n",
    "    return [x1,y1,x2,y2]\n"
   ],
   "metadata": {
    "id": "VGxk8vnk5wU9"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_binary_mask(img_rgb: Image.Image, mask_np: np.ndarray | None, outside_color=(5,5,5)) -> Image.Image:\n",
    "    if mask_np is None:\n",
    "        return img_rgb\n",
    "    mask_L = Image.fromarray(mask_np.astype(np.uint8))\n",
    "    mode = img_rgb.mode\n",
    "    if mode not in (\"RGB\", \"RGBA\", \"L\"):\n",
    "        img_rgb = img_rgb.convert(\"RGB\")\n",
    "        mode = \"RGB\"\n",
    "    if mode == \"RGB\":\n",
    "        if isinstance(outside_color, int):\n",
    "            outside_color = (outside_color,) * 3\n",
    "        bg = Image.new(\"RGB\", img_rgb.size, outside_color)\n",
    "    elif mode == \"RGBA\":\n",
    "        if isinstance(outside_color, int):\n",
    "            outside_color = (outside_color,) * 3 + (255,)\n",
    "        elif len(outside_color) == 3:\n",
    "            outside_color = (*outside_color, 255)\n",
    "        bg = Image.new(\"RGBA\", img_rgb.size, outside_color)\n",
    "    else:\n",
    "        if isinstance(outside_color, tuple):\n",
    "            outside_color = int(np.mean(outside_color))\n",
    "        bg = Image.new(\"L\", img_rgb.size, int(outside_color))\n",
    "    return Image.composite(img_rgb, bg, mask_L)\n",
    "\n",
    "\n",
    "# Dynamic, perimeter-based mask gating for detect_detail (logo-friendly)\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "\n",
    "# --- VRAM juggling to avoid SAM3 + IA sharing GPU at the same time ----------------\n",
    "if \"IA_DEVICE\" not in globals():\n",
    "    IA_DEVICE = GPU_DEVICE\n",
    "\n",
    "def _move_sam3_to(target: torch.device):\n",
    "    global SAM3_DEVICE\n",
    "    target = torch.device(target)\n",
    "    if target.type == \"cuda\" and not torch.cuda.is_available():\n",
    "        target = CPU_DEVICE\n",
    "    if SAM3_DEVICE == target:\n",
    "        return\n",
    "    sam3_model.to(target)\n",
    "    SAM3_DEVICE = target\n",
    "    if target.type == \"cpu\":\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "\n",
    "def _move_insert_anything_to(target: torch.device):\n",
    "    global IA_DEVICE\n",
    "    target = torch.device(target)\n",
    "    if target.type == \"cuda\" and not torch.cuda.is_available():\n",
    "        target = CPU_DEVICE\n",
    "    if IA_DEVICE == target:\n",
    "        return\n",
    "    pipe.to(target)\n",
    "    redux.to(target)\n",
    "    IA_DEVICE = target\n",
    "    if target.type == \"cpu\":\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "\n",
    "def _prepare_for_sam3():\n",
    "    _move_insert_anything_to(CPU_DEVICE)\n",
    "    _move_sam3_to(SAM3_PREFERRED_DEVICE)\n",
    "\n",
    "\n",
    "def _prepare_for_insert_anything():\n",
    "    _move_sam3_to(CPU_DEVICE)\n",
    "    _move_insert_anything_to(GPU_DEVICE)\n",
    "\n",
    "\n",
    "# --- SAM3 helpers -----------------------------------------------------------\n",
    "def _clip_box_to_image(box_xyxy, w: int, h: int):\n",
    "    x1, y1, x2, y2 = box_xyxy\n",
    "    x1 = max(0, min(w, int(round(x1))))\n",
    "    y1 = max(0, min(h, int(round(y1))))\n",
    "    x2 = max(0, min(w, int(round(x2))))\n",
    "    y2 = max(0, min(h, int(round(y2))))\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def _sam3_predict_text(image_pil: Image.Image, prompt: str, *, max_dets: int = 12, score_threshold: float = SAM3_CONFIDENCE):\n",
    "    \"\"\"Run SAM3 (HF) with a text prompt and return sorted predictions.\"\"\"\n",
    "    if not prompt:\n",
    "        return []\n",
    "    if \"sam3_processor\" not in globals() or \"sam3_model\" not in globals():\n",
    "        raise RuntimeError(\"SAM3 is not initialized. Run the SAM3 setup cell first.\")\n",
    "\n",
    "    inputs = sam3_processor(images=image_pil, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(SAM3_DEVICE) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = sam3_model(**inputs)\n",
    "\n",
    "    processed = sam3_processor.post_process_instance_segmentation(\n",
    "        outputs,\n",
    "        threshold=score_threshold,\n",
    "        target_sizes=[image_pil.size[::-1]],\n",
    "    )[0]\n",
    "\n",
    "    boxes = processed.get(\"boxes\")\n",
    "    scores = processed.get(\"scores\")\n",
    "    masks = processed.get(\"masks\")\n",
    "    if boxes is None or scores is None or boxes.numel() == 0:\n",
    "        return []\n",
    "\n",
    "    boxes_np = boxes.detach().cpu().numpy()\n",
    "    scores_np = scores.detach().cpu().numpy()\n",
    "    masks_np = masks.detach().cpu().numpy() if masks is not None else None\n",
    "\n",
    "    order = scores_np.argsort()[::-1]\n",
    "    preds = []\n",
    "    for idx in order[:max_dets]:\n",
    "        mask_np = None\n",
    "        if masks_np is not None:\n",
    "            mask_np = (masks_np[idx] > 0.5).astype(np.uint8)\n",
    "        preds.append({\n",
    "            \"box\": boxes_np[idx].tolist(),\n",
    "            \"score\": float(scores_np[idx]),\n",
    "            \"mask\": mask_np,\n",
    "        })\n",
    "    return preds\n",
    "\n",
    "def _mask_crop_to_full(mask_crop: np.ndarray | None, crop_box_on_full, full_size):\n",
    "    \"\"\"\n",
    "    Place a mask (aligned to a crop image) back onto the full canvas.\n",
    "    crop_box_on_full = (lx, ty, rx, by) used to produce the crop.\n",
    "    full_size = (W, H) of the destination image.\n",
    "    \"\"\"\n",
    "    if mask_crop is None or crop_box_on_full is None:\n",
    "        return mask_crop\n",
    "\n",
    "    full_w, full_h = full_size\n",
    "    lx, ty, rx, by = [int(round(v)) for v in crop_box_on_full]\n",
    "    x0, y0 = max(0, lx), max(0, ty)\n",
    "    x1, y1 = min(rx, full_w), min(by, full_h)\n",
    "    if x1 <= x0 or y1 <= y0:\n",
    "        return np.zeros((full_h, full_w), np.uint8)\n",
    "\n",
    "    mx0, my0 = max(0, -lx), max(0, -ty)\n",
    "    mx1, my1 = mx0 + (x1 - x0), my0 + (y1 - y0)\n",
    "\n",
    "    patch = mask_crop[my0:my1, mx0:mx1]\n",
    "    if patch.shape[1] != (x1 - x0) or patch.shape[0] != (y1 - y0):\n",
    "        patch = cv2.resize(patch, (x1 - x0, y1 - y0), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    full_mask = np.zeros((full_h, full_w), np.uint8)\n",
    "    full_mask[y0:y1, x0:x1] = (patch > 0).astype(np.uint8) * 255\n",
    "    return full_mask\n",
    "\n",
    "\n",
    "# Spatially guided detect_detail:\n",
    "# - perimeter-based mask polarity (as before)\n",
    "# - strict+tolerant mask gates\n",
    "# - spatial prior from source garment (normalized center+area)\n",
    "# - combined score = w_score * norm_score + w_spatial * spatial_affinity\n",
    "\n",
    "\n",
    "# ---- helpers ---------------------------------------------------------------\n",
    "# Top-7 spatial re-ranking for detail detection\n",
    "# - Keeps perimeter-based mask polarity & light gates\n",
    "# - Takes SAM3's highest-scoring proposals, filters with mask gates, then\n",
    "#   re-ranks TOP_K using spatial prior from the SOURCE garment\n",
    "# - Normalizes SAM3 scores within those K and slightly down-weights rank-1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers you already use elsewhere ----------\n",
    "def make_spatial_prior_from_box(bb_xyxy, img_size):\n",
    "    \"\"\"Build prior from SOURCE detail box on its garment crop. Normalized to [0,1].\"\"\"\n",
    "    if bb_xyxy is None:\n",
    "        return None\n",
    "    W, H = img_size\n",
    "    x1, y1, x2, y2 = [float(v) for v in bb_xyxy]\n",
    "    x1 = max(0.0, min(W, x1)); y1 = max(0.0, min(H, y1))\n",
    "    x2 = max(0.0, min(W, x2)); y2 = max(0.0, min(H, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    cx = ((x1 + x2) / 2.0) / max(1.0, W)\n",
    "    cy = ((y1 + y2) / 2.0) / max(1.0, H)\n",
    "    area = ((x2 - x1) * (y2 - y1)) / max(1.0, (W * H))\n",
    "    return {\"cx\": float(cx), \"cy\": float(cy), \"area\": float(area)}\n",
    "\n",
    "def _spatial_affinity(cx_n, cy_n, area_n, prior, mirror_ok=True,\n",
    "                      sigma_center=0.16, sigma_area=0.50):\n",
    "    \"\"\"Gaussian affinity in [0,1] for center & (log)area; mirror-aware.\"\"\"\n",
    "    def _aff(cx_p):\n",
    "        dc2 = (cx_n - cx_p)**2 + (cy_n - prior[\"cy\"])**2\n",
    "        s_center = np.exp(- dc2 / (2.0 * (sigma_center**2)))\n",
    "        a = max(1e-6, area_n); ap = max(1e-6, prior[\"area\"])\n",
    "        dlog = np.log(a / ap)\n",
    "        s_area = np.exp(- (dlog**2) / (2.0 * (sigma_area**2)))\n",
    "        return float(s_center * s_area)\n",
    "    base = _aff(prior[\"cx\"])\n",
    "    if mirror_ok:\n",
    "        return max(base, _aff(1.0 - prior[\"cx\"]))\n",
    "    return base\n",
    "\n",
    "# ---------- main: top-7 re-ranking ----------\n",
    "\n",
    "\n",
    "def _ensure_mask_for_image(mask_input, image_pil, *, crop_box_on_full=None):\n",
    "    \"\"\"\n",
    "    Align a mask to image_pil.\n",
    "\n",
    "    mask_input:\n",
    "      \u2022 np.ndarray aligned to image_pil (H\u00d7W)  OR\n",
    "      \u2022 (mask_full_np, \"FULL\") + crop_box_on_full=(lx,ty,rx,by) from crop_to_square\n",
    "\n",
    "    Returns: uint8 mask (0/255) aligned to image_pil.size, with the same padding\n",
    "    behavior as crop_to_square (i.e., if the crop went outside, we pad zeros).\n",
    "    \"\"\"\n",
    "    if mask_input is None or (isinstance(mask_input, tuple) and len(mask_input)==2 and mask_input[0] is None):\n",
    "        return None\n",
    "\n",
    "    # Case 1: already aligned to this image\n",
    "    if not (isinstance(mask_input, tuple) and len(mask_input) == 2 and isinstance(mask_input[0], np.ndarray) and mask_input[1] == \"FULL\"):\n",
    "        m = mask_input\n",
    "        if m.ndim == 3:\n",
    "            m = m[...,0] if m.shape[2] > 1 else m.squeeze(-1)\n",
    "        if m.dtype != np.uint8:\n",
    "            m = (m > 0).astype(np.uint8) * 255\n",
    "        if (m.shape[1], m.shape[0]) != image_pil.size:\n",
    "            m = cv2.resize(m, image_pil.size, interpolation=cv2.INTER_NEAREST)\n",
    "        return m\n",
    "\n",
    "    # Case 2: FULL mask + crop box from crop_to_square\n",
    "    mask_full, _ = mask_input\n",
    "    assert crop_box_on_full is not None, \"crop_box_on_full is required for FULL mask.\"\n",
    "\n",
    "    Hf, Wf = mask_full.shape[:2]\n",
    "    lx, ty, rx, by = crop_box_on_full  # exactly what crop_to_square returned\n",
    "\n",
    "    # Target canvas (the square side used by crop_to_square)\n",
    "    tgt_w = int(round(rx - lx))\n",
    "    tgt_h = int(round(by - ty))\n",
    "\n",
    "    # Source window (clamped to the full image bounds)\n",
    "    sx1 = int(np.floor(max(0, lx)))\n",
    "    sy1 = int(np.floor(max(0, ty)))\n",
    "    sx2 = int(np.ceil(min(Wf, rx)))\n",
    "    sy2 = int(np.ceil(min(Hf, by)))\n",
    "\n",
    "    # Offsets where the source window lands on the target canvas\n",
    "    dx = int(np.floor(max(0, -lx)))   # same as crop_to_square's dx\n",
    "    dy = int(np.floor(max(0, -ty)))   # same as crop_to_square's dy\n",
    "\n",
    "    # Build canvas and paste the clipped region at (dx,dy)\n",
    "    canvas = np.zeros((tgt_h, tgt_w), dtype=np.uint8)\n",
    "    if sx2 > sx1 and sy2 > sy1:\n",
    "        patch = mask_full[sy1:sy2, sx1:sx2]\n",
    "        if patch.ndim == 3:\n",
    "            patch = patch[...,0] if patch.shape[2] > 1 else patch.squeeze(-1)\n",
    "        ph, pw = patch.shape[:2]\n",
    "        canvas[dy:dy+ph, dx:dx+pw] = (patch > 0).astype(np.uint8) * 255\n",
    "\n",
    "    # If image_pil size differs by a pixel due to rounding, align by resize\n",
    "    if (canvas.shape[1], canvas.shape[0]) != image_pil.size:\n",
    "        canvas = cv2.resize(canvas, image_pil.size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def _build_inside_mask_1024(mask_aligned_np, image_pil, *,\n",
    "                            border_sample_px=2, erode_px=1, dilate_px=2,\n",
    "                            debug=False):\n",
    "    \"\"\"\n",
    "    Build 1024\u00d71024 INSIDE mask with perimeter-based polarity, using the\n",
    "    SAME resize_and_pad as the image to guarantee geometric alignment.\n",
    "    \"\"\"\n",
    "    if mask_aligned_np is None:\n",
    "        return None\n",
    "\n",
    "    # 1) pad the mask to 1024 with the SAME routine as the image\n",
    "    mL = Image.fromarray(mask_aligned_np, mode=\"L\")\n",
    "    m1024L = resize_and_pad(mL, target_size=1024).convert(\"L\")\n",
    "    m1024 = (np.array(m1024L) > 0)\n",
    "\n",
    "    # 2) Perimeter-majority: which value dominates the border?\n",
    "    h, w = m1024.shape\n",
    "    b = max(1, int(border_sample_px))\n",
    "    perim = np.concatenate([m1024[0:b,:].ravel(), m1024[h-b:h,:].ravel(),\n",
    "                            m1024[:,0:b].ravel(), m1024[:,w-b:w].ravel()])\n",
    "    ones = int(perim.sum()); zeros = int(perim.size - perim.sum())\n",
    "    background_is_true = (ones >= zeros)   # majority on border = background\n",
    "    inside = (~m1024) if background_is_true else m1024\n",
    "\n",
    "    # 3) Moprhology for robust gating\n",
    "    if erode_px > 0:\n",
    "        k_e = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erode_px*2+1, erode_px*2+1))\n",
    "        inside = cv2.erode(inside.astype(np.uint8), k_e, 1).astype(bool)\n",
    "    if dilate_px > 0:\n",
    "        k_d = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_px*2+1, dilate_px*2+1))\n",
    "        inside = cv2.dilate(inside.astype(np.uint8), k_d, 1).astype(bool)\n",
    "\n",
    "    if debug:\n",
    "        bg_txt = \"white/True\" if background_is_true else \"black/False\"\n",
    "        cov = float(inside.mean())\n",
    "        print(f\"[mask1024] perimeter True={ones} False={zeros} \u2192 background={bg_txt}; inside_cov={cov:.3f}\")\n",
    "\n",
    "    return inside\n",
    "\n",
    "\n",
    "def detect_detail_topk7(image_pil: Image.Image,\n",
    "                        detail_type: str,\n",
    "                        *,\n",
    "                        source_prior: dict | None,\n",
    "                        restrict_mask,                 # EITHER aligned np.ndarray OR (mask_full_np, \"FULL\")\n",
    "                        crop_box_on_full=None,         # required if restrict_mask is (\"FULL\")\n",
    "                        threshold: float = 0.05,\n",
    "                        TOP_K: int = 7,\n",
    "                        mirror_ok: bool = True,\n",
    "                        # light mask gates\n",
    "                        min_inside_frac: float = 0.30,\n",
    "                        center_must_be_inside: bool = True,\n",
    "                        erode_px: int = 1,\n",
    "                        dilate_px: int = 2,\n",
    "                        border_sample_px: int = 2,\n",
    "                        # scoring weights\n",
    "                        w_spatial: float = 0.65,\n",
    "                        w_score: float = 0.35,\n",
    "                        rank_weights: list[float] = None,\n",
    "                        debug: bool = False,\n",
    "                        viz: bool = False,\n",
    "                        viz_overlay_mask: bool = True):\n",
    "    \"\"\"\n",
    "    Robust top-7 re-ranking using SAM3 detections and optional mask gates.\n",
    "    Returns (xyxy_on_image, raw_score, mask_on_image or None)\n",
    "    \"\"\"\n",
    "    if rank_weights is None:\n",
    "        rank_weights = [0.92, 1.00, 0.98, 0.97, 0.96, 0.955, 0.95]\n",
    "\n",
    "    W0, H0 = image_pil.size\n",
    "    prompt = (detail_type or \"\").strip() + \".\"\n",
    "\n",
    "    mask_aligned = _ensure_mask_for_image(restrict_mask, image_pil, crop_box_on_full=crop_box_on_full)\n",
    "    mask_bool = (mask_aligned > 0) if mask_aligned is not None else None\n",
    "\n",
    "    preds = _sam3_predict_text(image_pil, prompt, max_dets=max(TOP_K * 3, 12), score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return (None, None, None)\n",
    "\n",
    "    picked = []\n",
    "    for rank, p in enumerate(preds):\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], W0, H0)\n",
    "        x1, y1, x2, y2 = box\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        inside_frac = 1.0\n",
    "        center_ok = True\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[y1:y2, x1:x2]\n",
    "            area = max(1, (x2 - x1) * (y2 - y1))\n",
    "            inside_frac = float(crop.sum()) / float(area)\n",
    "            cxp, cyp = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            center_ok = (0 <= cxp < W0 and 0 <= cyp < H0 and bool(mask_bool[cyp, cxp]))\n",
    "            if inside_frac < min_inside_frac or (center_must_be_inside and not center_ok):\n",
    "                continue\n",
    "\n",
    "        picked.append({\n",
    "            \"box\": box,\n",
    "            \"score\": float(p[\"score\"]),\n",
    "            \"rank\": rank,\n",
    "            \"mask\": p[\"mask\"],\n",
    "            \"inside_frac\": inside_frac,\n",
    "        })\n",
    "        if len(picked) >= TOP_K:\n",
    "            break\n",
    "\n",
    "    if not picked:\n",
    "        base = preds[0]\n",
    "        picked = [{\"box\": _clip_box_to_image(base[\"box\"], W0, H0),\n",
    "                   \"score\": float(base[\"score\"]),\n",
    "                   \"rank\": 0,\n",
    "                   \"mask\": base[\"mask\"],\n",
    "                   \"inside_frac\": 0.0}]\n",
    "\n",
    "    s = np.array([p[\"score\"] for p in picked], dtype=np.float32)\n",
    "    s_min, s_max = float(s.min()), float(s.max())\n",
    "    s_norm = np.ones_like(s) * 0.5 if s_max == s_min else (s - s_min) / (s_max - s_min)\n",
    "\n",
    "    best = None\n",
    "    for j, p in enumerate(picked):\n",
    "        rw = rank_weights[p[\"rank\"]] if p[\"rank\"] < len(rank_weights) else rank_weights[-1]\n",
    "        score_normed = float(s_norm[j] * rw)\n",
    "        x1, y1, x2, y2 = p[\"box\"]\n",
    "        area = max(1, (x2 - x1) * (y2 - y1))\n",
    "        cx_n = ((x1 + x2) / 2.0) / max(1.0, W0)\n",
    "        cy_n = ((y1 + y2) / 2.0) / max(1.0, H0)\n",
    "        area_n = area / float(max(1, W0 * H0))\n",
    "        spatial = _spatial_affinity(cx_n, cy_n, area_n, source_prior, mirror_ok=mirror_ok) if source_prior else 0.0\n",
    "        combo = w_spatial * spatial + w_score * score_normed\n",
    "        if best is None or combo > best[\"combo\"]:\n",
    "            best = {**p, \"combo\": float(combo), \"spatial\": float(spatial), \"score_norm\": score_normed}\n",
    "\n",
    "    return best[\"box\"], best[\"score\"], best[\"mask\"]\n",
    "\n",
    "\n",
    "def detect_detail(image_pil: Image.Image,\n",
    "                  detail_type: str,\n",
    "                  threshold: float = 0.05,\n",
    "                  used_boxes=None,\n",
    "                  keep_best: bool = False,\n",
    "                  iou_thr: float = 0.35,\n",
    "                  restrict_mask: np.ndarray | None = None,\n",
    "                  min_inside_frac: float = 0.40,\n",
    "                  max_outside_frac: float = 0.70,\n",
    "                  center_must_be_inside: bool = True,\n",
    "                  erode_px: int = 1,\n",
    "                  dilate_px: int = 2,\n",
    "                  border_sample_px: int = 2,\n",
    "                  debug: bool = False,\n",
    "                  debug_topk: int = 5,\n",
    "                  crop_box_on_full=None):\n",
    "    \"\"\"\n",
    "    Simplified detail locator using SAM3 text grounding.\n",
    "    Returns: (xyxy_on_image, score, mask_on_image)\n",
    "    \"\"\"\n",
    "    used_boxes = used_boxes or []\n",
    "    prompt = (detail_type or \"\").strip() + \".\"\n",
    "    W, H = image_pil.size\n",
    "\n",
    "    mask_aligned = _ensure_mask_for_image(restrict_mask, image_pil, crop_box_on_full=crop_box_on_full)\n",
    "    mask_bool = (mask_aligned > 0) if mask_aligned is not None else None\n",
    "\n",
    "    preds = _sam3_predict_text(image_pil, prompt, max_dets=10, score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return (None, None, None)\n",
    "\n",
    "    def _iou(a, b):\n",
    "        ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "        xi1, yi1 = max(ax1, bx1), max(ay1, by1)\n",
    "        xi2, yi2 = min(ax2, bx2), min(ay2, by2)\n",
    "        iw, ih = max(0, xi2 - xi1), max(0, yi2 - yi1)\n",
    "        inter = iw * ih\n",
    "        if inter == 0:\n",
    "            return 0.0\n",
    "        area_a = max(1, (ax2 - ax1) * (ay2 - ay1))\n",
    "        area_b = max(1, (bx2 - bx1) * (by2 - by1))\n",
    "        union = area_a + area_b - inter\n",
    "        return inter / union\n",
    "\n",
    "    best = None\n",
    "    debug_rows = []\n",
    "    for p in preds:\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], W, H)\n",
    "        x1, y1, x2, y2 = box\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        if any(_iou(box, ub) > iou_thr for ub in used_boxes):\n",
    "            continue\n",
    "\n",
    "        inside_frac = 1.0\n",
    "        outside_frac = 0.0\n",
    "        center_ok = True\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[y1:y2, x1:x2]\n",
    "            area = max(1, (x2 - x1) * (y2 - y1))\n",
    "            inside_frac = float(crop.sum()) / float(area)\n",
    "            outside_frac = 1.0 - inside_frac\n",
    "            cx_i, cy_i = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            center_ok = (0 <= cx_i < W and 0 <= cy_i < H and bool(mask_bool[cy_i, cx_i])) if center_must_be_inside else True\n",
    "            if not center_ok or inside_frac < min_inside_frac or outside_frac > max_outside_frac:\n",
    "                continue\n",
    "\n",
    "        if best is None or p[\"score\"] > best[\"score\"]:\n",
    "            best = {\"box\": box, \"score\": float(p[\"score\"]), \"mask\": p[\"mask\"], \"inside_frac\": inside_frac}\n",
    "\n",
    "        if debug and len(debug_rows) < debug_topk:\n",
    "            debug_rows.append({\n",
    "                \"score\": float(p[\"score\"]),\n",
    "                \"box\": box,\n",
    "                \"inside\": inside_frac,\n",
    "                \"outside\": outside_frac,\n",
    "                \"center\": center_ok,\n",
    "            })\n",
    "\n",
    "    if best is None:\n",
    "        if keep_best:\n",
    "            base = preds[0]\n",
    "            best = {\"box\": _clip_box_to_image(base[\"box\"], W, H), \"score\": float(base[\"score\"]), \"mask\": base[\"mask\"], \"inside_frac\": 0.0}\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"[detect_detail] No candidate satisfied mask gates.\")\n",
    "            return (None, None, None)\n",
    "\n",
    "    if debug and debug_rows:\n",
    "        print(\"[detect_detail/debug] top candidates (after score>thr):\")\n",
    "        for row in sorted(debug_rows, key=lambda r: r[\"score\"], reverse=True):\n",
    "            print(f\"  score={row['score']:.3f} inside={row['inside']:.2f} outside={row['outside']:.2f} center={row['center']} box={row['box']}\")\n",
    "\n",
    "    return best[\"box\"], best[\"score\"], best[\"mask\"]\n",
    "\n",
    "\n",
    "def detect_garment_box(img: Image.Image, garment_tag: str, threshold=0.25, restrict_mask: np.ndarray | None = None):\n",
    "    O_W, O_H = img.size\n",
    "    if restrict_mask is not None:\n",
    "        m1024 = resize_and_pad(Image.fromarray(restrict_mask, 'L'), 1024).convert('L')\n",
    "        mask_1024_np = (np.array(m1024) > 127)\n",
    "        ys, xs = np.where(mask_1024_np > 0)\n",
    "        if xs.size == 0 or ys.size == 0:\n",
    "            return None\n",
    "        x1, y1, x2, y2 = [float(xs.min()), float(ys.min()), float(xs.max()), float(ys.max())]\n",
    "        return box_1024_to_original([x1, y1, x2, y2], O_W, O_H)\n",
    "\n",
    "    preds = _sam3_predict_text(img, f\"{garment_tag.strip()} .\", max_dets=6, score_threshold=threshold)\n",
    "    if not preds:\n",
    "        return None\n",
    "\n",
    "    mask_bool = (restrict_mask > 0) if restrict_mask is not None else None\n",
    "    best = None\n",
    "    for p in preds:\n",
    "        if threshold is not None and float(p[\"score\"]) < threshold:\n",
    "            continue\n",
    "        box = _clip_box_to_image(p[\"box\"], O_W, O_H)\n",
    "        if box[2] <= box[0] or box[3] <= box[1]:\n",
    "            continue\n",
    "        if mask_bool is not None:\n",
    "            crop = mask_bool[box[1]:box[3], box[0]:box[2]]\n",
    "            if crop.size == 0 or float(crop.mean()) < 0.05:\n",
    "                continue\n",
    "        if best is None or p[\"score\"] > best[\"score\"]:\n",
    "            best = {\"box\": box, \"score\": float(p[\"score\"])}\n",
    "\n",
    "    return best[\"box\"] if best else None\n",
    "\n",
    "\n",
    "def bbox_to_mask(bb, img_size, pad_px=10):\n",
    "    W, H = img_size\n",
    "    x1, y1, x2, y2 = bb\n",
    "    x1 = max(0, x1 - pad_px); y1 = max(0, y1 - pad_px)\n",
    "    x2 = min(W - 1, x2 + pad_px); y2 = min(H - 1, y2 + pad_px)\n",
    "    m = np.zeros((H, W), np.uint8)\n",
    "    m[y1:y2, x1:x2] = 255\n",
    "    return m\n",
    "\n",
    "\n",
    "def crop_detail(image_pil, mask_np, bb_xyxy, out_size=1024, pad_px=20):\n",
    "    W, H = image_pil.size\n",
    "    x1, y1, x2, y2 = bb_xyxy\n",
    "    x1 = max(0, x1 - pad_px); y1 = max(0, y1 - pad_px)\n",
    "    x2 = min(W, x2 + pad_px); y2 = min(H, y2 + pad_px)\n",
    "    side = max(x2 - x1, y2 - y1)\n",
    "    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    lx = max(0, cx - side // 2); rx = lx + side\n",
    "    ty = max(0, cy - side // 2); by = ty + side\n",
    "    if rx > W:\n",
    "        lx -= (rx - W); rx = W\n",
    "    if by > H:\n",
    "        ty -= (by - H); by = H\n",
    "    crop_box = (lx, ty, rx, by)\n",
    "    img_c = image_pil.crop(crop_box).resize((out_size, out_size), Image.Resampling.LANCZOS)\n",
    "    m_c = mask_np[ty:by, lx:rx]\n",
    "    m_c = cv2.resize(m_c, (out_size, out_size), interpolation=cv2.INTER_NEAREST)\n",
    "    return img_c, m_c, crop_box\n",
    "\n",
    "\n",
    "def adaptive_brightness(img, strength_dark=0.15, strength_light=0.03, clip=(0, 245)):\n",
    "    a = np.asarray(img).astype(np.float32)\n",
    "    lum = 0.2126 * a[..., 0] + 0.7152 * a[..., 1] + 0.0722 * a[..., 2]\n",
    "    mean_lum = float(lum.mean() / 255.0)\n",
    "    if mean_lum < 0.5:\n",
    "        factor = 1 + (-strength_dark) * (0.5 - mean_lum) * 2\n",
    "    else:\n",
    "        factor = 1 + (strength_light) * (mean_lum - 0.5) * 2\n",
    "    out = np.clip(a * factor, *clip).astype(np.uint8)\n",
    "    return Image.fromarray(out)\n",
    "\n",
    "\n",
    "def paste_crop_back(full_img: Image.Image, edited_crop: Image.Image, crop_box, crop_mask: np.ndarray,\n",
    "                    expand_px=20, feather_px=10) -> Image.Image:\n",
    "    edited_crop = adaptive_brightness(edited_crop, strength_dark=0.15, strength_light=0.03)\n",
    "    x1, y1, x2, y2 = crop_box\n",
    "    tgt_w, tgt_h = x2 - x1, y2 - y1\n",
    "    edit_rs = edited_crop.resize((tgt_w, tgt_h), Image.Resampling.LANCZOS)\n",
    "    mask_np = cv2.resize(crop_mask, (tgt_w, tgt_h), interpolation=cv2.INTER_NEAREST)\n",
    "    bin_mask = (mask_np > 0).astype(np.uint8)\n",
    "    if expand_px > 0:\n",
    "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (expand_px * 2 + 1, expand_px * 2 + 1))\n",
    "        bin_mask = cv2.dilate(bin_mask, k, iterations=1)\n",
    "    alpha = cv2.GaussianBlur(bin_mask.astype(np.float32) * 255, (0, 0), sigmaX=feather_px, sigmaY=feather_px)\n",
    "    alpha[mask_np > 0] = 255\n",
    "    alpha = alpha.clip(0, 255).astype(np.uint8)\n",
    "    mask_img = Image.fromarray(alpha)\n",
    "\n",
    "    region = full_img.crop((x1, y1, x2, y2))\n",
    "    comp = Image.composite(edit_rs, region, mask_img)\n",
    "    full_img.paste(comp, (x1, y1))\n",
    "    return full_img\n",
    "\n"
   ],
   "metadata": {
    "id": "9TD4abQU5yio"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalization for detail types from legacy names\n",
    "def _normalize_detail_type(t: str) -> str:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    mapping = {\n",
    "        \"waistband lettering\": \"waist text\",\n",
    "        \"sleeve lettering\": \"sleeve text\",\n",
    "        \"sleeve_text\": \"sleeve text\",\n",
    "        \"waist_text\": \"waist text\",\n",
    "    }\n",
    "    return mapping.get(t, t)\n",
    "\n",
    "def _postprocess_details(payload: dict) -> dict:\n",
    "    details = payload.get(\"details\", [])\n",
    "    fixed = []\n",
    "    for d in details:\n",
    "        typ = _normalize_detail_type(d.get(\"type\"))\n",
    "        col = d.get(\"color\")\n",
    "        if typ in ALLOWED_DETAIL_TYPES:\n",
    "            ent = {\"type\": typ}\n",
    "            if typ != \"sleeve text\" and isinstance(col, str) and col.strip():\n",
    "                ent[\"color\"] = col.strip()\n",
    "            fixed.append(ent)\n",
    "    return {\"details\": fixed}\n",
    "\n",
    "def _try_parse_json(s: str) -> dict | None:\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"details\" in obj:\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # try to extract first {...}\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            if isinstance(obj, dict) and \"details\" in obj:\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def read_details_from_metadata(img_path: str) -> dict:\n",
    "    \"\"\"Return {'details':[...]} or {'details':[{'type':'logo'}]} if metadata not found.\"\"\"\n",
    "    try:\n",
    "        im = Image.open(img_path)\n",
    "        # 1) PNG/JPEG info dict\n",
    "        for k, v in (im.info or {}).items():\n",
    "            if isinstance(v, str):\n",
    "                obj = _try_parse_json(v)\n",
    "                if obj:\n",
    "                    return _postprocess_details(obj)\n",
    "        # 2) EXIF: UserComment / XPComment\n",
    "        try:\n",
    "            exif_dict = piexif.load(im.info.get(\"exif\", b\"\") or im.tobytes())\n",
    "        except Exception:\n",
    "            exif_dict = None\n",
    "\n",
    "        def _decode_uc(x):\n",
    "            if isinstance(x, bytes):\n",
    "                for head in [b\"ASCII\\0\\0\\0\", b\"UNICODE\\0\", b\"JIS\\0\\0\\0\"]:\n",
    "                    if x.startswith(head):\n",
    "                        x = x[len(head):]\n",
    "                try:\n",
    "                    return x.decode(\"utf-8\", \"ignore\")\n",
    "                except Exception:\n",
    "                    return x.decode(\"latin-1\", \"ignore\")\n",
    "            if isinstance(x, str):\n",
    "                return x\n",
    "            return None\n",
    "\n",
    "        if exif_dict:\n",
    "            uc = exif_dict.get(\"Exif\", {}).get(piexif.ExifIFD.UserComment, None)\n",
    "            s = _decode_uc(uc)\n",
    "            if s:\n",
    "                obj = _try_parse_json(s)\n",
    "                if obj:\n",
    "                    return _postprocess_details(obj)\n",
    "            xp = exif_dict.get(\"0th\", {}).get(0x9C9C, None)\n",
    "            if xp:\n",
    "                try:\n",
    "                    s = bytes(xp).decode(\"utf-16le\", \"ignore\").rstrip(\"\\x00\")\n",
    "                    obj = _try_parse_json(s)\n",
    "                    if obj:\n",
    "                        return _postprocess_details(obj)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # 3) XMP sidecar embedded?\n",
    "        if \"XML:com.adobe.xmp\" in (im.info or {}):\n",
    "            obj = _try_parse_json(im.info[\"XML:com.adobe.xmp\"])\n",
    "            if obj:\n",
    "                return _postprocess_details(obj)\n",
    "        # 4) Optional sidecar .json next to image\n",
    "        side = Path(img_path).with_suffix(\".json\")\n",
    "        if side.exists():\n",
    "            try:\n",
    "                obj = json.loads(side.read_text())\n",
    "                if \"details\" in obj:\n",
    "                    return _postprocess_details(obj)\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f metadata read failed for {img_path}: {e}\")\n",
    "\n",
    "    # \ud83d\udc47 Fallback when nothing found\n",
    "    return {\"details\": [{\"type\": \"logo\"}]}\n",
    "\n",
    "\n",
    "# Garment type inference (from path)\n",
    "def extract_garment_type_from_path(image_path: str, allowed_types=ALLOWED_GARMENT_TYPES) -> str:\n",
    "    from pathlib import Path as _P\n",
    "    import re\n",
    "    def singularize(s):\n",
    "        if len(s)>4:\n",
    "            if s.endswith(\"es\"): return s[:-2]\n",
    "            if s.endswith(\"s\"):  return s[:-1]\n",
    "        return s\n",
    "    def normalize_key(s): return singularize(s.replace(\"-\",\"\").replace(\"_\",\"\").lower().strip())\n",
    "    norm_map = {}\n",
    "    for t in allowed_types:\n",
    "        base = normalize_key(t)\n",
    "        norm_map[base]=t\n",
    "        if not base.endswith(\"s\"): norm_map[base+\"s\"]=t\n",
    "        else:\n",
    "            if base.endswith(\"es\"): norm_map[base[:-2]]=t\n",
    "            else: norm_map[base[:-1]]=t\n",
    "    p = _P(image_path)\n",
    "    file_compact = re.sub(r\"[^a-z]+\",\"\", p.stem.lower())\n",
    "    for k,v in norm_map.items():\n",
    "        if k and k in file_compact: return v\n",
    "    # parent folders\n",
    "    for part in reversed(p.parts[:-1]):\n",
    "        if part.startswith(\".\"): continue\n",
    "        toks = [singularize(t) for t in re.split(r\"[^a-z]+\", part.lower()) if t]\n",
    "        for tok in toks:\n",
    "            if tok in norm_map: return norm_map[tok]\n",
    "    return \"\"\n"
   ],
   "metadata": {
    "id": "DL1y4tUs51S_"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# === Robust SKU+angle parsing (handles: \"SS-28623_fr (1).png\", \"Copy of SS-12345_bc_lft_v2.png\", \"SS-55555_fr_cl.png\") ===\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "# Reuse your global config: BASE_NAMES, ACCEPTABLE_SUFFIXES, VALID_EXTS, WORKING_DIR\n",
    "\n",
    "_SKU_RE = re.compile(r\"(SS-\\d{3,7})\", re.IGNORECASE)\n",
    "_COPY_RE = re.compile(r\"^(?:copy of\\s+)+\", re.IGNORECASE)\n",
    "\n",
    "def _strip_copy_prefix(s: str) -> str:\n",
    "    return _COPY_RE.sub(\"\", s).strip()\n",
    "\n",
    "def _angle_tokens_desc() -> list[str]:\n",
    "    # Longest-first to prefer 'fr_rght' over 'fr'\n",
    "    return sorted(list(set(BASE_NAMES)), key=len, reverse=True)\n",
    "\n",
    "def _token_delim_search(token: str, text: str) -> re.Match | None:\n",
    "    \"\"\"\n",
    "    Find token delimited by non-alphanumerics (underscore is allowed as a delimiter).\n",
    "    We treat [A-Za-z0-9] as 'wordy'; underscores/spaces/()/- etc. are delimiters.\n",
    "    \"\"\"\n",
    "    # Escape underscores in token for regex\n",
    "    tok = re.escape(token)\n",
    "    pattern = rf\"(?<![A-Za-z0-9]){tok}(?![A-Za-z0-9])\"\n",
    "    return re.search(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "def extract_sku_and_angle_from_path(path_like: str) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    Returns (SKU like 'SS-12345', angle_base like 'fr_lft'/'fr').\n",
    "    Strategy:\n",
    "      1) Extract SKU from filename; if not found, try parent dirs.\n",
    "      2) After SKU in the filename, scan the suffix for the LONGEST valid angle token.\n",
    "      3) Fallback to whole filename scan, then parent dirs.\n",
    "    \"\"\"\n",
    "    p = Path(path_like)\n",
    "    name = _strip_copy_prefix(p.name)\n",
    "\n",
    "    # --- 1) SKU from filename, else parents\n",
    "    m = _SKU_RE.search(name)\n",
    "    sku = m.group(1).upper() if m else None\n",
    "    if sku is None:\n",
    "        for part in reversed(p.parts):\n",
    "            mm = _SKU_RE.search(part)\n",
    "            if mm:\n",
    "                sku = mm.group(1).upper()\n",
    "                break\n",
    "\n",
    "    # --- 2) Angle after SKU region\n",
    "    angle = None\n",
    "    tokens = _angle_tokens_desc()\n",
    "    if sku:\n",
    "        mname = _SKU_RE.search(name)\n",
    "        if mname:\n",
    "            suffix = name[mname.end():]  # everything after the SKU\n",
    "            for tok in tokens:\n",
    "                if _token_delim_search(tok, suffix):\n",
    "                    angle = tok\n",
    "                    break\n",
    "\n",
    "    # --- 3) Fallback: whole filename, then parents\n",
    "    if angle is None:\n",
    "        for tok in tokens:\n",
    "            if _token_delim_search(tok, name):\n",
    "                angle = tok\n",
    "                break\n",
    "    if angle is None:\n",
    "        # Look in parent folders\n",
    "        for part in reversed(p.parts[:-1]):\n",
    "            part_clean = _strip_copy_prefix(part)\n",
    "            for tok in tokens:\n",
    "                if _token_delim_search(tok, part_clean):\n",
    "                    angle = tok\n",
    "                    break\n",
    "            if angle:\n",
    "                break\n",
    "\n",
    "    return sku, angle\n",
    "\n",
    "# ===================== Source finding via SKU folder anywhere =====================\n",
    "@lru_cache(maxsize=1024)\n",
    "def _find_sku_folder_anywhere(working_root: str, sku_name: str) -> Path | None:\n",
    "    wr = Path(working_root)\n",
    "    if not wr.exists():\n",
    "        return None\n",
    "    sku_low = sku_name.lower()\n",
    "    best: tuple[int, Path] | None = None\n",
    "    for dirpath, dirnames, _ in os.walk(wr):\n",
    "        leaf = os.path.basename(dirpath)\n",
    "        if leaf.lower() == sku_low:\n",
    "            depth = len(Path(dirpath).parts)\n",
    "            cand = Path(dirpath)\n",
    "            if best is None or depth < best[0]:\n",
    "                best = (depth, cand)\n",
    "    return best[1] if best else None\n",
    "\n",
    "\n",
    "def _list_valid_images(folder: Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Return candidate source images in `folder`, excluding:\n",
    "      - any with 'generated', 'inpainted', '_nd', '_no_details', '_processed_by_detailer_'\n",
    "      - any with '_sec' anywhere in the filename (case-insensitive)\n",
    "    \"\"\"\n",
    "    deny_substrings = (\n",
    "        \"generated\",\n",
    "        \"inpainted\",\n",
    "        \"_nd\",\n",
    "        \"_no_details\",\n",
    "        \"_processed_by_detailer_\",\n",
    "        \"_sec\",   # \u2190 NEW: ignore secondary variants\n",
    "    )\n",
    "    out = []\n",
    "    for p in folder.iterdir():\n",
    "        if not (p.is_file() and p.suffix in VALID_EXTS):\n",
    "            continue\n",
    "        name_low = p.name.lower()\n",
    "        if any(s in name_low for s in deny_substrings):\n",
    "            continue\n",
    "        out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _rank_exact_angle(norm_stem: str, base: str, acceptable_suffixes: set[str]) -> int | None:\n",
    "    if norm_stem == f\"{base}_cut\": return 1\n",
    "    if norm_stem.startswith(base + \"_\") and norm_stem.endswith(\"_cut\"): return 2\n",
    "    if norm_stem == base: return 3\n",
    "    if norm_stem.startswith(base + \"_\"):\n",
    "        suf = norm_stem[len(base)+1:]\n",
    "        if suf in acceptable_suffixes: return 4\n",
    "    return None\n",
    "\n",
    "def _is_fr_family(base: str | None) -> bool:\n",
    "    if not base: return False\n",
    "    return base in (\"fr\",\"fr_cl\",\"fr_lft\",\"fr_rght\") or base.startswith(\"fr\")\n",
    "\n",
    "def _pick_source_in_dir(angle_base: str, directory: Path) -> Path | None:\n",
    "    entries = _list_valid_images(directory)\n",
    "    if not entries: return None\n",
    "    acceptable = set(ACCEPTABLE_SUFFIXES)\n",
    "\n",
    "    def _norm(p: Path) -> str:\n",
    "        return _strip_copy_prefix(p.stem).lower()\n",
    "\n",
    "    ranked: list[tuple[int,int,Path]] = []\n",
    "    if _is_fr_family(angle_base):\n",
    "        for p in entries:\n",
    "            n = _norm(p)\n",
    "            if n == \"fr_cut\": ranked.append((1,len(p.name),p)); continue\n",
    "            if n.startswith(\"fr_\") and n.endswith(\"_cut\"): ranked.append((2,len(p.name),p)); continue\n",
    "            if n == \"fr\": ranked.append((3,len(p.name),p)); continue\n",
    "            if n.startswith(\"fr_\"):\n",
    "                suf = n[len(\"fr_\"):]\n",
    "                if suf in acceptable: ranked.append((4,len(p.name),p)); continue\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        ranked=[]\n",
    "        for p in entries:\n",
    "            n=_norm(p)\n",
    "            r=_rank_exact_angle(n, angle_base, acceptable)\n",
    "            if r is not None: ranked.append((r,len(p.name),p))\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        return None\n",
    "    else:\n",
    "        for p in entries:\n",
    "            n=_norm(p)\n",
    "            r=_rank_exact_angle(n, angle_base, acceptable)\n",
    "            if r is not None: ranked.append((r,len(p.name),p))\n",
    "        if ranked:\n",
    "            ranked.sort(key=lambda t: (t[0], t[1], t[2].name))\n",
    "            return ranked[0][2]\n",
    "        return None\n",
    "\n",
    "def find_source_via_sku(gen_path: Path | str, working_root: Path | str) -> Path | None:\n",
    "    gen_path = Path(gen_path)\n",
    "    sku, angle_base = extract_sku_and_angle_from_path(str(gen_path))\n",
    "\n",
    "    if not sku:\n",
    "        print(f\"\u274c Could not extract SKU from: {gen_path.name}\")\n",
    "        return None\n",
    "\n",
    "    if not angle_base:\n",
    "        # No noisy warning anymore; we\u2019ll gracefully default.\n",
    "        angle_base = \"fr\"\n",
    "\n",
    "    sku_dir = _find_sku_folder_anywhere(str(working_root), sku)\n",
    "    if not sku_dir:\n",
    "        print(f\"\u274c SKU folder '{sku}' not found anywhere under {working_root}\")\n",
    "        return None\n",
    "\n",
    "    ricardo = sku_dir / \"Ricardo\"\n",
    "    for d in (ricardo, sku_dir):\n",
    "        if d.exists() and d.is_dir():\n",
    "            hit = _pick_source_in_dir(angle_base, d)\n",
    "            if hit: return hit\n",
    "\n",
    "    print(f\"\u26a0\ufe0f No suitable source found in '{sku_dir}' (Ricardo or root) for angle '{angle_base}'\")\n",
    "    return None\n",
    "\n",
    "def build_inpaint_suffix(details: list[dict]) -> str:\n",
    "    def slug(s: str) -> str:\n",
    "        s = s.lower().replace(\" \", \"-\")\n",
    "        return re.sub(r\"[^a-z0-9\\-]+\", \"\", s).strip(\"-\")\n",
    "    parts=[]\n",
    "    for d in details:\n",
    "        t = d[\"type\"]\n",
    "        c = d.get(\"color\",\"\")\n",
    "        if t != \"sleeve text\" and c:\n",
    "            parts.append(slug(f\"{c}-{t}\"))\n",
    "        else:\n",
    "            parts.append(slug(t))\n",
    "    return \"_\".join(parts) if parts else \"none\"\n",
    "\n",
    "# --- Build the required base \"SS-12345-bc_lft\" from the queued filename ---\n",
    "def build_out_base_from_gen(gen_path: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Returns (sku_upper, angle_lower, out_base).\n",
    "    out_base is 'SS-12345-bc_lft' (SKU + '-' + angle).\n",
    "    \"\"\"\n",
    "    sku, angle = extract_sku_and_angle_from_path(gen_path)\n",
    "    if not sku:\n",
    "        raise ValueError(f\"Cannot derive SKU from: {gen_path}\")\n",
    "    if not angle:\n",
    "        angle = \"fr\"\n",
    "    sku_up = sku.upper()\n",
    "    angle_lo = angle.lower()\n",
    "    return sku_up, angle_lo, f\"{sku_up}-{angle_lo}\"\n",
    "\n",
    "def target_already_has_inpainted(target_dir: str, sku: str, angle: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check TARGET_DIR for any file starting with 'SS-12345-bc_lft_inpainted'.\n",
    "    Case-insensitive; extension-agnostic.\n",
    "    \"\"\"\n",
    "    td = Path(target_dir)\n",
    "    if not td.exists():\n",
    "        return False\n",
    "    prefix = f\"{sku.upper()}-{angle.lower()}_inpainted\"\n",
    "    prefix_low = prefix.lower()\n",
    "    for p in td.iterdir():\n",
    "        if p.is_file() and p.suffix in VALID_EXTS:\n",
    "            if p.stem.lower().startswith(prefix_low):\n",
    "                return True\n",
    "    return False"
   ],
   "metadata": {
    "id": "pbCd-SBI530x"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def _inpaint_one_detail(gen_full: Image.Image,\n",
    "                        src_full: Image.Image,\n",
    "                        detail_prompt: str,\n",
    "                        *,\n",
    "                        garment_tag: str,\n",
    "                        restrict_mask_full: np.ndarray | None,\n",
    "                        generous_pad_px: int,\n",
    "                        tiny_pad_px: int,\n",
    "                        seed: int,\n",
    "                        visualize: bool) -> tuple[Image.Image, bool]:\n",
    "\n",
    "    _prepare_for_sam3()\n",
    "    modified = False\n",
    "\n",
    "    gen_view_for_sam3 = apply_binary_mask(gen_full, restrict_mask_full) if restrict_mask_full is not None else gen_full\n",
    "\n",
    "    gar_src_bb = detect_garment_box(src_full, garment_tag)\n",
    "    gar_gen_bb = detect_garment_box(gen_view_for_sam3, garment_tag, restrict_mask=restrict_mask_full)\n",
    "    if gar_src_bb is None or gar_gen_bb is None:\n",
    "        print(\"\u274c garment detection failed\"); return gen_full, modified\n",
    "\n",
    "    # square garment crops\n",
    "    def crop_to_square(image: Image.Image, bbox, pad_px=0):\n",
    "        x1,y1,x2,y2 = bbox\n",
    "        w,h = x2-x1, y2-y1\n",
    "        side = max(w,h) + 2*pad_px\n",
    "        cx,cy = (x1+x2)//2, (y1+y2)//2\n",
    "        lx=max(0,cx-side//2); ty=max(0,cy-side//2)\n",
    "        rx=lx+side; by=ty+side\n",
    "        W,H=image.size\n",
    "        if rx>W: lx -= (rx-W); rx=W\n",
    "        if by>H: ty -= (by-H); by=H\n",
    "        crop = image.crop((max(lx,0),max(ty,0),min(rx,W),min(by,H)))\n",
    "        out  = Image.new(\"RGB\",(side,side),(255,255,255))\n",
    "        dx=max(0,-lx); dy=max(0,-ty)\n",
    "        out.paste(crop,(dx,dy))\n",
    "        return out, (lx,ty,rx,by)\n",
    "\n",
    "    src_sq, sq_src = crop_to_square(src_full, gar_src_bb)\n",
    "    gen_sq, sq_gen = crop_to_square(gen_view_for_sam3, gar_gen_bb)\n",
    "\n",
    "    src_garm_sq, sq_coords_src = crop_to_square(src_full, gar_src_bb, pad_px=0)\n",
    "    gen_garm_sq, sq_coords_gen = crop_to_square(gen_view_for_sam3, gar_gen_bb, pad_px=0)\n",
    "\n",
    "    det_src_bb, _, det_src_mask_crop = detect_detail(src_sq, detail_prompt, crop_box_on_full=sq_src)\n",
    "    prior = make_spatial_prior_from_box(det_src_bb, src_garm_sq.size)\n",
    "\n",
    "    det_gen_bb, _, det_gen_mask_crop = detect_detail_topk7(\n",
    "        gen_garm_sq,\n",
    "        detail_prompt,\n",
    "        source_prior=prior,\n",
    "        restrict_mask=(restrict_mask_full, \"FULL\"),  # pass FULL mask\n",
    "        crop_box_on_full=sq_coords_gen,              # the (x1,y1,x2,y2) used to make gen_garm_sq\n",
    "        viz=False, debug=False\n",
    "    )\n",
    "    if det_src_bb is None or det_gen_bb is None:\n",
    "        print(f\"\u274c detail not found: {detail_prompt}\"); return gen_full, modified\n",
    "\n",
    "    # back to full coords\n",
    "    lx_s, ty_s, _, _ = sq_src\n",
    "    lx_g, ty_g, _, _ = sq_gen\n",
    "    src_det_bb = [det_src_bb[0]+lx_s, det_src_bb[1]+ty_s, det_src_bb[2]+lx_s, det_src_bb[3]+ty_s]\n",
    "    gen_det_bb = [det_gen_bb[0]+lx_g, det_gen_bb[1]+ty_g, det_gen_bb[2]+lx_g, det_gen_bb[3]+ty_g]\n",
    "\n",
    "    src_mask_full = _mask_crop_to_full(det_src_mask_crop, sq_src, src_full.size) if det_src_mask_crop is not None else None\n",
    "    gen_mask_full = _mask_crop_to_full(det_gen_mask_crop, sq_coords_gen, gen_full.size) if det_gen_mask_crop is not None else None\n",
    "    if src_mask_full is None:\n",
    "        src_mask_full = bbox_to_mask(src_det_bb, src_full.size, INPAINT_TINY_PAD)\n",
    "    if gen_mask_full is None:\n",
    "        gen_mask_full = bbox_to_mask(gen_det_bb, gen_full.size, INPAINT_TINY_PAD)\n",
    "    else:\n",
    "        gen_mask_full = enlarge_mask(gen_mask_full, scale=1.05)\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"detail on source\", _draw_bbox(src_full, src_det_bb)),\n",
    "            (\"detail on generated (masked)\", _draw_bbox(gen_view_for_sam3, gen_det_bb))\n",
    "        ], cols=2, figsize=(12,8))\n",
    "\n",
    "    # crops for IA\n",
    "    src_crop, src_mask, _   = crop_detail(src_full, src_mask_full, src_det_bb, 1024, 20)\n",
    "    gen_crop, gen_mask, box = crop_detail(gen_full, gen_mask_full, gen_det_bb, 1024, INPAINT_GENEROUS_PAD)\n",
    "\n",
    "    # diptych\n",
    "    src_arr = np.array(src_crop)\n",
    "    masked_src = src_arr  # keep source unmasked for IA\n",
    "\n",
    "    gen_arr = np.array(gen_crop)\n",
    "    gen_msk3 = np.stack([gen_mask]*3, -1)\n",
    "    zeros = np.zeros_like(masked_src)\n",
    "\n",
    "    diptych = np.concatenate([masked_src, gen_arr], axis=1).astype(np.uint8)\n",
    "    dip_mask = np.concatenate([zeros, gen_msk3], axis=1).astype(np.uint8)\n",
    "    dip_mask[dip_mask>0]=255\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"diptych\", Image.fromarray(diptych)),\n",
    "            (\"diptych mask\", Image.fromarray(dip_mask).convert(\"RGB\"))\n",
    "        ], cols=2, figsize=(12,8))\n",
    "\n",
    "    _prepare_for_insert_anything()\n",
    "\n",
    "    prior = redux(Image.fromarray(masked_src))\n",
    "    gen_obj = torch.Generator(IA_DEVICE).manual_seed(seed)\n",
    "    ia_out = pipe(\n",
    "        image=Image.fromarray(diptych),\n",
    "        mask_image=Image.fromarray(dip_mask),\n",
    "        height=1024,\n",
    "        width=2048,\n",
    "        max_sequence_length=512,\n",
    "        num_inference_steps=60,\n",
    "        guidance_scale=30,\n",
    "        generator=gen_obj,\n",
    "        **prior\n",
    "    ).images[0]\n",
    "\n",
    "    right_crop = ia_out.crop((1024,0,2048,1024))\n",
    "    gen_full = paste_crop_back(gen_full, right_crop, box, gen_mask)\n",
    "    modified = True\n",
    "\n",
    "    if visualize:\n",
    "        _show_images([\n",
    "            (\"IA result (2048\u00d71024)\", ia_out),\n",
    "            (\"after this detail\", gen_full)\n",
    "        ], cols=2, figsize=(14,8))\n",
    "\n",
    "    return gen_full, modified\n",
    "\n",
    "def inpaint_with_details_list(generated_path: str,\n",
    "                              source_path: str,\n",
    "                              details: list[dict],\n",
    "                              garment_type: str | None,\n",
    "                              visualize: bool = True) -> tuple[Image.Image, bool]:\n",
    "\n",
    "    gen_full = open_upright(generated_path)\n",
    "    src_full = open_source_with_black_bg(source_path)\n",
    "\n",
    "    restrict_mask_full = load_binary_mask_for_generated(generated_path, source_path, gen_full)\n",
    "\n",
    "    if restrict_mask_full is None:\n",
    "        print(\"\u26a0\ufe0f  No garment mask found \u2014 proceeding without restriction.\")\n",
    "    else:\n",
    "        print(\"\u2705 Garment mask loaded & aligned for\", os.path.basename(generated_path))\n",
    "\n",
    "    if garment_type is None or not garment_type.strip():\n",
    "        garment_type = extract_garment_type_from_path(source_path)\n",
    "    if not garment_type:\n",
    "        garment_type = \"t-shirt\"  # conservative default prompt\n",
    "\n",
    "    # twinset (optional, keep simple)\n",
    "    garment_tags = [garment_type.lower()]\n",
    "    if garment_type.lower() in TWINSET_TYPES:\n",
    "        garment_tags = [TOP_GARMENTS[0], BOTTOM_GARMENTS[0]]\n",
    "\n",
    "    out_img = gen_full.copy()\n",
    "    any_modified = False\n",
    "    for gtag in garment_tags:\n",
    "        for d in details:\n",
    "            d_type = d[\"type\"]\n",
    "            prompt_str = f\"{d_type}\".strip()\n",
    "            print(f\"\ud83d\udd04 Inpainting detail: {prompt_str}  (garment={gtag})\")\n",
    "            out_img, did_modify = _inpaint_one_detail(\n",
    "                out_img, src_full, prompt_str,\n",
    "                garment_tag=gtag,\n",
    "                restrict_mask_full=restrict_mask_full,\n",
    "                generous_pad_px=INPAINT_GENEROUS_PAD,\n",
    "                tiny_pad_px=INPAINT_TINY_PAD,\n",
    "                seed=INPAINT_SEED,\n",
    "                visualize=visualize\n",
    "            )\n",
    "            any_modified = any_modified or did_modify\n",
    "\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return out_img, any_modified\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "UkjJwDwh558v"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "def process_detailer_queue():\n",
    "    queue_root = Path(DETAILER_QUEUE_FOLDER)\n",
    "    if not queue_root.exists():\n",
    "        print(f\"\u274c Queue folder does not exist: {queue_root}\")\n",
    "        return\n",
    "\n",
    "    gen_files = [p for p in queue_root.rglob(\"*\") if p.is_file() and p.suffix in VALID_EXTS]\n",
    "    if not gen_files:\n",
    "        print(f\"\u2139\ufe0f No images found in {queue_root}\")\n",
    "        return\n",
    "\n",
    "    processed = skipped = failed = 0\n",
    "\n",
    "    for gen_path in sorted(gen_files, key=lambda p: (str(p.parent), p.name)):\n",
    "        try:\n",
    "            print(\"\\n\" + \"_\"*80)\n",
    "            print(f\"\ud83c\udfaf Queue item: {gen_path}\")\n",
    "\n",
    "            # 1) Read details from metadata\n",
    "            meta = read_details_from_metadata(str(gen_path))\n",
    "            if not meta or not meta.get(\"details\"):\n",
    "                print(\"\u23ed\ufe0f  No details found in metadata \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            details = [d for d in meta[\"details\"] if d[\"type\"] in ALLOWED_DETAIL_TYPES]\n",
    "            if not details:\n",
    "                print(\"\u23ed\ufe0f  Details list empty after normalization \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Find source garment near this item\n",
    "            src_p = find_source_via_sku(gen_path, Path(WORKING_DIR))\n",
    "            if not src_p:\n",
    "                print(\"\u23ed\ufe0f  Source garment not found \u2014 skipping\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            source_base = Path(src_p).stem  # e.g. SS-12345_fr\n",
    "            sku_up, angle_lo, out_base = build_out_base_from_gen(str(gen_path))\n",
    "            out_ext = \".png\"\n",
    "\n",
    "            # 3) Skip guard: any prior inpainted for this SKU+angle?\n",
    "            if SKIP_IF_ALREADY_INPAINTED and target_already_has_inpainted(TARGET_DIR, sku_up, angle_lo):\n",
    "              print(f\"\u23ed\ufe0f  Already have inpainted for {out_base} in TARGET_DIR \u2014 skipping\")\n",
    "              skipped += 1\n",
    "              continue\n",
    "\n",
    "            # 4) Inpaint\n",
    "            garment_type = extract_garment_type_from_path(str(src_p))\n",
    "            out_img, modified = inpaint_with_details_list(\n",
    "                str(gen_path),\n",
    "                str(src_p),\n",
    "                details=details,\n",
    "                garment_type=garment_type,\n",
    "                visualize=VISUALIZE\n",
    "            )\n",
    "            if not modified:\n",
    "                print(\"\u23ed\ufe0f  No details applied \u2014 skipping save\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # 5) Build output names (keep source naming; append detail suffixes)\n",
    "            suffix   = build_inpaint_suffix(details)   # unchanged\n",
    "            dst_src  = Path(TARGET_DIR) / f\"{out_base}{out_ext}\"                         # e.g., SS-12345-bc_lft.jpg\n",
    "            dst_ia   = Path(TARGET_DIR) / f\"{out_base}_inpainted_{suffix}{out_ext}\"      # e.g., SS-12345-bc_lft_inpainted_red_logo.jpg\n",
    "\n",
    "            # 6) Save outputs: copy source + save inpainted\n",
    "            #if not dst_src.exists():\n",
    "            #    shutil.copy2(str(src_p), str(dst_src))\n",
    "            #    print(f\"\ud83d\udcce Saved source \u2192 {dst_src.name}\")\n",
    "            #else:\n",
    "            #    print(f\"\ud83d\udcce Source already present in TARGET_DIR \u2192 {dst_src.name}\")\n",
    "\n",
    "            # --- Save inpainted result ---\n",
    "            out_img.save(str(dst_ia))\n",
    "            print(f\"\u2705 Saved inpainted \u2192 {dst_ia.name}\")\n",
    "            processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Failed on {gen_path.name}: {e}\")\n",
    "            failed += 1\n",
    "\n",
    "    print(\"\\n==== SUMMARY ====\")\n",
    "    print(f\"Processed: {processed}  |  Skipped: {skipped}  |  Failed: {failed}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ekO6QtZR59KR"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#RUN"
   ],
   "metadata": {
    "id": "XPGXKHLhO_hS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run\n",
    "process_detailer_queue()"
   ],
   "metadata": {
    "id": "pxMzQe-U5_Om",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c75b0d7-e2c0-4182-d5ad-761c383e487d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "________________________________________________________________________________\n",
      "\ud83c\udfaf Queue item: /content/drive/MyDrive/DETAILER_TODO/plisse/SS-29102-bc_both.png\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#UNASSIGN"
   ],
   "metadata": {
    "id": "qpoDyMhnPO83"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "metadata": {
    "id": "GUsJyqO4PN0L"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}